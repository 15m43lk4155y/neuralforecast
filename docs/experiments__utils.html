---

title: Experiment Utils


keywords: fastai
sidebar: home_sidebar

summary: "Set of functions to easily perform experiments."
description: "Set of functions to easily perform experiments."
nb_path: "nbs/experiments__utils.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/experiments__utils.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_mask_dfs" class="doc_header"><code>get_mask_dfs</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L37" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_mask_dfs</code>(<strong><code>Y_df</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>ds_in_test</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_random_mask_dfs" class="doc_header"><code>get_random_mask_dfs</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L67" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_random_mask_dfs</code>(<strong><code>Y_df</code></strong>, <strong><code>ds_in_test</code></strong>, <strong><code>n_val_windows</code></strong>, <strong><code>n_ds_val_window</code></strong>, <strong><code>n_uids</code></strong>, <strong><code>freq</code></strong>)</p>
</blockquote>
<p>Generates train, test and random validation mask.
Train mask begins by avoiding ds_in_test</p>
<p>Validation mask: 1) samples n_uids unique ids
                 2) creates windows of size n_ds_val_window</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>ds_in_test: int
    Number of ds in test.
n_uids: int
    Number of unique ids in validation.
n_val_windows: int
    Number of windows for validation.
n_ds_val_window: int
    Number of ds in each validation window.
periods: int
    ds_in_test multiplier.
freq: str
    string that determines datestamp frequency, used in
    random windows creation.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="scale_data" class="doc_header"><code>scale_data</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L123" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>scale_data</code>(<strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>mask_df</code></strong>, <strong><code>normalizer_y</code></strong>, <strong><code>normalizer_x</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="create_datasets" class="doc_header"><code>create_datasets</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L141" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>create_datasets</code>(<strong><code>mc</code></strong>, <strong><code>S_df</code></strong>, <strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>f_cols</code></strong>, <strong><code>ds_in_test</code></strong>, <strong><code>ds_in_val</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_loaders" class="doc_header"><code>instantiate_loaders</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L204" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_loaders</code>(<strong><code>mc</code></strong>, <strong><code>train_dataset</code></strong>, <strong><code>val_dataset</code></strong>, <strong><code>test_dataset</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_nbeats" class="doc_header"><code>instantiate_nbeats</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L226" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_nbeats</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_esrnn" class="doc_header"><code>instantiate_esrnn</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L258" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_esrnn</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_mqesrnn" class="doc_header"><code>instantiate_mqesrnn</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L289" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_mqesrnn</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_deepmidas" class="doc_header"><code>instantiate_deepmidas</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L317" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_deepmidas</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_model" class="doc_header"><code>instantiate_model</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L350" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_model</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="predict" class="doc_header"><code>predict</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L358" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>predict</code>(<strong><code>mc</code></strong>, <strong><code>model</code></strong>, <strong><code>trainer</code></strong>, <strong><code>loader</code></strong>, <strong><code>scaler_y</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="model_fit_predict" class="doc_header"><code>model_fit_predict</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L375" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>model_fit_predict</code>(<strong><code>mc</code></strong>, <strong><code>S_df</code></strong>, <strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>f_cols</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>ds_in_test</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="evaluate_model" class="doc_header"><code>evaluate_model</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L440" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>evaluate_model</code>(<strong><code>mc</code></strong>, <strong><code>loss_function_val</code></strong>, <strong><code>loss_functions_test</code></strong>, <strong><code>S_df</code></strong>, <strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>f_cols</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>ds_in_test</code></strong>, <strong><code>return_forecasts</code></strong>, <strong><code>loss_kwargs</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="hyperopt_tunning" class="doc_header"><code>hyperopt_tunning</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L490" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>hyperopt_tunning</code>(<strong><code>space</code></strong>, <strong><code>hyperopt_max_evals</code></strong>, <strong><code>loss_function_val</code></strong>, <strong><code>loss_functions_test</code></strong>, <strong><code>S_df</code></strong>, <strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>f_cols</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>ds_in_test</code></strong>, <strong><code>return_forecasts</code></strong>, <strong><code>loss_kwargs</code></strong>=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Experiment-Utils-Examples">Experiment Utils Examples<a class="anchor-link" href="#Experiment-Utils-Examples"> </a></h1>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span>
<span class="kn">from</span> <span class="nn">nixtlats.losses.numpy</span> <span class="kn">import</span> <span class="n">mae</span><span class="p">,</span> <span class="n">mape</span><span class="p">,</span> <span class="n">smape</span><span class="p">,</span> <span class="n">rmse</span><span class="p">,</span> <span class="n">pinball_loss</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span>
<span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span> <span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span>  

<span class="n">deepmidas_space</span><span class="o">=</span> <span class="p">{</span><span class="c1"># Architecture parameters</span>
               <span class="s1">&#39;model&#39;</span><span class="p">:</span><span class="s1">&#39;deepmidas&#39;</span><span class="p">,</span>
               <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="s1">&#39;simple&#39;</span><span class="p">,</span>
               <span class="s1">&#39;n_time_in&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_time_in&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">7</span><span class="o">*</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;n_time_out&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_time_out&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;n_x_hidden&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;n_x_hidden&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="s1">&#39;n_s_hidden&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_s_hidden&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
               <span class="s1">&#39;shared_weights&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;shared_weights&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">]),</span>
               <span class="s1">&#39;activation&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;SELU&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;initialization&#39;</span><span class="p">:</span>  <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;initialization&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;glorot_normal&#39;</span><span class="p">,</span><span class="s1">&#39;he_normal&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;stack_types&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;stack_types&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;identity&#39;</span><span class="p">]]),</span>
               <span class="s1">&#39;n_blocks&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_blocks&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="p">]),</span>
               <span class="s1">&#39;n_layers&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_layers&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="p">]),</span>
               <span class="s1">&#39;n_hidden&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_hidden&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="mi">256</span> <span class="p">]),</span>
               <span class="s1">&#39;n_pool_kernel_size&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_pool_kernel_size&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="p">[</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span> <span class="p">]</span> <span class="p">]),</span>
               <span class="s1">&#39;n_freq_downsample&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_freq_downsample&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="p">[</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">1</span> <span class="p">]</span> <span class="p">]),</span>
               <span class="c1"># Regularization and optimization parameters</span>
               <span class="s1">&#39;batch_normalization&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;batch_normalization&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">]),</span>
               <span class="s1">&#39;dropout_prob_theta&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">&#39;dropout_prob_theta&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
               <span class="s1">&#39;dropout_prob_exogenous&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">&#39;dropout_prob_exogenous&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
               <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">5e-4</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)),</span>
               <span class="s1">&#39;lr_decay&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">&#39;lr_decay&#39;</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
               <span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">]),</span> 
               <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">5e-5</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">5e-3</span><span class="p">)),</span>
               <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;max_epochs&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">]),</span> <span class="c1">#&#39;n_iterations&#39;: hp.choice(&#39;n_iterations&#39;, [10])</span>
               <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;max_steps&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">]),</span>
               <span class="s1">&#39;early_stop_patience&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;early_stop_patience&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">16</span><span class="p">]),</span>
               <span class="s1">&#39;eval_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;eval_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">50</span><span class="p">]),</span>
               <span class="s1">&#39;loss_train&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;MAE&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;loss_hypar&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss_hypar&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]),</span>                
               <span class="s1">&#39;loss_valid&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss_valid&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;MAE&#39;</span><span class="p">]),</span> <span class="c1">#[args.val_loss]),</span>
               <span class="s1">&#39;l1_theta&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;l1_theta&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
               <span class="c1"># Data parameters</span>
               <span class="s1">&#39;normalizer_y&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;normalizer_y&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">]),</span>
               <span class="s1">&#39;normalizer_x&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;normalizer_x&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;median&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;complete_windows&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;complete_windows&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">]),</span>
               <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;frequency&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;H&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;seasonality&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;seasonality&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>      
               <span class="s1">&#39;idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;idx_to_sample_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">256</span><span class="p">]),</span>
               <span class="s1">&#39;random_seed&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;random_seed&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;device&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">device</span><span class="p">])}</span>

<span class="n">mc</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span><span class="s1">&#39;deepmidas&#39;</span><span class="p">,</span>
      <span class="c1"># Architecture parameters</span>
      <span class="s1">&#39;n_time_in&#39;</span><span class="p">:</span> <span class="mi">7</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;n_time_out&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;n_x_hidden&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
      <span class="s1">&#39;n_s_hidden&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
      <span class="s1">&#39;shared_weights&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;activation&#39;</span><span class="p">:</span> <span class="s1">&#39;SELU&#39;</span><span class="p">,</span>
      <span class="s1">&#39;initialization&#39;</span><span class="p">:</span> <span class="s1">&#39;he_normal&#39;</span><span class="p">,</span>
      <span class="s1">&#39;stack_types&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;identity&#39;</span><span class="p">,</span> <span class="s1">&#39;identity&#39;</span><span class="p">],</span>
      <span class="s1">&#39;n_blocks&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
      <span class="s1">&#39;n_layers&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
      <span class="s1">&#39;n_pool_kernel_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
      <span class="s1">&#39;n_freq_downsample&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
      <span class="s1">&#39;n_hidden&#39;</span><span class="p">:</span> <span class="mi">364</span><span class="p">,</span>
      <span class="c1"># Regularization and optimization parameters</span>
      <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="c1">#&#39;n_iterations&#39;: 100,</span>
      <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>      
      <span class="s1">&#39;early_stop_patience&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
      <span class="s1">&#39;batch_normalization&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;dropout_prob_theta&#39;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
      <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.0005</span><span class="p">,</span> <span class="c1">#0.002,</span>
      <span class="s1">&#39;lr_decay&#39;</span><span class="p">:</span> <span class="mf">0.64</span><span class="p">,</span>
      <span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
      <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.00015</span><span class="p">,</span>
      <span class="s1">&#39;eval_freq&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
      <span class="s1">&#39;loss_train&#39;</span><span class="p">:</span> <span class="s1">&#39;PINBALL&#39;</span><span class="p">,</span>
      <span class="s1">&#39;loss_hypar&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="c1">#0.49,</span>
      <span class="s1">&#39;loss_valid&#39;</span><span class="p">:</span> <span class="s1">&#39;MAE&#39;</span><span class="p">,</span>
      <span class="s1">&#39;l1_theta&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
      <span class="c1"># Data parameters</span>
      <span class="s1">&#39;normalizer_y&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
      <span class="s1">&#39;normalizer_x&#39;</span><span class="p">:</span> <span class="s1">&#39;median&#39;</span><span class="p">,</span>
      <span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">:</span> <span class="mi">100_000</span><span class="p">,</span>
      <span class="s1">&#39;complete_windows&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;frequency&#39;</span><span class="p">:</span><span class="s1">&#39;H&#39;</span><span class="p">,</span>
      <span class="s1">&#39;seasonality&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
      <span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;random_seed&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
      <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cpu&#39;</span><span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">nixtlats.data.datasets.epf</span> <span class="kn">import</span> <span class="n">EPF</span><span class="p">,</span> <span class="n">EPFInfo</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NP&#39;</span><span class="p">]</span>

<span class="n">Y_df</span><span class="p">,</span> <span class="n">X_df</span><span class="p">,</span> <span class="n">S_df</span> <span class="o">=</span> <span class="n">EPF</span><span class="o">.</span><span class="n">load_groups</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">dataset</span><span class="p">)</span>

<span class="n">X_df</span> <span class="o">=</span> <span class="n">X_df</span><span class="p">[[</span><span class="s1">&#39;unique_id&#39;</span><span class="p">,</span> <span class="s1">&#39;ds&#39;</span><span class="p">,</span> <span class="s1">&#39;week_day&#39;</span><span class="p">]]</span>
<span class="n">Y_min</span> <span class="o">=</span> <span class="n">Y_df</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="c1">#Y_df.y = Y_df.y - Y_min + 20</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Y_df</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># backpropagation trough time is slow</span>
<span class="c1"># result = evaluate_model(loss_function=mae, mc=mc, </span>
<span class="c1">#                         S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],</span>
<span class="c1">#                         ds_in_test=0, ds_in_val=728*24,</span>
<span class="c1">#                         n_uids=None, n_val_windows=None, freq=None,</span>
<span class="c1">#                         is_val_random=False, loss_kwargs={})</span>
<span class="c1"># result</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># plt.plot(Y_df[&#39;y&#39;][-728*24:].values)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trials</span> <span class="o">=</span> <span class="n">hyperopt_tunning</span><span class="p">(</span><span class="n">space</span><span class="o">=</span><span class="n">deepmidas_space</span><span class="p">,</span> <span class="n">hyperopt_max_evals</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loss_function_val</span><span class="o">=</span><span class="n">mae</span><span class="p">,</span>
                          <span class="n">loss_functions_test</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;mae&#39;</span><span class="p">:</span> <span class="n">mae</span><span class="p">,</span> <span class="s1">&#39;rmse&#39;</span><span class="p">:</span> <span class="n">rmse</span><span class="p">},</span>
                          <span class="n">S_df</span><span class="o">=</span><span class="n">S_df</span><span class="p">,</span> <span class="n">Y_df</span><span class="o">=</span><span class="n">Y_df</span><span class="p">,</span> <span class="n">X_df</span><span class="o">=</span><span class="n">X_df</span><span class="p">,</span> <span class="n">f_cols</span><span class="o">=</span><span class="p">[],</span>
                          <span class="n">ds_in_val</span><span class="o">=</span><span class="mi">7</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span> <span class="n">ds_in_test</span><span class="o">=</span><span class="mi">7</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span> <span class="n">return_forecasts</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">loss_kwargs</span><span class="o">=</span><span class="p">{})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>  0%|          | 0/2 [00:00&lt;?, ?trial/s, best loss=?]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:hyperopt.tpe:build_posterior_wrapper took 0.016587 seconds
INFO:hyperopt.tpe:TPE using 0 trials
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>===============================================

activation                                SELU
batch_normalization                      False
batch_size                                 256
complete_windows                         False
device                                     cpu
dropout_prob_exogenous                0.382591
dropout_prob_theta                    0.297799
early_stop_patience                         16
eval_freq                                   50
frequency                                    H
idx_to_sample_freq                          24
initialization                       he_normal
l1_theta                                     0
learning_rate                         0.000787
loss_hypar                                 0.5
loss_train                                 MAE
loss_valid                                 MAE
lr_decay                              0.482621
lr_decay_step_size                         100
max_epochs                                  10
max_steps                                 None
mode                                    simple
model                                deepmidas
n_blocks                                (1, 1)
n_freq_downsample                      (24, 1)
n_hidden                                   256
n_layers                                (2, 2)
n_pool_kernel_size                      (4, 1)
n_s_hidden                                   0
n_time_in                                  168
n_time_out                                  24
n_x_hidden                                 5.0
normalizer_x                            median
normalizer_y                              None
random_seed                               19.0
seasonality                                 24
shared_weights                           False
stack_types               (identity, identity)
val_idx_to_sample_freq                      24
weight_decay                          0.002182
dtype: object
===============================================

  0%|          | 0/2 [00:00&lt;?, ?trial/s, best loss=?]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2018-12-11 2018-12-24 23:00:00
          1           2013-01-01 2018-12-10 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=99.36, 	52080 time stamps 
Outsample percentage=0.64, 	336 time stamps 

/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument &#39;labels&#39; will be keyword-only
  X.drop([&#39;unique_id&#39;, &#39;ds&#39;], 1, inplace=True)

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2018-12-24 23:00:00
          1           2018-12-11 2018-12-17 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=0.32, 	168 time stamps 
Outsample percentage=99.68, 	52248 time stamps 

/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument &#39;labels&#39; will be keyword-only
  X.drop([&#39;unique_id&#39;, &#39;ds&#39;], 1, inplace=True)

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2018-12-17 23:00:00
          1           2018-12-18 2018-12-24 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=0.32, 	168 time stamps 
Outsample percentage=99.68, 	52248 time stamps 

/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument &#39;labels&#39; will be keyword-only
  X.drop([&#39;unique_id&#39;, &#39;ds&#39;], 1, inplace=True)

GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs

  | Name  | Type       | Params
-------------------------------------
0 | model | _DeepMIDAS | 376 K 
-------------------------------------
376 K     Trainable params
0         Non-trainable params
376 K     Total params
1.508     Total estimated model params size (MB)
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Validation sanity check: 0it [00:00, ?it/s]
Validation sanity check:   0%|          | 0/1 [00:00&lt;?, ?it/s]
  0%|          | 0/2 [00:01&lt;?, ?trial/s, best loss=?]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f&#34;The dataloader, {name}, does not have many workers which may be a bottleneck.&#34;

/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f&#34;The dataloader, {name}, does not have many workers which may be a bottleneck.&#34;

/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:323: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  f&#34;The number of training samples ({self.num_training_batches}) is smaller than the logging interval&#34;

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Training: -1it [00:00, ?it/s]
Training:   0%|          | 0/1 [00:00&lt;00:00, 17924.38it/s]
Epoch 0:   0%|          | 0/1 [00:00&lt;00:00, 246.68it/s]   
  0%|          | 0/2 [00:01&lt;?, ?trial/s, best loss=?]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459064158/work/c10/core/TensorImpl.h:1156.)
  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch 0: 100%|##########| 1/1 [00:00&lt;00:00, 12.83it/s] 
Epoch 0: 100%|##########| 1/1 [00:00&lt;00:00, 12.51it/s, loss=4.38, v_num=74, train_loss_step=4.380]
Epoch 0:   0%|          | 0/1 [00:00&lt;00:00, 17848.10it/s, loss=4.38, v_num=74, train_loss_step=4.380]
Epoch 1:   0%|          | 0/1 [00:00&lt;00:00, 245.76it/s, loss=4.38, v_num=74, train_loss_step=4.380]  
Epoch 1: 100%|##########| 1/1 [00:00&lt;00:00, 13.61it/s, loss=4.38, v_num=74, train_loss_step=4.380] 
Epoch 1: 100%|##########| 1/1 [00:00&lt;00:00, 13.11it/s, loss=5.88, v_num=74, train_loss_step=7.390, train_loss_epoch=4.380]
Epoch 1:   0%|          | 0/1 [00:00&lt;00:00, 15363.75it/s, loss=5.88, v_num=74, train_loss_step=7.390, train_loss_epoch=4.380]
Epoch 2:   0%|          | 0/1 [00:00&lt;00:00, 289.86it/s, loss=5.88, v_num=74, train_loss_step=7.390, train_loss_epoch=4.380]  
Epoch 2: 100%|##########| 1/1 [00:00&lt;00:00, 13.23it/s, loss=5.88, v_num=74, train_loss_step=7.390, train_loss_epoch=4.380] 
Epoch 2: 100%|##########| 1/1 [00:00&lt;00:00, 12.63it/s, loss=5.21, v_num=74, train_loss_step=3.860, train_loss_epoch=7.390]
Epoch 2:   0%|          | 0/1 [00:00&lt;00:00, 15650.39it/s, loss=5.21, v_num=74, train_loss_step=3.860, train_loss_epoch=7.390]
Epoch 3:   0%|          | 0/1 [00:00&lt;00:00, 118.10it/s, loss=5.21, v_num=74, train_loss_step=3.860, train_loss_epoch=7.390]  
Epoch 3: 100%|##########| 1/1 [00:00&lt;00:00, 11.53it/s, loss=5.21, v_num=74, train_loss_step=3.860, train_loss_epoch=7.390] 
Epoch 3: 100%|##########| 1/1 [00:00&lt;00:00, 11.28it/s, loss=5.21, v_num=74, train_loss_step=5.230, train_loss_epoch=3.860]
Epoch 3:   0%|          | 0/1 [00:00&lt;00:00, 16710.37it/s, loss=5.21, v_num=74, train_loss_step=5.230, train_loss_epoch=3.860]
Epoch 4:   0%|          | 0/1 [00:00&lt;00:00, 209.34it/s, loss=5.21, v_num=74, train_loss_step=5.230, train_loss_epoch=3.860]  
Epoch 4: 100%|##########| 1/1 [00:00&lt;00:00, 13.56it/s, loss=5.21, v_num=74, train_loss_step=5.230, train_loss_epoch=3.860] 
Epoch 4: 100%|##########| 1/1 [00:00&lt;00:00, 13.17it/s, loss=5.15, v_num=74, train_loss_step=4.870, train_loss_epoch=5.230]
Epoch 4:   0%|          | 0/1 [00:00&lt;00:00, 18157.16it/s, loss=5.15, v_num=74, train_loss_step=4.870, train_loss_epoch=5.230]
Epoch 5:   0%|          | 0/1 [00:00&lt;00:00, 252.84it/s, loss=5.15, v_num=74, train_loss_step=4.870, train_loss_epoch=5.230]  
Epoch 5: 100%|##########| 1/1 [00:00&lt;00:00, 14.39it/s, loss=5.15, v_num=74, train_loss_step=4.870, train_loss_epoch=5.230] 
Epoch 5: 100%|##########| 1/1 [00:00&lt;00:00, 14.02it/s, loss=4.85, v_num=74, train_loss_step=3.350, train_loss_epoch=4.870]
Epoch 5:   0%|          | 0/1 [00:00&lt;00:00, 18157.16it/s, loss=4.85, v_num=74, train_loss_step=3.350, train_loss_epoch=4.870]
Epoch 6:   0%|          | 0/1 [00:00&lt;00:00, 300.02it/s, loss=4.85, v_num=74, train_loss_step=3.350, train_loss_epoch=4.870]  
Epoch 6: 100%|##########| 1/1 [00:00&lt;00:00, 14.26it/s, loss=4.85, v_num=74, train_loss_step=3.350, train_loss_epoch=4.870] 
Epoch 6: 100%|##########| 1/1 [00:00&lt;00:00, 13.86it/s, loss=4.61, v_num=74, train_loss_step=3.210, train_loss_epoch=3.350]
Epoch 6:   0%|          | 0/1 [00:00&lt;00:00, 17848.10it/s, loss=4.61, v_num=74, train_loss_step=3.210, train_loss_epoch=3.350]
Epoch 7:   0%|          | 0/1 [00:00&lt;00:00, 249.81it/s, loss=4.61, v_num=74, train_loss_step=3.210, train_loss_epoch=3.350]  
Epoch 7: 100%|##########| 1/1 [00:00&lt;00:00, 14.38it/s, loss=4.61, v_num=74, train_loss_step=3.210, train_loss_epoch=3.350] 
Epoch 7: 100%|##########| 1/1 [00:00&lt;00:00, 13.98it/s, loss=4.51, v_num=74, train_loss_step=3.800, train_loss_epoch=3.210]
Epoch 7:   0%|          | 0/1 [00:00&lt;00:00, 17476.27it/s, loss=4.51, v_num=74, train_loss_step=3.800, train_loss_epoch=3.210]
Epoch 8:   0%|          | 0/1 [00:00&lt;00:00, 273.07it/s, loss=4.51, v_num=74, train_loss_step=3.800, train_loss_epoch=3.210]  
Epoch 8: 100%|##########| 1/1 [00:00&lt;00:00, 14.65it/s, loss=4.51, v_num=74, train_loss_step=3.800, train_loss_epoch=3.210] 
Epoch 8: 100%|##########| 1/1 [00:00&lt;00:00, 14.24it/s, loss=4.42, v_num=74, train_loss_step=3.690, train_loss_epoch=3.800]
Epoch 8:   0%|          | 0/1 [00:00&lt;00:00, 19239.93it/s, loss=4.42, v_num=74, train_loss_step=3.690, train_loss_epoch=3.800]
Epoch 9:   0%|          | 0/1 [00:00&lt;00:00, 309.86it/s, loss=4.42, v_num=74, train_loss_step=3.690, train_loss_epoch=3.800]  
Epoch 9: 100%|##########| 1/1 [00:00&lt;00:00, 14.34it/s, loss=4.42, v_num=74, train_loss_step=3.690, train_loss_epoch=3.800] 
Epoch 9: 100%|##########| 1/1 [00:00&lt;00:00, 13.95it/s, loss=4.29, v_num=74, train_loss_step=3.090, train_loss_epoch=3.690]
Epoch 9: 100%|##########| 1/1 [00:00&lt;00:00, 13.44it/s, loss=4.29, v_num=74, train_loss_step=3.090, train_loss_epoch=3.690]
  0%|          | 0/2 [00:03&lt;?, ?trial/s, best loss=?]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f&#34;The dataloader, {name}, does not have many workers which may be a bottleneck.&#34;

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Predicting: 1it [00:00, ?it/s]
Predicting: 100%|##########| 1/1 [00:00&lt;?, ?it/s]
VAL y_true.shape: (7, 24)
VAL y_hat.shape: (7, 24)
Predicting: 1it [00:00, ?it/s]
Predicting: 100%|##########| 1/1 [00:00&lt;?, ?it/s]
TEST y_true.shape: (7, 24)
TEST y_hat.shape: (7, 24)
 50%|█████     | 1/2 [00:03&lt;00:03,  3.19s/trial, best loss: 6.4836297035217285]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:hyperopt.tpe:build_posterior_wrapper took 0.014987 seconds
INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 6.483630
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>===============================================

activation                                SELU
batch_normalization                      False
batch_size                                 256
complete_windows                         False
device                                     cpu
dropout_prob_exogenous                0.038663
dropout_prob_theta                    0.330628
early_stop_patience                         16
eval_freq                                   50
frequency                                    H
idx_to_sample_freq                          24
initialization                       he_normal
l1_theta                                     0
learning_rate                         0.000977
loss_hypar                                 0.5
loss_train                                 MAE
loss_valid                                 MAE
lr_decay                              0.354789
lr_decay_step_size                         100
max_epochs                                  10
max_steps                                 None
mode                                    simple
model                                deepmidas
n_blocks                                (1, 1)
n_freq_downsample                      (24, 1)
n_hidden                                   256
n_layers                                (2, 2)
n_pool_kernel_size                      (4, 1)
n_s_hidden                                   0
n_time_in                                  168
n_time_out                                  24
n_x_hidden                                 5.0
normalizer_x                            median
normalizer_y                              None
random_seed                               14.0
seasonality                                 24
shared_weights                           False
stack_types               (identity, identity)
val_idx_to_sample_freq                      24
weight_decay                           0.00253
dtype: object
===============================================

 50%|█████     | 1/2 [00:03&lt;00:03,  3.19s/trial, best loss: 6.4836297035217285]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2018-12-11 2018-12-24 23:00:00
          1           2013-01-01 2018-12-10 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=99.36, 	52080 time stamps 
Outsample percentage=0.64, 	336 time stamps 

/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument &#39;labels&#39; will be keyword-only
  X.drop([&#39;unique_id&#39;, &#39;ds&#39;], 1, inplace=True)

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2018-12-24 23:00:00
          1           2018-12-11 2018-12-17 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=0.32, 	168 time stamps 
Outsample percentage=99.68, 	52248 time stamps 

/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument &#39;labels&#39; will be keyword-only
  X.drop([&#39;unique_id&#39;, &#39;ds&#39;], 1, inplace=True)

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2018-12-17 23:00:00
          1           2018-12-18 2018-12-24 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=0.32, 	168 time stamps 
Outsample percentage=99.68, 	52248 time stamps 

/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument &#39;labels&#39; will be keyword-only
  X.drop([&#39;unique_id&#39;, &#39;ds&#39;], 1, inplace=True)

GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs

  | Name  | Type       | Params
-------------------------------------
0 | model | _DeepMIDAS | 376 K 
-------------------------------------
376 K     Trainable params
0         Non-trainable params
376 K     Total params
1.508     Total estimated model params size (MB)
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Validation sanity check: 0it [00:00, ?it/s]
Validation sanity check:   0%|          | 0/1 [00:00&lt;?, ?it/s]
 50%|█████     | 1/2 [00:04&lt;00:03,  3.19s/trial, best loss: 6.4836297035217285]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f&#34;The dataloader, {name}, does not have many workers which may be a bottleneck.&#34;

/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f&#34;The dataloader, {name}, does not have many workers which may be a bottleneck.&#34;

/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:323: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  f&#34;The number of training samples ({self.num_training_batches}) is smaller than the logging interval&#34;

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Training: -1it [00:00, ?it/s]
Training:   0%|          | 0/1 [00:00&lt;00:00, 17549.39it/s]
Epoch 0:   0%|          | 0/1 [00:00&lt;00:00, 253.36it/s]   
Epoch 0: 100%|##########| 1/1 [00:00&lt;00:00, 14.83it/s] 
Epoch 0: 100%|##########| 1/1 [00:00&lt;00:00, 14.46it/s, loss=4.34, v_num=75, train_loss_step=4.340]
Epoch 0:   0%|          | 0/1 [00:00&lt;00:00, 16980.99it/s, loss=4.34, v_num=75, train_loss_step=4.340]
Epoch 1:   0%|          | 0/1 [00:00&lt;00:00, 245.34it/s, loss=4.34, v_num=75, train_loss_step=4.340]  
Epoch 1: 100%|##########| 1/1 [00:00&lt;00:00, 14.74it/s, loss=4.34, v_num=75, train_loss_step=4.340] 
Epoch 1: 100%|##########| 1/1 [00:00&lt;00:00, 14.34it/s, loss=5.19, v_num=75, train_loss_step=6.030, train_loss_epoch=4.340]
Epoch 1:   0%|          | 0/1 [00:00&lt;00:00, 15887.52it/s, loss=5.19, v_num=75, train_loss_step=6.030, train_loss_epoch=4.340]
Epoch 2:   0%|          | 0/1 [00:00&lt;00:00, 246.13it/s, loss=5.19, v_num=75, train_loss_step=6.030, train_loss_epoch=4.340]  
Epoch 2: 100%|##########| 1/1 [00:00&lt;00:00, 14.36it/s, loss=5.19, v_num=75, train_loss_step=6.030, train_loss_epoch=4.340] 
Epoch 2: 100%|##########| 1/1 [00:00&lt;00:00, 13.90it/s, loss=4.73, v_num=75, train_loss_step=3.800, train_loss_epoch=6.030]
Epoch 2:   0%|          | 0/1 [00:00&lt;00:00, 16131.94it/s, loss=4.73, v_num=75, train_loss_step=3.800, train_loss_epoch=6.030]
Epoch 3:   0%|          | 0/1 [00:00&lt;00:00, 258.86it/s, loss=4.73, v_num=75, train_loss_step=3.800, train_loss_epoch=6.030]  
Epoch 3: 100%|##########| 1/1 [00:00&lt;00:00, 14.42it/s, loss=4.73, v_num=75, train_loss_step=3.800, train_loss_epoch=6.030] 
Epoch 3: 100%|##########| 1/1 [00:00&lt;00:00, 13.92it/s, loss=4.64, v_num=75, train_loss_step=4.400, train_loss_epoch=3.800]
Epoch 3:   0%|          | 0/1 [00:00&lt;00:00, 10538.45it/s, loss=4.64, v_num=75, train_loss_step=4.400, train_loss_epoch=3.800]
Epoch 4:   0%|          | 0/1 [00:00&lt;00:00, 249.56it/s, loss=4.64, v_num=75, train_loss_step=4.400, train_loss_epoch=3.800]  
Epoch 4: 100%|##########| 1/1 [00:00&lt;00:00, 13.84it/s, loss=4.64, v_num=75, train_loss_step=4.400, train_loss_epoch=3.800] 
Epoch 4: 100%|##########| 1/1 [00:00&lt;00:00, 13.35it/s, loss=4.37, v_num=75, train_loss_step=3.280, train_loss_epoch=4.400]
Epoch 4:   0%|          | 0/1 [00:00&lt;00:00, 16448.25it/s, loss=4.37, v_num=75, train_loss_step=3.280, train_loss_epoch=4.400]
Epoch 5:   0%|          | 0/1 [00:00&lt;00:00, 252.21it/s, loss=4.37, v_num=75, train_loss_step=3.280, train_loss_epoch=4.400]  
Epoch 5: 100%|##########| 1/1 [00:00&lt;00:00, 14.46it/s, loss=4.37, v_num=75, train_loss_step=3.280, train_loss_epoch=4.400] 
Epoch 5: 100%|##########| 1/1 [00:00&lt;00:00, 14.04it/s, loss=4.15, v_num=75, train_loss_step=3.070, train_loss_epoch=3.280]
Epoch 5:   0%|          | 0/1 [00:00&lt;00:00, 18315.74it/s, loss=4.15, v_num=75, train_loss_step=3.070, train_loss_epoch=3.280]
Epoch 6:   0%|          | 0/1 [00:00&lt;00:00, 295.77it/s, loss=4.15, v_num=75, train_loss_step=3.070, train_loss_epoch=3.280]  
Epoch 6: 100%|##########| 1/1 [00:00&lt;00:00, 14.91it/s, loss=4.15, v_num=75, train_loss_step=3.070, train_loss_epoch=3.280] 
Epoch 6: 100%|##########| 1/1 [00:00&lt;00:00, 14.49it/s, loss=4.05, v_num=75, train_loss_step=3.400, train_loss_epoch=3.070]
Epoch 6:   0%|          | 0/1 [00:00&lt;00:00, 17848.10it/s, loss=4.05, v_num=75, train_loss_step=3.400, train_loss_epoch=3.070]
Epoch 7:   0%|          | 0/1 [00:00&lt;00:00, 258.67it/s, loss=4.05, v_num=75, train_loss_step=3.400, train_loss_epoch=3.070]  
Epoch 7: 100%|##########| 1/1 [00:00&lt;00:00, 15.21it/s, loss=4.05, v_num=75, train_loss_step=3.400, train_loss_epoch=3.070] 
Epoch 7: 100%|##########| 1/1 [00:00&lt;00:00, 14.78it/s, loss=3.93, v_num=75, train_loss_step=3.100, train_loss_epoch=3.400]
Epoch 7:   0%|          | 0/1 [00:00&lt;00:00, 20460.02it/s, loss=3.93, v_num=75, train_loss_step=3.100, train_loss_epoch=3.400]
Epoch 8:   0%|          | 0/1 [00:00&lt;00:00, 310.18it/s, loss=3.93, v_num=75, train_loss_step=3.100, train_loss_epoch=3.400]  
Epoch 8: 100%|##########| 1/1 [00:00&lt;00:00, 15.16it/s, loss=3.93, v_num=75, train_loss_step=3.100, train_loss_epoch=3.400] 
Epoch 8: 100%|##########| 1/1 [00:00&lt;00:00, 14.73it/s, loss=3.8, v_num=75, train_loss_step=2.740, train_loss_epoch=3.100] 
Epoch 8:   0%|          | 0/1 [00:00&lt;00:00, 18893.26it/s, loss=3.8, v_num=75, train_loss_step=2.740, train_loss_epoch=3.100]
Epoch 9:   0%|          | 0/1 [00:00&lt;00:00, 272.78it/s, loss=3.8, v_num=75, train_loss_step=2.740, train_loss_epoch=3.100]  
Epoch 9: 100%|##########| 1/1 [00:00&lt;00:00, 15.00it/s, loss=3.8, v_num=75, train_loss_step=2.740, train_loss_epoch=3.100] 
Epoch 9: 100%|##########| 1/1 [00:00&lt;00:00, 14.57it/s, loss=3.7, v_num=75, train_loss_step=2.800, train_loss_epoch=2.740]
Epoch 9: 100%|##########| 1/1 [00:00&lt;00:00, 13.96it/s, loss=3.7, v_num=75, train_loss_step=2.800, train_loss_epoch=2.740]
 50%|█████     | 1/2 [00:06&lt;00:03,  3.19s/trial, best loss: 6.4836297035217285]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  f&#34;The dataloader, {name}, does not have many workers which may be a bottleneck.&#34;

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Predicting: 1it [00:00, ?it/s]
Predicting: 100%|##########| 1/1 [00:00&lt;?, ?it/s]
VAL y_true.shape: (7, 24)
VAL y_hat.shape: (7, 24)
Predicting: 1it [00:00, ?it/s]
Predicting: 100%|##########| 1/1 [00:00&lt;?, ?it/s]
TEST y_true.shape: (7, 24)
TEST y_hat.shape: (7, 24)
100%|██████████| 2/2 [00:06&lt;00:00,  3.06s/trial, best loss: 5.814513683319092]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trials</span><span class="o">.</span><span class="n">trials</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[{&#39;state&#39;: 2,
  &#39;tid&#39;: 0,
  &#39;spec&#39;: None,
  &#39;result&#39;: {&#39;loss&#39;: 6.4836297035217285,
   &#39;mc&#39;: {&#39;activation&#39;: &#39;SELU&#39;,
    &#39;batch_normalization&#39;: False,
    &#39;batch_size&#39;: 256,
    &#39;complete_windows&#39;: False,
    &#39;device&#39;: &#39;cpu&#39;,
    &#39;dropout_prob_exogenous&#39;: 0.38259054148933763,
    &#39;dropout_prob_theta&#39;: 0.29779920353083345,
    &#39;early_stop_patience&#39;: 16,
    &#39;eval_freq&#39;: 50,
    &#39;frequency&#39;: &#39;H&#39;,
    &#39;idx_to_sample_freq&#39;: 24,
    &#39;initialization&#39;: &#39;he_normal&#39;,
    &#39;l1_theta&#39;: 0,
    &#39;learning_rate&#39;: 0.0007869636178342339,
    &#39;loss_hypar&#39;: 0.5,
    &#39;loss_train&#39;: &#39;MAE&#39;,
    &#39;loss_valid&#39;: &#39;MAE&#39;,
    &#39;lr_decay&#39;: 0.4826214429016166,
    &#39;lr_decay_step_size&#39;: 100,
    &#39;max_epochs&#39;: 10,
    &#39;max_steps&#39;: None,
    &#39;mode&#39;: &#39;simple&#39;,
    &#39;model&#39;: &#39;deepmidas&#39;,
    &#39;n_blocks&#39;: (1, 1),
    &#39;n_freq_downsample&#39;: (24, 1),
    &#39;n_hidden&#39;: 256,
    &#39;n_layers&#39;: (2, 2),
    &#39;n_pool_kernel_size&#39;: (4, 1),
    &#39;n_s_hidden&#39;: 0,
    &#39;n_time_in&#39;: 168,
    &#39;n_time_out&#39;: 24,
    &#39;n_x_hidden&#39;: 5.0,
    &#39;normalizer_x&#39;: &#39;median&#39;,
    &#39;normalizer_y&#39;: None,
    &#39;random_seed&#39;: 19.0,
    &#39;seasonality&#39;: 24,
    &#39;shared_weights&#39;: False,
    &#39;stack_types&#39;: (&#39;identity&#39;, &#39;identity&#39;),
    &#39;val_idx_to_sample_freq&#39;: 24,
    &#39;weight_decay&#39;: 0.0021822157334433897,
    &#39;n_x&#39;: 1,
    &#39;n_s&#39;: 1,
    &#39;n_theta_hidden&#39;: [[256, 256], [256, 256]]},
   &#39;run_time&#39;: 3.1427860260009766,
   &#39;status&#39;: &#39;ok&#39;,
   &#39;test_losses&#39;: {&#39;mae&#39;: 3.0701606, &#39;rmse&#39;: 4.2011194},
   &#39;run_forecasts&#39;: {&#39;val_y_true&#39;: array([[44.62, 43.71, 43.34, 43.4 , 44.07, 46.47, 48.84, 51.65, 53.01,
            53.32, 53.23, 53.65, 53.99, 54.35, 56.09, 59.04, 61.75, 62.31,
            58.07, 53.76, 53.19, 51.55, 49.8 , 48.58],
           [48.79, 48.08, 47.81, 47.8 , 48.34, 51.  , 55.2 , 69.53, 75.93,
            73.66, 69.44, 67.42, 66.89, 65.02, 67.18, 74.15, 78.2 , 78.19,
            74.15, 64.04, 53.57, 53.57, 50.91, 49.21],
           [49.57, 48.77, 48.16, 47.86, 48.24, 50.09, 54.84, 67.43, 72.23,
            69.97, 61.79, 58.27, 63.83, 62.6 , 63.84, 72.35, 68.05, 61.68,
            59.  , 54.79, 52.97, 52.67, 50.74, 49.65],
           [49.48, 48.64, 48.11, 48.45, 49.32, 52.41, 57.65, 69.61, 73.23,
            70.83, 63.09, 61.81, 64.07, 60.76, 61.8 , 63.9 , 74.91, 71.79,
            62.88, 56.33, 55.04, 54.43, 53.12, 51.36],
           [53.1 , 51.56, 50.92, 51.06, 50.5 , 50.78, 51.07, 52.22, 53.07,
            53.96, 54.72, 54.97, 53.97, 53.92, 53.97, 53.88, 53.63, 55.41,
            52.88, 51.42, 50.3 , 49.4 , 48.08, 46.47],
           [46.95, 46.87, 46.52, 46.35, 46.31, 46.38, 46.78, 47.27, 48.19,
            48.67, 49.09, 50.55, 51.52, 51.45, 51.71, 52.49, 54.38, 54.76,
            54.5 , 52.94, 52.13, 51.65, 50.67, 49.86],
           [50.41, 49.94, 49.77, 49.05, 49.47, 52.03, 57.19, 70.21, 77.37,
            77.82, 76.64, 77.36, 76.62, 76.62, 77.14, 77.33, 77.37, 77.34,
            76.67, 69.5 , 56.52, 55.14, 53.56, 52.49]], dtype=float32),
    &#39;val_y_hat&#39;: array([[46.292877, 45.809456, 44.517246, 44.370335, 44.50484 , 43.95786 ,
            47.437855, 48.96695 , 50.157528, 50.310585, 49.65537 , 49.460632,
            48.75617 , 49.280853, 48.784573, 47.216614, 47.39984 , 47.79566 ,
            47.962116, 48.53254 , 47.99333 , 47.821285, 46.651405, 46.43819 ],
           [50.440487, 50.260197, 49.134773, 48.598385, 48.898857, 48.484795,
            52.23737 , 54.065594, 54.86892 , 55.379642, 54.42297 , 54.08336 ,
            53.193195, 53.766724, 53.130222, 51.34432 , 51.993595, 52.18864 ,
            52.70723 , 53.208275, 52.42636 , 52.213566, 51.175724, 50.679455],
           [50.974346, 50.970272, 49.79125 , 49.337364, 49.157063, 48.961983,
            52.901802, 54.979004, 55.369064, 56.165234, 55.03769 , 54.652985,
            53.984936, 54.512253, 53.702564, 51.958748, 52.778736, 52.96066 ,
            53.570637, 53.954475, 53.138256, 53.17415 , 51.72167 , 50.91551 ],
           [50.683777, 50.31587 , 49.12396 , 49.76565 , 48.963345, 48.702938,
            52.89323 , 54.48886 , 54.860912, 55.845413, 54.88936 , 54.4816  ,
            54.067265, 54.247734, 53.614212, 51.887356, 53.00425 , 52.376297,
            53.3945  , 53.904327, 52.95192 , 52.867123, 51.41592 , 50.351025],
           [51.93758 , 51.493614, 49.90081 , 50.76373 , 49.616825, 49.58987 ,
            53.66174 , 55.469315, 55.831394, 56.480568, 55.066776, 55.23211 ,
            55.061962, 55.262196, 54.20887 , 53.0508  , 53.76503 , 53.397038,
            54.300217, 54.658775, 54.499187, 53.97856 , 52.336758, 51.343536],
           [47.215237, 46.41577 , 44.76737 , 45.85603 , 44.57414 , 44.46398 ,
            47.955822, 49.587273, 50.53635 , 50.885983, 49.653023, 50.191547,
            49.50689 , 49.6438  , 48.965588, 47.86803 , 48.427864, 48.14459 ,
            49.062435, 49.202477, 49.04812 , 48.576622, 47.454437, 46.68124 ],
           [51.736168, 50.648476, 49.34126 , 49.978382, 49.409992, 49.192078,
            52.607677, 54.02534 , 55.32432 , 55.464016, 54.750244, 54.595398,
            54.235455, 54.616695, 53.696846, 52.69579 , 52.87418 , 52.74857 ,
            53.373398, 54.224236, 53.96314 , 53.260937, 52.11252 , 51.259876]],
          dtype=float32),
    &#39;val_mask&#39;: array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),
    &#39;val_meta_data&#39;: [array([[&#39;NP&#39;, Timestamp(&#39;2013-01-01 00:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2013-01-01 01:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2013-01-01 02:00:00&#39;)],
            ...,
            [&#39;NP&#39;, Timestamp(&#39;2018-12-24 21:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2018-12-24 22:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2018-12-24 23:00:00&#39;)]], dtype=object)],
    &#39;test_y_true&#39;: array([[52.33, 51.02, 50.24, 49.42, 50.18, 52.89, 56.46, 71.25, 74.69,
            72.06, 69.72, 67.63, 65.92, 66.08, 68.05, 69.94, 67.68, 65.96,
            57.86, 55.12, 53.54, 52.55, 50.67, 48.69],
           [49.55, 48.84, 48.18, 47.82, 48.65, 50.6 , 52.14, 54.25, 55.69,
            55.27, 55.03, 55.32, 55.71, 55.68, 55.74, 56.31, 57.19, 56.71,
            56.27, 55.2 , 54.34, 53.22, 51.67, 50.12],
           [48.35, 48.08, 48.31, 47.36, 47.94, 50.62, 54.38, 57.84, 58.63,
            57.53, 56.45, 56.31, 57.39, 57.21, 57.52, 57.81, 57.88, 57.2 ,
            55.7 , 54.31, 52.25, 51.29, 50.  , 48.12],
           [47.21, 46.42, 46.  , 46.12, 46.49, 49.97, 52.73, 55.65, 57.04,
            57.13, 56.98, 56.04, 56.22, 55.78, 54.6 , 54.11, 54.25, 54.69,
            54.21, 52.79, 52.05, 51.43, 50.07, 49.01],
           [48.39, 47.72, 47.23, 46.6 , 46.94, 47.76, 48.41, 49.44, 50.29,
            51.52, 52.56, 53.08, 52.99, 53.13, 52.93, 53.78, 55.04, 56.52,
            57.07, 55.52, 53.05, 52.05, 51.09, 50.47],
           [51.49, 50.83, 50.74, 50.14, 49.94, 50.46, 50.88, 51.37, 51.61,
            52.22, 52.8 , 53.  , 53.11, 52.93, 52.93, 53.75, 55.99, 61.2 ,
            61.2 , 57.42, 55.61, 53.99, 53.86, 52.32],
           [51.09, 50.19, 48.98, 48.8 , 48.52, 49.8 , 50.05, 50.55, 52.33,
            53.26, 53.14, 52.88, 53.03, 52.46, 52.44, 52.89, 53.26, 52.61,
            51.28, 50.72, 49.86, 49.09, 49.02, 48.1 ]], dtype=float32),
    &#39;test_y_hat&#39;: array([[54.498062, 53.963875, 52.802265, 52.54548 , 52.15377 , 51.99901 ,
            55.743492, 57.9606  , 59.232117, 59.631542, 58.20206 , 58.25278 ,
            57.36585 , 58.005882, 57.43746 , 55.591633, 55.681236, 56.125553,
            56.769222, 57.4183  , 56.50161 , 56.275078, 55.19024 , 54.540394],
           [50.862854, 50.3661  , 49.39258 , 49.562874, 49.03361 , 48.658096,
            53.060505, 55.17837 , 55.581375, 56.56272 , 55.078358, 54.998516,
            54.113384, 54.728745, 53.7     , 51.929245, 52.924973, 52.88182 ,
            54.00698 , 54.29088 , 52.869873, 52.88745 , 51.202774, 50.812386],
           [51.96343 , 51.367363, 49.80986 , 49.819862, 50.003284, 50.413227,
            53.962017, 55.782265, 56.514538, 57.234936, 55.979042, 55.355526,
            55.53573 , 55.225372, 54.054558, 52.769016, 53.649128, 53.59691 ,
            54.22144 , 55.16018 , 54.564102, 53.703056, 52.561863, 51.865383],
           [49.691067, 49.23978 , 47.59798 , 47.6978  , 47.803078, 47.479904,
            50.972656, 52.77952 , 53.50872 , 54.193493, 53.290943, 53.034416,
            52.554222, 52.46737 , 51.86604 , 50.215977, 50.703365, 50.79298 ,
            51.52339 , 51.868298, 51.70623 , 51.244915, 50.349445, 49.71837 ],
           [50.495064, 49.38451 , 48.17412 , 49.10283 , 47.935467, 47.367485,
            51.08173 , 52.490147, 53.32482 , 53.658295, 53.298595, 53.297104,
            52.468   , 53.47331 , 52.279343, 51.477146, 51.4318  , 51.476368,
            52.275925, 52.656643, 52.18878 , 51.708557, 50.33871 , 49.870968],
           [52.010605, 50.818928, 49.771732, 49.661545, 49.520054, 49.27632 ,
            52.725395, 53.97213 , 55.721924, 55.381157, 54.643047, 54.42325 ,
            54.4162  , 54.5066  , 53.74236 , 52.22853 , 52.63466 , 53.269672,
            53.530384, 53.728905, 53.27113 , 53.058437, 52.207157, 51.77833 ],
           [54.379074, 54.12967 , 52.76119 , 52.215668, 52.651344, 52.32973 ,
            55.953728, 57.862873, 58.567924, 59.3005  , 58.502987, 57.572243,
            57.500202, 57.523357, 56.760666, 54.950794, 55.456852, 56.172855,
            56.621586, 56.810905, 56.332882, 56.23965 , 55.064064, 54.95643 ]],
          dtype=float32),
    &#39;test_mask&#39;: array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),
    &#39;test_meta_data&#39;: [array([[&#39;NP&#39;, Timestamp(&#39;2013-01-01 00:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2013-01-01 01:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2013-01-01 02:00:00&#39;)],
            ...,
            [&#39;NP&#39;, Timestamp(&#39;2018-12-24 21:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2018-12-24 22:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2018-12-24 23:00:00&#39;)]], dtype=object)]}},
  &#39;misc&#39;: {&#39;tid&#39;: 0,
   &#39;cmd&#39;: (&#39;domain_attachment&#39;, &#39;FMinIter_Domain&#39;),
   &#39;workdir&#39;: None,
   &#39;idxs&#39;: {&#39;activation&#39;: [0],
    &#39;batch_normalization&#39;: [0],
    &#39;batch_size&#39;: [0],
    &#39;complete_windows&#39;: [0],
    &#39;device&#39;: [0],
    &#39;dropout_prob_exogenous&#39;: [0],
    &#39;dropout_prob_theta&#39;: [0],
    &#39;early_stop_patience&#39;: [0],
    &#39;eval_freq&#39;: [0],
    &#39;frequency&#39;: [0],
    &#39;idx_to_sample_freq&#39;: [0],
    &#39;initialization&#39;: [0],
    &#39;l1_theta&#39;: [0],
    &#39;learning_rate&#39;: [0],
    &#39;loss&#39;: [0],
    &#39;loss_hypar&#39;: [0],
    &#39;loss_valid&#39;: [0],
    &#39;lr_decay&#39;: [0],
    &#39;lr_decay_step_size&#39;: [0],
    &#39;max_epochs&#39;: [0],
    &#39;max_steps&#39;: [0],
    &#39;n_blocks&#39;: [0],
    &#39;n_freq_downsample&#39;: [0],
    &#39;n_hidden&#39;: [0],
    &#39;n_layers&#39;: [0],
    &#39;n_pool_kernel_size&#39;: [0],
    &#39;n_s_hidden&#39;: [0],
    &#39;n_time_in&#39;: [0],
    &#39;n_time_out&#39;: [0],
    &#39;n_x_hidden&#39;: [0],
    &#39;normalizer_x&#39;: [0],
    &#39;normalizer_y&#39;: [0],
    &#39;random_seed&#39;: [0],
    &#39;seasonality&#39;: [0],
    &#39;shared_weights&#39;: [0],
    &#39;stack_types&#39;: [0],
    &#39;val_idx_to_sample_freq&#39;: [0],
    &#39;weight_decay&#39;: [0]},
   &#39;vals&#39;: {&#39;activation&#39;: [0],
    &#39;batch_normalization&#39;: [0],
    &#39;batch_size&#39;: [0],
    &#39;complete_windows&#39;: [0],
    &#39;device&#39;: [0],
    &#39;dropout_prob_exogenous&#39;: [0.38259054148933763],
    &#39;dropout_prob_theta&#39;: [0.29779920353083345],
    &#39;early_stop_patience&#39;: [0],
    &#39;eval_freq&#39;: [0],
    &#39;frequency&#39;: [0],
    &#39;idx_to_sample_freq&#39;: [0],
    &#39;initialization&#39;: [1],
    &#39;l1_theta&#39;: [0],
    &#39;learning_rate&#39;: [0.0007869636178342339],
    &#39;loss&#39;: [0],
    &#39;loss_hypar&#39;: [0],
    &#39;loss_valid&#39;: [0],
    &#39;lr_decay&#39;: [0.4826214429016166],
    &#39;lr_decay_step_size&#39;: [0],
    &#39;max_epochs&#39;: [0],
    &#39;max_steps&#39;: [0],
    &#39;n_blocks&#39;: [0],
    &#39;n_freq_downsample&#39;: [0],
    &#39;n_hidden&#39;: [0],
    &#39;n_layers&#39;: [0],
    &#39;n_pool_kernel_size&#39;: [0],
    &#39;n_s_hidden&#39;: [0],
    &#39;n_time_in&#39;: [0],
    &#39;n_time_out&#39;: [0],
    &#39;n_x_hidden&#39;: [5.0],
    &#39;normalizer_x&#39;: [0],
    &#39;normalizer_y&#39;: [0],
    &#39;random_seed&#39;: [19.0],
    &#39;seasonality&#39;: [0],
    &#39;shared_weights&#39;: [0],
    &#39;stack_types&#39;: [0],
    &#39;val_idx_to_sample_freq&#39;: [0],
    &#39;weight_decay&#39;: [0.0021822157334433897]}},
  &#39;exp_key&#39;: None,
  &#39;owner&#39;: None,
  &#39;version&#39;: 0,
  &#39;book_time&#39;: datetime.datetime(2021, 11, 10, 21, 54, 26, 404000),
  &#39;refresh_time&#39;: datetime.datetime(2021, 11, 10, 21, 54, 29, 569000)},
 {&#39;state&#39;: 2,
  &#39;tid&#39;: 1,
  &#39;spec&#39;: None,
  &#39;result&#39;: {&#39;loss&#39;: 5.814513683319092,
   &#39;mc&#39;: {&#39;activation&#39;: &#39;SELU&#39;,
    &#39;batch_normalization&#39;: False,
    &#39;batch_size&#39;: 256,
    &#39;complete_windows&#39;: False,
    &#39;device&#39;: &#39;cpu&#39;,
    &#39;dropout_prob_exogenous&#39;: 0.038662987633889645,
    &#39;dropout_prob_theta&#39;: 0.3306282220179785,
    &#39;early_stop_patience&#39;: 16,
    &#39;eval_freq&#39;: 50,
    &#39;frequency&#39;: &#39;H&#39;,
    &#39;idx_to_sample_freq&#39;: 24,
    &#39;initialization&#39;: &#39;he_normal&#39;,
    &#39;l1_theta&#39;: 0,
    &#39;learning_rate&#39;: 0.0009772468946913485,
    &#39;loss_hypar&#39;: 0.5,
    &#39;loss_train&#39;: &#39;MAE&#39;,
    &#39;loss_valid&#39;: &#39;MAE&#39;,
    &#39;lr_decay&#39;: 0.35478934936311995,
    &#39;lr_decay_step_size&#39;: 100,
    &#39;max_epochs&#39;: 10,
    &#39;max_steps&#39;: None,
    &#39;mode&#39;: &#39;simple&#39;,
    &#39;model&#39;: &#39;deepmidas&#39;,
    &#39;n_blocks&#39;: (1, 1),
    &#39;n_freq_downsample&#39;: (24, 1),
    &#39;n_hidden&#39;: 256,
    &#39;n_layers&#39;: (2, 2),
    &#39;n_pool_kernel_size&#39;: (4, 1),
    &#39;n_s_hidden&#39;: 0,
    &#39;n_time_in&#39;: 168,
    &#39;n_time_out&#39;: 24,
    &#39;n_x_hidden&#39;: 5.0,
    &#39;normalizer_x&#39;: &#39;median&#39;,
    &#39;normalizer_y&#39;: None,
    &#39;random_seed&#39;: 14.0,
    &#39;seasonality&#39;: 24,
    &#39;shared_weights&#39;: False,
    &#39;stack_types&#39;: (&#39;identity&#39;, &#39;identity&#39;),
    &#39;val_idx_to_sample_freq&#39;: 24,
    &#39;weight_decay&#39;: 0.0025304703924177697,
    &#39;n_x&#39;: 1,
    &#39;n_s&#39;: 1,
    &#39;n_theta_hidden&#39;: [[256, 256], [256, 256]]},
   &#39;run_time&#39;: 2.897136926651001,
   &#39;status&#39;: &#39;ok&#39;,
   &#39;test_losses&#39;: {&#39;mae&#39;: 3.173148, &#39;rmse&#39;: 4.189379},
   &#39;run_forecasts&#39;: {&#39;val_y_true&#39;: array([[44.62, 43.71, 43.34, 43.4 , 44.07, 46.47, 48.84, 51.65, 53.01,
            53.32, 53.23, 53.65, 53.99, 54.35, 56.09, 59.04, 61.75, 62.31,
            58.07, 53.76, 53.19, 51.55, 49.8 , 48.58],
           [48.79, 48.08, 47.81, 47.8 , 48.34, 51.  , 55.2 , 69.53, 75.93,
            73.66, 69.44, 67.42, 66.89, 65.02, 67.18, 74.15, 78.2 , 78.19,
            74.15, 64.04, 53.57, 53.57, 50.91, 49.21],
           [49.57, 48.77, 48.16, 47.86, 48.24, 50.09, 54.84, 67.43, 72.23,
            69.97, 61.79, 58.27, 63.83, 62.6 , 63.84, 72.35, 68.05, 61.68,
            59.  , 54.79, 52.97, 52.67, 50.74, 49.65],
           [49.48, 48.64, 48.11, 48.45, 49.32, 52.41, 57.65, 69.61, 73.23,
            70.83, 63.09, 61.81, 64.07, 60.76, 61.8 , 63.9 , 74.91, 71.79,
            62.88, 56.33, 55.04, 54.43, 53.12, 51.36],
           [53.1 , 51.56, 50.92, 51.06, 50.5 , 50.78, 51.07, 52.22, 53.07,
            53.96, 54.72, 54.97, 53.97, 53.92, 53.97, 53.88, 53.63, 55.41,
            52.88, 51.42, 50.3 , 49.4 , 48.08, 46.47],
           [46.95, 46.87, 46.52, 46.35, 46.31, 46.38, 46.78, 47.27, 48.19,
            48.67, 49.09, 50.55, 51.52, 51.45, 51.71, 52.49, 54.38, 54.76,
            54.5 , 52.94, 52.13, 51.65, 50.67, 49.86],
           [50.41, 49.94, 49.77, 49.05, 49.47, 52.03, 57.19, 70.21, 77.37,
            77.82, 76.64, 77.36, 76.62, 76.62, 77.14, 77.33, 77.37, 77.34,
            76.67, 69.5 , 56.52, 55.14, 53.56, 52.49]], dtype=float32),
    &#39;val_y_hat&#39;: array([[45.74769 , 44.760956, 44.796494, 44.397514, 45.23768 , 47.046993,
            47.83739 , 50.29268 , 52.17981 , 50.844467, 51.067627, 52.030533,
            50.77355 , 50.45314 , 50.085148, 49.041206, 49.043247, 49.99334 ,
            49.677807, 50.52954 , 49.99665 , 49.18119 , 48.409782, 47.04438 ],
           [49.899834, 49.26437 , 49.14279 , 48.89493 , 49.678703, 51.36877 ,
            52.065098, 54.570004, 56.34672 , 55.1538  , 55.71962 , 56.35843 ,
            55.08295 , 54.885838, 54.348522, 53.30954 , 53.98083 , 54.820663,
            54.135162, 54.835876, 54.547203, 53.429928, 52.965748, 50.94576 ],
           [50.70067 , 49.92371 , 50.190205, 49.58713 , 50.736313, 52.48966 ,
            52.87592 , 55.384995, 57.037556, 55.72293 , 57.030155, 57.554317,
            56.04649 , 56.097183, 54.770557, 54.075798, 55.117657, 55.802895,
            54.91976 , 55.454586, 55.41101 , 54.016323, 53.792656, 51.357338],
           [50.459915, 49.265476, 50.22779 , 49.232834, 50.14183 , 51.824017,
            52.921135, 54.76953 , 56.344784, 55.84156 , 56.035397, 56.95729 ,
            55.94393 , 55.897068, 54.179165, 53.796925, 53.840725, 55.542286,
            55.022938, 55.12386 , 55.045876, 53.549614, 53.559914, 51.301563],
           [51.646317, 50.517616, 51.461178, 50.24439 , 51.21954 , 53.211967,
            54.12253 , 56.09706 , 57.580807, 56.923725, 57.49653 , 58.36562 ,
            56.93299 , 57.08332 , 55.336224, 54.997158, 55.113583, 56.692135,
            56.26584 , 56.29948 , 56.44805 , 54.52821 , 54.562572, 53.28968 ],
           [47.08339 , 45.632484, 46.88327 , 45.640522, 46.314354, 48.253452,
            49.50792 , 51.54476 , 53.49311 , 52.71562 , 52.520996, 53.81446 ,
            52.955296, 52.353065, 50.966427, 50.86141 , 49.67104 , 52.37226 ,
            51.725277, 52.25246 , 51.649185, 50.55403 , 49.817226, 49.017536],
           [51.011753, 49.533607, 50.209347, 49.565258, 50.025246, 52.252743,
            53.55553 , 55.649746, 58.015163, 56.38468 , 56.681625, 57.93226 ,
            56.458298, 56.113953, 55.072937, 54.64654 , 53.929634, 55.546585,
            55.29352 , 56.028786, 55.787086, 54.394142, 53.741703, 53.210423]],
          dtype=float32),
    &#39;val_mask&#39;: array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),
    &#39;val_meta_data&#39;: [array([[&#39;NP&#39;, Timestamp(&#39;2013-01-01 00:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2013-01-01 01:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2013-01-01 02:00:00&#39;)],
            ...,
            [&#39;NP&#39;, Timestamp(&#39;2018-12-24 21:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2018-12-24 22:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2018-12-24 23:00:00&#39;)]], dtype=object)],
    &#39;test_y_true&#39;: array([[52.33, 51.02, 50.24, 49.42, 50.18, 52.89, 56.46, 71.25, 74.69,
            72.06, 69.72, 67.63, 65.92, 66.08, 68.05, 69.94, 67.68, 65.96,
            57.86, 55.12, 53.54, 52.55, 50.67, 48.69],
           [49.55, 48.84, 48.18, 47.82, 48.65, 50.6 , 52.14, 54.25, 55.69,
            55.27, 55.03, 55.32, 55.71, 55.68, 55.74, 56.31, 57.19, 56.71,
            56.27, 55.2 , 54.34, 53.22, 51.67, 50.12],
           [48.35, 48.08, 48.31, 47.36, 47.94, 50.62, 54.38, 57.84, 58.63,
            57.53, 56.45, 56.31, 57.39, 57.21, 57.52, 57.81, 57.88, 57.2 ,
            55.7 , 54.31, 52.25, 51.29, 50.  , 48.12],
           [47.21, 46.42, 46.  , 46.12, 46.49, 49.97, 52.73, 55.65, 57.04,
            57.13, 56.98, 56.04, 56.22, 55.78, 54.6 , 54.11, 54.25, 54.69,
            54.21, 52.79, 52.05, 51.43, 50.07, 49.01],
           [48.39, 47.72, 47.23, 46.6 , 46.94, 47.76, 48.41, 49.44, 50.29,
            51.52, 52.56, 53.08, 52.99, 53.13, 52.93, 53.78, 55.04, 56.52,
            57.07, 55.52, 53.05, 52.05, 51.09, 50.47],
           [51.49, 50.83, 50.74, 50.14, 49.94, 50.46, 50.88, 51.37, 51.61,
            52.22, 52.8 , 53.  , 53.11, 52.93, 52.93, 53.75, 55.99, 61.2 ,
            61.2 , 57.42, 55.61, 53.99, 53.86, 52.32],
           [51.09, 50.19, 48.98, 48.8 , 48.52, 49.8 , 50.05, 50.55, 52.33,
            53.26, 53.14, 52.88, 53.03, 52.46, 52.44, 52.89, 53.26, 52.61,
            51.28, 50.72, 49.86, 49.09, 49.02, 48.1 ]], dtype=float32),
    &#39;test_y_hat&#39;: array([[53.99761 , 53.02463 , 53.51192 , 52.956966, 54.32635 , 56.24719 ,
            56.018456, 59.42499 , 61.56162 , 59.342964, 60.712185, 61.617817,
            60.059174, 59.745686, 59.202442, 58.584583, 58.26781 , 59.35592 ,
            58.81104 , 59.56843 , 58.897778, 58.060154, 57.07128 , 55.242332],
           [50.96232 , 49.524246, 50.500183, 50.113056, 50.642483, 52.203354,
            52.938206, 56.107143, 58.23969 , 56.847443, 57.611645, 57.936752,
            57.355354, 56.325043, 55.455635, 54.521515, 55.194885, 56.723072,
            55.668983, 56.77734 , 55.914192, 55.0504  , 54.78996 , 51.807396],
           [51.631435, 50.42228 , 51.030346, 50.039455, 50.95452 , 53.15246 ,
            53.76111 , 57.055195, 59.256474, 57.196053, 58.70356 , 59.002388,
            57.604042, 57.40393 , 56.22746 , 55.20829 , 56.300236, 57.36135 ,
            56.597218, 57.06897 , 57.4838  , 55.369358, 55.260044, 53.477722],
           [48.891033, 47.61571 , 48.686813, 47.449276, 48.573143, 50.367897,
            50.810207, 53.79646 , 56.276848, 54.367363, 54.94339 , 55.76675 ,
            54.914047, 54.130802, 53.06208 , 53.298363, 51.990814, 54.28035 ,
            53.5298  , 54.225155, 54.02257 , 52.883522, 51.77271 , 51.009296],
           [49.90067 , 48.45827 , 49.16942 , 48.649017, 49.152058, 51.091034,
            52.17648 , 53.824196, 55.581413, 54.792843, 54.718605, 55.78093 ,
            54.8675  , 54.10603 , 53.28904 , 53.06069 , 52.090042, 54.344387,
            54.13526 , 54.30397 , 53.594254, 52.99896 , 52.246624, 51.542576],
           [51.475822, 50.43576 , 50.21133 , 49.89577 , 50.854206, 52.168777,
            53.2521  , 55.919384, 57.99317 , 56.20019 , 56.78317 , 57.386   ,
            56.473145, 55.678753, 55.82368 , 54.16653 , 55.032578, 56.114403,
            55.948055, 56.253597, 55.60811 , 54.724197, 54.463177, 52.69032 ],
           [53.96929 , 53.066574, 53.16184 , 52.674007, 53.55371 , 55.097466,
            55.596516, 58.94163 , 61.610516, 59.085884, 60.16968 , 60.587677,
            59.565495, 58.99388 , 58.487476, 57.687775, 58.105194, 59.08411 ,
            58.395306, 59.3259  , 59.009483, 57.926838, 57.22373 , 55.27079 ]],
          dtype=float32),
    &#39;test_mask&#39;: array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
            1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),
    &#39;test_meta_data&#39;: [array([[&#39;NP&#39;, Timestamp(&#39;2013-01-01 00:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2013-01-01 01:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2013-01-01 02:00:00&#39;)],
            ...,
            [&#39;NP&#39;, Timestamp(&#39;2018-12-24 21:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2018-12-24 22:00:00&#39;)],
            [&#39;NP&#39;, Timestamp(&#39;2018-12-24 23:00:00&#39;)]], dtype=object)]}},
  &#39;misc&#39;: {&#39;tid&#39;: 1,
   &#39;cmd&#39;: (&#39;domain_attachment&#39;, &#39;FMinIter_Domain&#39;),
   &#39;workdir&#39;: None,
   &#39;idxs&#39;: {&#39;activation&#39;: [1],
    &#39;batch_normalization&#39;: [1],
    &#39;batch_size&#39;: [1],
    &#39;complete_windows&#39;: [1],
    &#39;device&#39;: [1],
    &#39;dropout_prob_exogenous&#39;: [1],
    &#39;dropout_prob_theta&#39;: [1],
    &#39;early_stop_patience&#39;: [1],
    &#39;eval_freq&#39;: [1],
    &#39;frequency&#39;: [1],
    &#39;idx_to_sample_freq&#39;: [1],
    &#39;initialization&#39;: [1],
    &#39;l1_theta&#39;: [1],
    &#39;learning_rate&#39;: [1],
    &#39;loss&#39;: [1],
    &#39;loss_hypar&#39;: [1],
    &#39;loss_valid&#39;: [1],
    &#39;lr_decay&#39;: [1],
    &#39;lr_decay_step_size&#39;: [1],
    &#39;max_epochs&#39;: [1],
    &#39;max_steps&#39;: [1],
    &#39;n_blocks&#39;: [1],
    &#39;n_freq_downsample&#39;: [1],
    &#39;n_hidden&#39;: [1],
    &#39;n_layers&#39;: [1],
    &#39;n_pool_kernel_size&#39;: [1],
    &#39;n_s_hidden&#39;: [1],
    &#39;n_time_in&#39;: [1],
    &#39;n_time_out&#39;: [1],
    &#39;n_x_hidden&#39;: [1],
    &#39;normalizer_x&#39;: [1],
    &#39;normalizer_y&#39;: [1],
    &#39;random_seed&#39;: [1],
    &#39;seasonality&#39;: [1],
    &#39;shared_weights&#39;: [1],
    &#39;stack_types&#39;: [1],
    &#39;val_idx_to_sample_freq&#39;: [1],
    &#39;weight_decay&#39;: [1]},
   &#39;vals&#39;: {&#39;activation&#39;: [0],
    &#39;batch_normalization&#39;: [0],
    &#39;batch_size&#39;: [0],
    &#39;complete_windows&#39;: [0],
    &#39;device&#39;: [0],
    &#39;dropout_prob_exogenous&#39;: [0.038662987633889645],
    &#39;dropout_prob_theta&#39;: [0.3306282220179785],
    &#39;early_stop_patience&#39;: [0],
    &#39;eval_freq&#39;: [0],
    &#39;frequency&#39;: [0],
    &#39;idx_to_sample_freq&#39;: [0],
    &#39;initialization&#39;: [1],
    &#39;l1_theta&#39;: [0],
    &#39;learning_rate&#39;: [0.0009772468946913485],
    &#39;loss&#39;: [0],
    &#39;loss_hypar&#39;: [0],
    &#39;loss_valid&#39;: [0],
    &#39;lr_decay&#39;: [0.35478934936311995],
    &#39;lr_decay_step_size&#39;: [0],
    &#39;max_epochs&#39;: [0],
    &#39;max_steps&#39;: [0],
    &#39;n_blocks&#39;: [0],
    &#39;n_freq_downsample&#39;: [0],
    &#39;n_hidden&#39;: [0],
    &#39;n_layers&#39;: [0],
    &#39;n_pool_kernel_size&#39;: [0],
    &#39;n_s_hidden&#39;: [0],
    &#39;n_time_in&#39;: [0],
    &#39;n_time_out&#39;: [0],
    &#39;n_x_hidden&#39;: [5.0],
    &#39;normalizer_x&#39;: [0],
    &#39;normalizer_y&#39;: [0],
    &#39;random_seed&#39;: [14.0],
    &#39;seasonality&#39;: [0],
    &#39;shared_weights&#39;: [0],
    &#39;stack_types&#39;: [0],
    &#39;val_idx_to_sample_freq&#39;: [0],
    &#39;weight_decay&#39;: [0.0025304703924177697]}},
  &#39;exp_key&#39;: None,
  &#39;owner&#39;: None,
  &#39;version&#39;: 0,
  &#39;book_time&#39;: datetime.datetime(2021, 11, 10, 21, 54, 29, 592000),
  &#39;refresh_time&#39;: datetime.datetime(2021, 11, 10, 21, 54, 32, 505000)}]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trials</span><span class="o">.</span><span class="n">trials</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;result&#39;</span><span class="p">][</span><span class="s1">&#39;test_losses&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;mae&#39;: 3.0701606, &#39;rmse&#39;: 4.2011194}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

