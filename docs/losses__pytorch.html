---

title: PyTorch Losses


keywords: fastai
sidebar: home_sidebar

summary: "Training losses."
description: "Training losses."
nb_path: "nbs/losses__pytorch.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/losses__pytorch.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mean-Absolute-Percentage-Error">Mean Absolute Percentage Error<a class="anchor-link" href="#Mean-Absolute-Percentage-Error"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MAPELoss" class="doc_header"><code>MAPELoss</code><a href="https://github.com/Nixtla/neuralforecast/tree/main/neuralforecast/losses/pytorch.py#L21" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MAPELoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Calculates Mean Absolute Percentage Error (MAPE) between
y and y_hat. MAPE measures the relative prediction
accuracy of a forecasting method by calculating the
percentual deviation of the prediction and the true
value at a given time and averages these devations
over the length of the series.
As defined in: <a href="https://en.wikipedia.org/wiki/Mean_absolute_percentage_error">https://en.wikipedia.org/wiki/Mean_absolute_percentage_error</a></p>

<pre><code>Parameters
----------
y: tensor (batch_size, output_size)
    actual values in torch tensor.
y_hat: tensor (batch_size, output_size)
    predicted values in torch tensor.
mask: tensor (batch_size, output_size)
    specifies date stamps per serie
    to consider in loss

Returns
-------
mape:
Mean absolute percentage error.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mean-Squared-Error">Mean Squared Error<a class="anchor-link" href="#Mean-Squared-Error"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MSELoss" class="doc_header"><code>MSELoss</code><a href="https://github.com/Nixtla/neuralforecast/tree/main/neuralforecast/losses/pytorch.py#L55" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MSELoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Calculates Mean Squared Error (MSE) between
y and y_hat. MAPE measures the relative prediction
accuracy of a forecasting method by calculating the
percentual deviation of the prediction and the true
value at a given time and averages these devations
over the length of the series.</p>

<pre><code>Parameters
----------
y: tensor (batch_size, output_size)
    actual values in torch tensor.
y_hat: tensor (batch_size, output_size)
    predicted values in torch tensor.
mask: tensor (batch_size, output_size)
    specifies date stamps per serie
    to consider in loss

Returns
-------
mse:
Mean Squared Error.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Root-Mean-Squared-Error">Root Mean Squared Error<a class="anchor-link" href="#Root-Mean-Squared-Error"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="RMSELoss" class="doc_header"><code>RMSELoss</code><a href="https://github.com/Nixtla/neuralforecast/tree/main/neuralforecast/losses/pytorch.py#L88" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>RMSELoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Calculates Root Mean Squared Error (RMSE) between
y and y_hat. MAPE measures the relative prediction
accuracy of a forecasting method by calculating the
percentual deviation of the prediction and the true
value at a given time and averages these devations
over the length of the series.</p>

<pre><code>Parameters
----------
y: tensor (batch_size, output_size)
    actual values in torch tensor.
y_hat: tensor (batch_size, output_size)
    predicted values in torch tensor.
mask: tensor (batch_size, output_size)
    specifies date stamps per serie
    to consider in loss

Returns
-------
rmse:
Root Mean Squared Error.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Symmetric-Mean-Absolute-Percentage-Error">Symmetric Mean Absolute Percentage Error<a class="anchor-link" href="#Symmetric-Mean-Absolute-Percentage-Error"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="SMAPELoss" class="doc_header"><code>SMAPELoss</code><a href="https://github.com/Nixtla/neuralforecast/tree/main/neuralforecast/losses/pytorch.py#L121" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>SMAPELoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Calculates Symmetric Mean Absolute Percentage Error (sMAPE2).
sMAPE measures the relative prediction accuracy of a
forecasting method by calculating the relative deviation
of the prediction and the true value scaled by the sum of the
absolute values for the prediction and true value at a
given time, then averages these devations over the length
of the series. This allows the SMAPE to have bounds between
0% and 200% which is desireble compared to normal MAPE that
may be undetermined.</p>

<pre><code>Parameters
----------
y: tensor (batch_size, output_size)
    actual values in torch tensor.
y_hat: tensor (batch_size, output_size)
    predicted values in torch tensor.

Returns
-------
smape:
    symmetric mean absolute percentage error

References
----------
[1] https://robjhyndman.com/hyndsight/smape/ (Makridakis 1993)</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mean-Absolute-Scaled-Error">Mean Absolute Scaled Error<a class="anchor-link" href="#Mean-Absolute-Scaled-Error"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MASELoss" class="doc_header"><code>MASELoss</code><a href="https://github.com/Nixtla/neuralforecast/tree/main/neuralforecast/losses/pytorch.py#L160" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MASELoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>y_insample</code></strong>, <strong><code>seasonality</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Calculates the M4 Mean Absolute Scaled Error (MASE).
MASE measures the relative prediction accuracy of a
forecasting method by comparinng the mean absolute errors
of the prediction and the true value against the mean
absolute errors of the seasonal naive model.</p>

<pre><code>Parameters
----------
seasonality: int
    main frequency of the time series
    Hourly 24,  Daily 7, Weekly 52,
    Monthly 12, Quarterly 4, Yearly 1
y: tensor (batch_size, output_size)
    actual test values
y_hat: tensor (batch_size, output_size)
    predicted values
y_train: tensor (batch_size, input_size)
    actual insample values for Seasonal Naive predictions

Returns
-------
mase:
    mean absolute scaled error

References
----------
[1] https://robjhyndman.com/papers/mase.pdf</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mean-Absolute-Error">Mean Absolute Error<a class="anchor-link" href="#Mean-Absolute-Error"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MAELoss" class="doc_header"><code>MAELoss</code><a href="https://github.com/Nixtla/neuralforecast/tree/main/neuralforecast/losses/pytorch.py#L202" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MAELoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Calculates Mean Absolute Error (MAE) between
y and y_hat. MAE measures the relative prediction
accuracy of a forecasting method by calculating the
deviation of the prediction and the true
value at a given time and averages these devations
over the length of the series.</p>

<pre><code>Parameters
----------
y: tensor (batch_size, output_size)
    actual values in torch tensor.
y_hat: tensor (batch_size, output_size)
    predicted values in torch tensor.
mask: tensor (batch_size, output_size)
    specifies date stamps per serie
    to consider in loss

Returns
-------
mae:
Mean absolute error.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Quantile-Loss">Quantile Loss<a class="anchor-link" href="#Quantile-Loss"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="QuantileLoss" class="doc_header"><code>QuantileLoss</code><a href="https://github.com/Nixtla/neuralforecast/tree/main/neuralforecast/losses/pytorch.py#L234" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>QuantileLoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>, <strong><code>q</code></strong>=<em><code>0.5</code></em>)</p>
</blockquote>
<p>Computes the quantile loss (QL) between y and y_hat.</p>

<pre><code>Parameters
----------
y: tensor (batch_size, output_size)
    actual values in torch tensor.
y_hat: tensor (batch_size, output_size)
    predicted values in torch tensor.
q: float, between 0 and 1.
    The slope of the quantile loss, in the context of
    quantile regression, the q determines the conditional
    quantile level.

Returns
-------
quantile_loss:
    average accuracy for the predicted quantile</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ES-RNN-PyTorch-Loss">ES-RNN PyTorch Loss<a class="anchor-link" href="#ES-RNN-PyTorch-Loss"> </a></h2><p>The M4 competition winner, the exponential smoothing recurrent neural network
combines the Holt-Winter method for the seasonal and levels 
with a dilated Recurrent neural network</p>
$$
\begin{align}
    \text{Level:}            &amp; \quad l_{\tau} = \text{median}(y_{\tau})\\
    \text{Residual:}         &amp; \quad z_{\tau} = y_{\tau} - l_{\tau} \\
    \text{NN Forecast:}      &amp; \quad \hat{z}_{\tau} = \text{DRNN}(z_{\tau}, x_{\tau}, s_{\tau})\\
    \text{Level Forecast:}   &amp; \quad \hat{l}_{\tau} = \text{Naive}(l_{\tau})\\
    \text{Forecast:}         &amp; \quad \hat{y}_{\tau+H} = \hat{l}_{\tau+H} + \hat{z}_{\tau+h} 
\end{align}
$$
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="LevelVariabilityLoss" class="doc_header"><code>LevelVariabilityLoss</code><a href="https://github.com/Nixtla/neuralforecast/tree/main/neuralforecast/losses/pytorch.py#L264" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>LevelVariabilityLoss</code>(<strong><code>levels</code></strong>, <strong><code>level_variability_penalty</code></strong>)</p>
</blockquote>
<p>Computes the variability penalty for the level of the ES-RNN.
The levels of the ES-RNN are based on the Holt-Winters model.</p>
$$  Penalty = \lambda * (\hat{l}_{τ+1}-\hat{l}_{τ})^{2} $$
<pre><code>Parameters
----------
levels: tensor with shape (batch, n_time)
    levels obtained from exponential smoothing component of ESRNN
level_variability_penalty: float
    this parameter controls the strength of the penalization
    to the wigglines of the level vector, induces smoothness
    in the output

Returns
----------
level_var_loss:
    wiggliness loss for the level vector</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="SmylLoss" class="doc_header"><code>SmylLoss</code><a href="https://github.com/Nixtla/neuralforecast/tree/main/neuralforecast/losses/pytorch.py#L299" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>SmylLoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>levels</code></strong>, <strong><code>mask</code></strong>, <strong><code>tau</code></strong>, <strong><code>level_variability_penalty</code></strong>=<em><code>0.0</code></em>)</p>
</blockquote>
<p>Computes the Smyl Loss that combines level
variability regularization with with Quantile loss.</p>

<pre><code>Parameters
----------
windows_y: tensor of actual values,
                        shape (n_windows, batch_size, window_size).
windows_y_hat: tensor of predicted values,
                                shape (n_windows, batch_size, window_size).
levels: levels obtained from exponential smoothing component of ESRNN.
                tensor with shape (batch, n_time).

Returns
----------
return: smyl_loss.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Multi-Quantile-Loss">Multi-Quantile Loss<a class="anchor-link" href="#Multi-Quantile-Loss"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MQLoss" class="doc_header"><code>MQLoss</code><a href="https://github.com/Nixtla/neuralforecast/tree/main/neuralforecast/losses/pytorch.py#L330" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MQLoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>quantiles</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Calculates Average Multi-Quantile Loss (MQL), for
a given set of quantiles, based on the absolute
difference between predicted and true values.</p>

<pre><code>Parameters
----------
y: tensor (batch_size, output_size) actual values in torch tensor.
y_hat: tensor (batch_size, output_size, n_quantiles) predicted values in torch tensor.
mask: tensor (batch_size, output_size, n_quantiles) specifies date stamps per serie
      to consider in loss
quantiles: tensor(n_quantiles) quantiles to estimate from the distribution of y.

Returns
-------
mqloss: tensor(n_quantiles) average multi-quantile loss.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Weighted-Multi-Quantile-Loss">Weighted Multi-Quantile Loss<a class="anchor-link" href="#Weighted-Multi-Quantile-Loss"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="wMQLoss" class="doc_header"><code>wMQLoss</code><a href="https://github.com/Nixtla/neuralforecast/tree/main/neuralforecast/losses/pytorch.py#L363" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>wMQLoss</code>(<strong><code>y</code></strong>, <strong><code>y_hat</code></strong>, <strong><code>quantiles</code></strong>, <strong><code>mask</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Calculates weighted Multi-quantile Loss (wMQL), for
a given set of quantiles, based on the absolute
difference between predicted and true values.</p>

<pre><code>Parameters
----------
y: tensor (batch_size, output_size) actual values in torch tensor.
y_hat: tensor (batch_size, output_size, n_quantiles) predicted values in torch tensor.
mask: tensor (batch_size, output_size, n_quantiles) specifies date stamps per serie
      to consider in loss
quantiles: tensor(n_quantiles) quantiles to estimate from the distribution of y.

Returns
-------
wmqloss: tensor(n_quantiles) average multi-quantile loss.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Checks-for-PyTorch-train-losses">Checks for PyTorch train losses<a class="anchor-link" href="#Checks-for-PyTorch-train-losses"> </a></h1>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">hmean</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>  

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">horizon</span><span class="p">,</span> <span class="n">n_quantiles</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">horizon</span> <span class="o">=</span> <span class="n">horizon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles</span> <span class="o">=</span> <span class="n">n_quantiles</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">n_obs</span><span class="p">,</span> 
                                      <span class="n">out_features</span><span class="o">=</span><span class="n">horizon</span> <span class="o">*</span> <span class="n">n_quantiles</span><span class="p">,</span> 
                                      <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">horizon</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_quantiles</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_hat</span>
    
<span class="k">class</span> <span class="nc">Data</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    
    <span class="c1"># Constructor</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Getter</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    
    <span class="c1"># Get Length</span>
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">len</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>

<span class="c1"># Sample data</span>
<span class="n">n_ts</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n_obs</span> <span class="o">=</span> <span class="n">horizon</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">mean</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># to generate random numbers from N(mean, std)</span>
<span class="n">std</span> <span class="o">=</span> <span class="mf">7.0</span> <span class="c1"># to generate random numbers from N(mean, std)</span>
<span class="n">start</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="c1"># First quantile</span>
<span class="n">end</span> <span class="o">=</span> <span class="mf">0.95</span> <span class="c1"># Last quantiles</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># Number of quantiles</span>

<span class="c1"># Hyperparameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.08</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Sample data</span>
<span class="n">quantiles</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.0500</span><span class="p">,</span> <span class="mf">0.3500</span><span class="p">,</span> <span class="mf">0.6500</span><span class="p">,</span> <span class="mf">0.9500</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;quantiles:</span><span class="se">\n</span><span class="si">{</span><span class="n">quantiles</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_ts</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_ts</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">))</span>

<span class="n">Y_test</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_ts</span><span class="p">,</span> <span class="n">horizon</span><span class="p">))</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_ts</span><span class="p">,</span> <span class="n">horizon</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Y.shape: </span><span class="si">{</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, X.shape: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Y_test.shape: </span><span class="si">{</span><span class="n">Y_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, X_test.shape: </span><span class="si">{</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>quantiles:
tensor([0.0500, 0.3500, 0.6500, 0.9500])
Y.shape: torch.Size([1000, 10]), X.shape: torch.Size([1000, 10])
Y_test.shape: torch.Size([1000, 10]), X_test.shape: torch.Size([1000, 10])
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">horizon</span><span class="o">=</span><span class="n">horizon</span><span class="p">,</span> <span class="n">n_quantiles</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">Data</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">print_progress</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> 
    <span class="n">training_trajectory</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="p">[],</span>
                           <span class="s1">&#39;train_loss&#39;</span><span class="p">:</span> <span class="p">[]}</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="c1">#training_loss = wMQLoss(y=y, y_hat=y_hat, quantiles=quantiles)</span>
            <span class="n">training_loss</span> <span class="o">=</span> <span class="n">MQLoss</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="o">=</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="n">quantiles</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> 
                <span class="n">training_trajectory</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="n">training_trajectory</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">training_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="n">display_string</span> <span class="o">=</span> <span class="s1">&#39;Step: </span><span class="si">{}</span><span class="s1">, Time: </span><span class="si">{:03.3f}</span><span class="s1">, Insample </span><span class="si">{}</span><span class="s1">: </span><span class="si">{:.5f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> 
                                                                                    <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start</span><span class="p">,</span> 
                                                                                    <span class="s2">&quot;MQLoss&quot;</span><span class="p">,</span> 
                                                                                    <span class="n">training_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">print_progress</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="n">display_string</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">training_trajectory</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

