{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common._base_recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseRecurrent\n",
    "\n",
    "> The `BaseRecurrent` class contains standard methods shared across recurrent neural networks; these models possess the ability to process variable-length sequences of inputs through their internal memory states. The class is represented by `LSTM`, `GRU`, and `RNN`, along with other more sophisticated architectures.<br><br>The standard methods include data preprocessing `_normalization`, optimization utilities like parameter initialization, `training_step`, `validation_step`, and shared `fit` and `predict` methods.These shared methods enable all the `neuralforecast.models` compatibility with the `core.NeuralForecast` wrapper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "from neuralforecast.tsdataset import TimeSeriesDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseRecurrent(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 loss,\n",
    "                 batch_size=32,\n",
    "                 futr_exog_list=None,\n",
    "                 hist_exog_list=None,\n",
    "                 stat_exog_list=None,\n",
    "                 num_workers_loader=0,\n",
    "                 drop_last_loader=False,\n",
    "                 random_seed=1, \n",
    "                 **trainer_kwargs):\n",
    "        super(BaseRecurrent, self).__init__()\n",
    "        self.random_seed = random_seed\n",
    "        pl.seed_everything(self.random_seed, workers=True)\n",
    "        #fit arguments\n",
    "        self.val_size: int = 0\n",
    "        self.test_size: int = 0\n",
    "        #predict arguments\n",
    "        self.step_size: int = 1\n",
    "        \n",
    "        # Trainer\n",
    "        # we need to instantiate the trainer each time we want to use it\n",
    "        self.trainer_kwargs = {**trainer_kwargs}\n",
    "        if self.trainer_kwargs.get('callbacks', None) is None:\n",
    "            self.trainer_kwargs = {**{'callbacks': [TQDMProgressBar()], **trainer_kwargs}}\n",
    "        else:\n",
    "            self.trainer_kwargs = trainer_kwargs\n",
    "\n",
    "        # Add GPU accelerator if available\n",
    "        if self.trainer_kwargs.get('accelerator', None) is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.trainer_kwargs['accelerator'] = \"gpu\"\n",
    "        if self.trainer_kwargs.get('devices', None) is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.trainer_kwargs['devices'] = -1\n",
    "\n",
    "        # Variables\n",
    "        self.futr_exog_list = futr_exog_list if futr_exog_list is not None else []\n",
    "        self.hist_exog_list = hist_exog_list if hist_exog_list is not None else []\n",
    "        self.stat_exog_list = stat_exog_list if stat_exog_list is not None else []\n",
    "        \n",
    "        # DataModule arguments\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers_loader = num_workers_loader\n",
    "        self.drop_last_loader = drop_last_loader\n",
    "    \n",
    "    def on_fit_start(self):\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def _normalization(self, batch):\n",
    "        temporal = batch['temporal']\n",
    "        temporal_cols = batch['temporal_cols']\n",
    "\n",
    "        # Remove validation and test set to prevent leakeage\n",
    "        if self.val_size + self.test_size > 0:\n",
    "                cutoff = -self.val_size - self.test_size\n",
    "                temporal = temporal[:, :, :cutoff]\n",
    "\n",
    "        temporal_y = temporal[:, temporal_cols.get_loc('y'), :]\n",
    "        temporal_mask = temporal[:, temporal_cols.get_loc('available_mask'), :]\n",
    "\n",
    "        y_means = torch.sum(temporal_y * temporal_mask, dim=-1, keepdim=True) / torch.sum(temporal_mask, dim=-1, keepdim=True)\n",
    "        y_stds = torch.sqrt(torch.sum(temporal_mask*(temporal_y-y_means)**2, dim=-1, keepdim=True)/ \\\n",
    "                            torch.sum(temporal_mask, dim=-1, keepdim=True) )\n",
    "\n",
    "        temporal_y = (temporal_y - y_means) / y_stds\n",
    "        temporal[:, temporal_cols.get_loc('y'), :] = temporal_y\n",
    "        \n",
    "        return batch, y_means, y_stds\n",
    "\n",
    "    def _inv_normalization(self, y_hat, y_means, y_stds):\n",
    "        return y_stds[:, None]*y_hat + y_means[:, None]\n",
    "\n",
    "    def _create_windows(self, batch, step):\n",
    "        temporal = batch['temporal']\n",
    "        temporal_cols = batch['temporal_cols']\n",
    "\n",
    "        assert temporal.shape[-1] % self.step_size == 0, f'Temporal length of batch should be multiple of {self.step_size}.'\n",
    "        assert self.val_size % self.step_size == 0, f'Validation size should be multiple of {self.step_size}.'\n",
    "        assert self.test_size % self.step_size == 0, f'Test size should be multiple of {self.step_size}.'\n",
    "\n",
    "        if step == 'train':\n",
    "            if self.val_size + self.test_size > 0:\n",
    "                cutoff = -self.val_size - self.test_size\n",
    "                temporal = temporal[:, :, :cutoff]\n",
    "            temporal = self.padder(temporal)\n",
    "\n",
    "            # Truncate batch to shorter time-series \n",
    "            av_condition = torch.nonzero(torch.min(temporal[:, temporal_cols.get_loc('available_mask')], axis=0).values)\n",
    "            min_time_stamp = int(av_condition.min())\n",
    "            \n",
    "            available_ts = temporal.shape[-1] - min_time_stamp + 1 # +1, inclusive counting\n",
    "            if available_ts < self.input_size + self.h:\n",
    "                raise Exception(\n",
    "                    'Time series too short for given input and output size. \\n'\n",
    "                    f'Available timestamps: {available_ts}'\n",
    "                )\n",
    "\n",
    "            temporal = temporal[:, :, min_time_stamp:]\n",
    "\n",
    "        if step == 'val':\n",
    "            if self.test_size > 0:\n",
    "                temporal = temporal[:, :, :-self.test_size]\n",
    "            temporal = self.padder(temporal)\n",
    "\n",
    "        if step == 'predict':\n",
    "            if (self.test_size == 0) and (len(self.futr_exog_list)==0):\n",
    "                temporal = self.padder(temporal)\n",
    "\n",
    "        # Parse batch\n",
    "        window_size = self.input_size + self.h\n",
    "        windows = temporal.unfold(dimension=-1,\n",
    "                                  size=window_size,\n",
    "                                  step=self.step_size)\n",
    "\n",
    "        # think about interaction available * sample mask\n",
    "        # windows_batch = dict(Y=windows[:, temporal_cols.get_loc('y'), :, :],\n",
    "        #                     available_mask=windows[:, temporal_cols.get_loc('available_mask'), :, :])\n",
    "\n",
    "        # [B, C, Ws, L+H]\n",
    "        windows_batch = dict(temporal=windows,\n",
    "                             static=None,\n",
    "                             temporal_cols=temporal_cols)\n",
    "\n",
    "        return windows_batch\n",
    "\n",
    "    def _parse_windows(self, batch, windows):\n",
    "        # [B, C, Ws, L+H]\n",
    "        # Filter insample lags from outsample horizon\n",
    "        y_idx = batch['temporal_cols'].get_loc('y')\n",
    "        mask_idx = batch['temporal_cols'].get_loc('available_mask')\n",
    "        insample_y = windows['temporal'][:, y_idx, :, :-self.h]\n",
    "        insample_mask = windows['temporal'][:, mask_idx, :, :-self.h]\n",
    "        outsample_y = windows['temporal'][:, y_idx, :, -self.h:]\n",
    "        outsample_mask = windows['temporal'][:, mask_idx, :, -self.h:]\n",
    "\n",
    "        # Filter historic exogenous variables\n",
    "        if len(self.hist_exog_list):\n",
    "            hist_exog_idx = windows['temporal_cols'].get_indexer(self.hist_exog_list)\n",
    "            hist_exog = windows['temporal'][:, hist_exog_idx, :, :-self.h]\n",
    "        else:\n",
    "            hist_exog = None\n",
    "        \n",
    "        # Filter future exogenous variables\n",
    "        if len(self.futr_exog_list):\n",
    "            futr_exog_idx = windows['temporal_cols'].get_indexer(self.futr_exog_list)\n",
    "            futr_exog = windows['temporal'][:, futr_exog_idx, :, :]\n",
    "        else:\n",
    "            futr_exog = None\n",
    "        # Filter static variables\n",
    "        if len(self.stat_exog_list):\n",
    "            static_idx = windows['static_cols'].get_indexer(self.stat_exog_list)\n",
    "            stat_exog = windows['static'][:, 0, static_idx]\n",
    "        else:\n",
    "            stat_exog = None\n",
    "\n",
    "        return insample_y, insample_mask, outsample_y, outsample_mask, \\\n",
    "               hist_exog, futr_exog, stat_exog\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Normalize\n",
    "        if self.normalize:\n",
    "            batch, *_ = self._normalization(batch)\n",
    "\n",
    "        # Create windows\n",
    "        windows = self._create_windows(batch, step='train')\n",
    "\n",
    "        # Parse windows\n",
    "        insample_y, insample_mask, outsample_y, outsample_mask, \\\n",
    "               hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n",
    "\n",
    "        windows_batch = dict(insample_y=insample_y, # [Ws, L]\n",
    "                             insample_mask=insample_mask, # [Ws, L]\n",
    "                             futr_exog=futr_exog, # [Ws, L+H]\n",
    "                             hist_exog=hist_exog, # [Ws, L]\n",
    "                             stat_exog=stat_exog) # [Ws, 1]\n",
    "\n",
    "        y_hat = self(windows_batch)\n",
    "\n",
    "        loss = self.loss(y=outsample_y, y_hat=y_hat, mask=outsample_mask)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if self.val_size == 0:\n",
    "            return np.nan\n",
    "\n",
    "        # Normalize\n",
    "        if self.normalize:\n",
    "            batch, y_means, y_stds = self._normalization(batch)\n",
    "\n",
    "        # Create windows\n",
    "        windows = self._create_windows(batch, step='val')\n",
    "\n",
    "        # Parse windows\n",
    "        insample_y, insample_mask, outsample_y, outsample_mask, \\\n",
    "               hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n",
    "\n",
    "        windows_batch = dict(insample_y=insample_y, # [Ws, L]\n",
    "                             insample_mask=insample_mask, # [Ws, L]\n",
    "                             futr_exog=futr_exog, # [Ws, L+H]\n",
    "                             hist_exog=hist_exog, # [Ws, L]\n",
    "                             stat_exog=stat_exog) # [Ws, 1]\n",
    "\n",
    "        y_hat = self(windows_batch)\n",
    "\n",
    "        # Inv Normalize\n",
    "        if self.normalize:\n",
    "            y_hat = self._inv_normalization(y_hat, y_means, y_stds)\n",
    "\n",
    "        # Remove train y_hat\n",
    "        val_windows = (self.val_size // self.step_size) + 1\n",
    "        y_hat = y_hat[:, -val_windows:-1, :]\n",
    "        outsample_y = outsample_y[:, -val_windows:-1, :]\n",
    "        outsample_mask = outsample_mask[:, -val_windows:-1, :]\n",
    "\n",
    "        loss = self.loss(y=outsample_y, y_hat=y_hat, mask=outsample_mask)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        if self.val_size == 0:\n",
    "            return\n",
    "        avg_loss = torch.stack(outputs).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # Normalize\n",
    "        if self.normalize:\n",
    "            batch, y_means, y_stds = self._normalization(batch)\n",
    "\n",
    "        windows = self._create_windows(batch, step='predict')\n",
    "\n",
    "        # Parse windows\n",
    "        insample_y, insample_mask, _, _, \\\n",
    "               hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n",
    "\n",
    "        windows_batch = dict(insample_y=insample_y, # [Ws, L]\n",
    "                             insample_mask=insample_mask, # [Ws, L]\n",
    "                             futr_exog=futr_exog, # [Ws, L+H]\n",
    "                             hist_exog=hist_exog, # [Ws, L]\n",
    "                             stat_exog=stat_exog) # [Ws, 1]\n",
    "\n",
    "        y_hat = self(windows_batch)\n",
    "\n",
    "        # Inv Normalize\n",
    "        if self.normalize:\n",
    "            y_hat = self._inv_normalization(y_hat, y_means, y_stds)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "    def fit(self, dataset, val_size=0, test_size=0):\n",
    "        \"\"\"\n",
    "        Fits Model.\n",
    "        \n",
    "        **Parameters:**<br>\n",
    "        `dataset`: TimeSeriesDataset.<br>\n",
    "        `trainer`: pl.Trainer.<br>\n",
    "        `val_size`: int, validation size.<br>\n",
    "        `test_size`: int, test size.<br>\n",
    "        `data_kwargs`: extra arguments to be passed to TimeSeriesDataModule.\n",
    "        \"\"\"\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        datamodule = TimeSeriesDataModule(\n",
    "            dataset, \n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers_loader,\n",
    "            drop_last=self.drop_last_loader\n",
    "        )\n",
    "        trainer = pl.Trainer(**self.trainer_kwargs)\n",
    "        trainer.fit(self, datamodule=datamodule)\n",
    "\n",
    "    def predict(self, dataset, step_size=1, **data_kwargs):\n",
    "        \"\"\"\n",
    "        Predicts RNN Model.\n",
    "        \n",
    "        **Parameters:**<br>\n",
    "        `dataset`: TimeSeriesDataset.<br>\n",
    "        `trainer`: pl.Trainer.<br>\n",
    "        `data_kwargs`: extra arguments to be passed to TimeSeriesDataModule.\n",
    "        \"\"\"\n",
    "        if step_size != self.step_size:\n",
    "            print(f'WARNING: step_size {step_size} different than self.step_size, \\\n",
    "                   using self.step_size from training instead!')\n",
    "\n",
    "        self.predict_step_size = self.step_size\n",
    "        # fcsts (window, batch, h)\n",
    "        trainer = pl.Trainer(**self.trainer_kwargs)\n",
    "        fcsts = trainer.predict(self, datamodule=TimeSeriesDataModule(dataset, **data_kwargs))\n",
    "        if self.test_size > 0:\n",
    "            fcsts = torch.vstack([fcst[:, -(1+(self.test_size-self.h)//self.step_size ):,:] for fcst in fcsts])\n",
    "            fcsts = fcsts.numpy().flatten()\n",
    "            fcsts = fcsts.reshape(-1, self.loss.outputsize_multiplier)\n",
    "        else:\n",
    "            fcsts = torch.vstack([fcst[:, -1:, :] for fcst in fcsts]).numpy().flatten()\n",
    "            fcsts = fcsts.reshape(-1, self.loss.outputsize_multiplier)\n",
    "\n",
    "        return fcsts\n",
    "\n",
    "    def set_test_size(self, test_size):\n",
    "        self.test_size = test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BaseRecurrent, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BaseRecurrent.fit, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(BaseRecurrent.predict, title_level=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('neuralforecast')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
