{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common._base_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# Base windows class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "from neuralforecast.tsdataset import TimeSeriesDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70cd14-ecb1-4205-8511-fecbd26c8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseWindows(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, h, \n",
    "                 loss,\n",
    "                 batch_size=32,\n",
    "                 normalize=False,\n",
    "                 num_workers_loader=0,\n",
    "                 drop_last_loader=False,\n",
    "                 random_seed=1, \n",
    "                 **trainer_kwargs):\n",
    "        super(BaseWindows, self).__init__()\n",
    "        self.random_seed = random_seed\n",
    "        pl.seed_everything(self.random_seed, workers=True)\n",
    "        \n",
    "        # Padder to complete train windows\n",
    "        self.h = h\n",
    "        self.padder = nn.ConstantPad1d(padding=(0, self.h), value=0)\n",
    "        \n",
    "        # Loss\n",
    "        self.loss = loss\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # Base arguments\n",
    "        self.windows_batch_size: int = None\n",
    "        \n",
    "        # Fit arguments\n",
    "        self.val_size: int = 0\n",
    "        self.test_size: int = 0\n",
    "\n",
    "        # Predict arguments\n",
    "        self.step_size: int = 1\n",
    "\n",
    "        # Model state\n",
    "        self.decompose_forecast = False\n",
    "        \n",
    "        # Trainer\n",
    "        # we need to instantiate the trainer each time we want to use it\n",
    "        self.trainer_kwargs = {**trainer_kwargs}\n",
    "        if self.trainer_kwargs.get('callbacks', None) is None:\n",
    "            self.trainer_kwargs = {**{'callbacks': [TQDMProgressBar()], **trainer_kwargs}}\n",
    "        else:\n",
    "            self.trainer_kwargs = trainer_kwargs\n",
    "\n",
    "        # Add GPU accelerator if available\n",
    "        if self.trainer_kwargs.get('accelerator', None) is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.trainer_kwargs['accelerator'] = \"gpu\"\n",
    "        if self.trainer_kwargs.get('devices', None) is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.trainer_kwargs['devices'] = -1\n",
    "        \n",
    "        # DataModule arguments\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers_loader = num_workers_loader\n",
    "        self.drop_last_loader = drop_last_loader\n",
    "        \n",
    "    \n",
    "    def on_fit_start(self):\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def _create_windows(self, batch, step):\n",
    "        # Parse common data\n",
    "        window_size = self.input_size + self.h\n",
    "        temporal_cols = batch['temporal_cols']\n",
    "        temporal = batch['temporal']\n",
    "\n",
    "        if step == 'train':\n",
    "            if self.val_size + self.test_size > 0:\n",
    "                cutoff = -self.val_size - self.test_size\n",
    "                temporal = temporal[:, :, :cutoff]\n",
    "\n",
    "            temporal = self.padder(temporal)\n",
    "            windows = temporal.unfold(dimension=-1, \n",
    "                                      size=window_size, \n",
    "                                      step=self.step_size)\n",
    "\n",
    "            # [batch, channels, windows, window_size] 0, 1, 2, 3\n",
    "            # -> [batch * windows, window_size, channels] 0, 2, 3, 1\n",
    "            windows = windows.permute(0, 2, 3, 1).contiguous()\n",
    "            windows = windows.reshape(-1, window_size, len(temporal_cols))\n",
    "\n",
    "            available_idx = temporal_cols.get_loc('available_mask')\n",
    "            sample_condition = windows[:, -self.h:, available_idx]\n",
    "            sample_condition = torch.sum(sample_condition, axis=1)\n",
    "            sample_condition = (sample_condition > 0)\n",
    "            windows = windows[sample_condition]\n",
    "\n",
    "            # Sample windows\n",
    "            n_windows = len(windows)\n",
    "            if self.windows_batch_size is not None:\n",
    "                w_idxs = np.random.choice(n_windows, \n",
    "                                          size=self.windows_batch_size,\n",
    "                                          replace=(n_windows < self.windows_batch_size))\n",
    "                windows = windows[w_idxs]\n",
    "\n",
    "            # think about interaction available * sample mask\n",
    "            windows_batch = dict(temporal=windows,\n",
    "                                 static=None,\n",
    "                                 temporal_cols=temporal_cols)\n",
    "            return windows_batch\n",
    "\n",
    "        elif step in ['predict', 'val']:\n",
    "\n",
    "            if step == 'predict':\n",
    "                predict_step_size = self.predict_step_size\n",
    "                cutoff = - self.input_size - self.test_size\n",
    "                temporal = batch['temporal'][:, :, cutoff:]\n",
    "\n",
    "            elif step == 'val':\n",
    "                predict_step_size = self.step_size\n",
    "                cutoff = -self.input_size - self.val_size - self.test_size\n",
    "                if self.test_size > 0:\n",
    "                    temporal = batch['temporal'][:, :, cutoff:-self.test_size]\n",
    "                else:\n",
    "                    temporal = batch['temporal'][:, :, cutoff:]\n",
    "\n",
    "            if step=='predict' and self.test_size == 0:\n",
    "               temporal = self.padder(temporal)\n",
    "\n",
    "            windows = temporal.unfold(dimension=-1,\n",
    "                                      size=window_size,\n",
    "                                      step=predict_step_size)\n",
    "\n",
    "            # [batch, channels, windows, window_size] 0, 1, 2, 3\n",
    "            # -> [batch * windows, window_size, channels] 0, 2, 3, 1\n",
    "            windows = windows.permute(0, 2, 3, 1).contiguous()\n",
    "            windows = windows.reshape(-1, window_size, len(temporal_cols))\n",
    "            windows_batch = dict(temporal=windows,\n",
    "                                 static=None,\n",
    "                                 temporal_cols=temporal_cols)\n",
    "            return windows_batch\n",
    "        else:\n",
    "            raise ValueError(f'Unknown step {step}')\n",
    "            \n",
    "    def _normalization(self, windows):\n",
    "        # windows are already filtered by train/validation/test\n",
    "        # from the `create_windows_method` nor leakage risk\n",
    "        temporal = windows['temporal']           # B, L+H, C\n",
    "        temporal_cols = windows['temporal_cols'] # B, L+H, C\n",
    "        \n",
    "        # to avoid leakage compute means on the lags only\n",
    "        temporal_y = temporal[:, :-self.h, temporal_cols.get_loc('y')]\n",
    "        temporal_mask = temporal[:, :-self.h, temporal_cols.get_loc('available_mask')]\n",
    "        \n",
    "        # Take means in the window L+H\n",
    "        available_sum = torch.sum(temporal_mask, dim=1, keepdim=True)\n",
    "                \n",
    "        # Protection: when no observations are available denom = 1\n",
    "        mask_safe = available_sum.clone()\n",
    "        mask_safe[available_sum==0] = 1\n",
    "        \n",
    "        y_means = torch.sum(temporal_y * temporal_mask,\n",
    "                            dim=1, keepdim=True) / mask_safe\n",
    "        y_stds = torch.sqrt(torch.sum(temporal_mask*(temporal_y-y_means)**2,\n",
    "                                      dim=1, keepdim=True)/ mask_safe )\n",
    "\n",
    "        # Protection: when no variance or unavailable data change stds=1\n",
    "        y_stds[available_sum==0] = 1.0\n",
    "        y_stds[y_stds==0] = 1.0\n",
    "\n",
    "        # Normalize all target variable and replace in windows dict\n",
    "        all_y = temporal[:, :, temporal_cols.get_loc('y')]\n",
    "        all_y = (all_y - y_means) / y_stds\n",
    "        temporal[:, :, temporal_cols.get_loc('y')] = all_y\n",
    "        \n",
    "        windows['temporal'] = temporal\n",
    "        \n",
    "        return windows, y_means, y_stds\n",
    "\n",
    "    def _inv_normalization(self, y_hat, y_means, y_stds):\n",
    "        # Receives window predictions [B, H, output]\n",
    "        # Broadcasts outputs and inverts normalization\n",
    "        if self.loss.outputsize_multiplier>1:\n",
    "            y_stds = y_stds.unsqueeze(-1)\n",
    "            y_means = y_means.unsqueeze(-1)\n",
    "        return y_stds * y_hat + y_means\n",
    "\n",
    "    def training_step(self, batch, batch_idx):        \n",
    "        # Create windows [Ws, L+H, C]\n",
    "        windows = self._create_windows(batch, step='train')\n",
    "        \n",
    "        # Normalize windows\n",
    "        if self.normalize:\n",
    "            windows, *_ = self._normalization(windows)\n",
    "\n",
    "        # Filter insample lags from outsample horizon\n",
    "        y_idx = batch['temporal_cols'].get_loc('y')\n",
    "        mask_idx = batch['temporal_cols'].get_loc('available_mask')\n",
    "        insample_y = windows['temporal'][:, :-self.h, y_idx]\n",
    "        insample_mask = windows['temporal'][:, :-self.h, mask_idx]\n",
    "        outsample_y = windows['temporal'][:, -self.h:, y_idx]\n",
    "        outsample_mask = windows['temporal'][:, -self.h:, mask_idx]\n",
    "\n",
    "        # Input [Ws, L]\n",
    "        y_hat = self(insample_y, insample_mask)\n",
    "        loss = self.loss(y=outsample_y, y_hat=y_hat, mask=outsample_mask)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if self.val_size == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # Create windows [Ws, L+H, C]\n",
    "        windows = self._create_windows(batch, step='val')\n",
    "        \n",
    "        # Normalize windows\n",
    "        if self.normalize:\n",
    "            windows, *_ = self._normalization(windows)\n",
    "\n",
    "        # Filter insample lags from outsample horizon\n",
    "        y_idx = batch['temporal_cols'].get_loc('y')\n",
    "        mask_idx = batch['temporal_cols'].get_loc('available_mask')\n",
    "        insample_y = windows['temporal'][:, :-self.h, y_idx]\n",
    "        insample_mask = windows['temporal'][:, :-self.h, mask_idx]\n",
    "        outsample_y = windows['temporal'][:, -self.h:, y_idx]\n",
    "        outsample_mask = windows['temporal'][:, -self.h:, mask_idx]\n",
    "        \n",
    "        # Input [Ws, L]\n",
    "        y_hat = self(insample_y, insample_mask)\n",
    "        loss = self.loss(y=outsample_y, y_hat=y_hat, mask=outsample_mask)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        if self.val_size == 0:\n",
    "            return\n",
    "        avg_loss = torch.stack(outputs).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss)\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):        \n",
    "        # Create windows [Ws, L+H, C]\n",
    "        windows = self._create_windows(batch, step='predict')\n",
    "\n",
    "        # Normalize windows\n",
    "        if self.normalize:\n",
    "            windows, y_means, y_stds = self._normalization(windows)\n",
    "\n",
    "        # Filter insample lags from outsample horizon\n",
    "        y_idx = batch['temporal_cols'].get_loc('y')\n",
    "        mask_idx = batch['temporal_cols'].get_loc('available_mask')\n",
    "        insample_y = windows['temporal'][:, :-self.h, y_idx]\n",
    "        insample_mask = windows['temporal'][:, :-self.h, mask_idx]\n",
    "\n",
    "        # Input [Ws, L]\n",
    "        y_hat = self(insample_y, insample_mask)\n",
    "\n",
    "        # Inv Normalize\n",
    "        if self.normalize:\n",
    "            y_hat = self._inv_normalization(y_hat, y_means, y_stds)\n",
    "        return y_hat\n",
    "    \n",
    "    def fit(self, dataset, val_size=0, test_size=0):\n",
    "        \"\"\"\n",
    "        Fits Model.\n",
    "        \n",
    "        **Parameters:**<br>\n",
    "        `dataset`: TimeSeriesDataset.<br>\n",
    "        `trainer`: pl.Trainer.<br>\n",
    "        `val_size`: int, validation size.<br>\n",
    "        `test_size`: int, test size.<br>\n",
    "        `data_kwargs`: extra arguments to be passed to TimeSeriesDataModule.\n",
    "        \"\"\"\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        datamodule = TimeSeriesDataModule(\n",
    "            dataset, \n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers_loader,\n",
    "            drop_last=self.drop_last_loader\n",
    "        )\n",
    "        \n",
    "        trainer = pl.Trainer(**self.trainer_kwargs)\n",
    "        trainer.fit(self, datamodule=datamodule)\n",
    "        \n",
    "    def predict(self, dataset, step_size=1, **data_kwargs):\n",
    "        \"\"\"\n",
    "        Predicts Model.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `dataset`: TimeSeriesDataset.<br>\n",
    "        `trainer`: pl.Trainer.<br>\n",
    "        `step_size`: int, Step size between each window.<br>\n",
    "        `data_kwargs`: extra arguments to be passed to TimeSeriesDataModule.\n",
    "        \"\"\"\n",
    "        self.predict_step_size = step_size\n",
    "        self.decompose_forecast = False\n",
    "        datamodule = TimeSeriesDataModule(dataset, **data_kwargs)\n",
    "        trainer = pl.Trainer(**self.trainer_kwargs)\n",
    "        fcsts = trainer.predict(self, datamodule=datamodule)        \n",
    "        fcsts = torch.vstack(fcsts).numpy().flatten()    \n",
    "        fcsts = fcsts.reshape(-1, self.loss.outputsize_multiplier)\n",
    "        return fcsts\n",
    "\n",
    "    def decompose(self, dataset, step_size=1, **data_kwargs):\n",
    "        \"\"\"\n",
    "        Predicts with decomposition.\n",
    "        \n",
    "        **Parameters:**<br>\n",
    "        `dataset`: TimeSeriesDataset.<br>\n",
    "        `trainer`: pl.Trainer.<br>\n",
    "        `step_size`: int, Step size between each window.<br>\n",
    "        `data_kwargs`: extra arguments to be passed to TimeSeriesDataModule.\n",
    "        \"\"\"\n",
    "        self.predict_step_size = step_size\n",
    "        self.decompose_forecast = True\n",
    "        datamodule = TimeSeriesDataModule(dataset, **data_kwargs)\n",
    "        trainer = pl.Trainer(**self.trainer_kwargs)\n",
    "        fcsts = trainer.predict(self, datamodule=datamodule)\n",
    "        self.decompose_forecast = False # Default decomposition back to false\n",
    "        return torch.vstack(fcsts).numpy()\n",
    "\n",
    "    def forward(self, insample_y, insample_mask):\n",
    "        raise NotImplementedError('forward')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
