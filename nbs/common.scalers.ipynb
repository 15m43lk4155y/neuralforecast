{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common._scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c704dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56742bfb",
   "metadata": {},
   "source": [
    "# TemporalNorm\n",
    "\n",
    "> Temporal normalization has proven to be essential in neural forecasting tasks, as it enables network's non-linearities to express themselves. Forecasting scaling methods take particular interest in the temporal dimension where most of the variance dwells, contrary to other deep learning techniques like `BatchNorm` that normalizes across batch and temporal dimensions, and `LayerNorm` that normalizes across the feature dimension. Currently we support the following techniques: `std`, `median`, `norm`, `norm1`, `invariant`. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9319296d",
   "metadata": {},
   "source": [
    "![Figure 1. Illustration of temporal normalization (left), layer normalization (center) and batch normalization (right). The entries in green show the components used to compute the normalizing statistics.](imgs_models/temporal_norm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2cc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7250387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"axes.grid\"]=True\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams[\"figure.figsize\"] = (4,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef461e9c",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 1. Auxiliary Functions </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a249a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def masked_median(x, mask, dim=1, keepdim=True):\n",
    "    \"\"\" Masked Median\n",
    "\n",
    "    Compute the median of tensor `x` along dim, ignoring values where \n",
    "    `mask` is False. `x` and `mask` need to be broadcastable.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `x`: torch.Tensor shape [batch, time, features] to compute median of.<br>\n",
    "    `mask`: torch Tensor bool, shape  [batch, time] where `x` is valid and False\n",
    "            where `x` should be masked. Mask should not be all False in any column of\n",
    "            dimension dim to avoid NaNs from zero division.<br>\n",
    "    `dim` (int, optional): Dimension to take median of. Defaults to 0.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `x_median`: torch.Tensor same shape as `x`, except dimension dim reduced.\n",
    "    \"\"\"\n",
    "    x_nan = x.float().masked_fill(mask[:,:,None]<1, float(\"nan\"))\n",
    "    x_median, _ = x_nan.nanmedian(dim=dim, keepdim=keepdim)\n",
    "    return x_median\n",
    "\n",
    "def masked_mean(x, mask, dim=1, keepdim=True):\n",
    "    \"\"\" Masked Mean\n",
    "\n",
    "    Compute the mean of tensor `x` along dim, ignoring values where \n",
    "    `mask` is False. `x` and `mask` need to be broadcastable.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `x`: torch.Tensor shape [batch, time, features] to compute median of.<br>\n",
    "    `mask`: torch Tensor bool, shape  [batch, time] where `x` is valid and False\n",
    "            where `x` should be masked. Mask should not be all False in any column of\n",
    "            dimension dim to avoid NaNs from zero division.<br>\n",
    "    `dim` (int, optional): Dimension to take median of. Defaults to 0.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `x_mean`: torch.Tensor same shape as `x`, except dimension dim reduced.\n",
    "    \"\"\"\n",
    "    x_nan = x.float().masked_fill(mask[:,:,None]<1, float(\"nan\"))\n",
    "    x_mean = x_nan.nanmean(dim=dim, keepdim=keepdim)\n",
    "    return x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(masked_median, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e1b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(masked_mean, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a486a2",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 2. Scalers </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c76dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def minmax_scaler(x, mask, eps=1e-8):\n",
    "    \"\"\" MinMax Scaler\n",
    "\n",
    "    Standardizes temporal features by ensuring its range dweels between\n",
    "    [0,1] range. This transformation is often used as an alternative \n",
    "    to the standard scaler. The scaled features are obtained as:\n",
    "\n",
    "    $$\\mathbf{z} = (\\mathbf{x}_{[B,T,C]}-\\mathrm{min}({\\mathbf{x}})_{[B,1,C]})/\n",
    "        (\\mathrm{max}({\\mathbf{x}})_{[B,1,C]}- \\mathrm{min}({\\mathbf{x}})_{[B,1,C]})$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `x`: torch.Tensor shape [batch, time, channels].<br>\n",
    "    `mask`: torch Tensor bool, shape  [batch, time] where `x` is valid and False\n",
    "            where `x` should be masked. Mask should not be all False in any column of\n",
    "            dimension dim to avoid NaNs from zero division.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `z`: torch.Tensor same shape as `x`, except scaled.\n",
    "    \"\"\"\n",
    "    max_mask = (mask==0)[:,:,None] * (-1e12)\n",
    "    min_mask = (mask==0)[:,:,None] * (1e12)\n",
    "    x_max = torch.max(x + max_mask, dim=1, keepdim=True)[0]\n",
    "    x_min = torch.min(x + min_mask, dim=1, keepdim=True)[0]\n",
    "\n",
    "    z = (x - x_min) / (x_max - x_min + eps)\n",
    "    return z, x_min, x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4eb5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def inv_minmax_scaler(z, x_min, x_max, eps=1e-8):\n",
    "    return z * ((x_max - x_min) + eps) + x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(minmax_scaler, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def minmax1_scaler(x, mask, eps=1e-8):\n",
    "    \"\"\" MinMax1 Scaler\n",
    "\n",
    "    Standardizes temporal features by ensuring its range dweels between\n",
    "    [-1,1] range. This transformation is often used as an alternative \n",
    "    to the standard scaler or classic Min Max Scaler. \n",
    "    The scaled features are obtained as:\n",
    "\n",
    "    $$\\mathbf{z} = 2 (\\mathbf{x}_{[B,T,C]}-\\mathrm{min}({\\mathbf{x}})_{[B,1,C]})/ (\\mathrm{max}({\\mathbf{x}})_{[B,1,C]}- \\mathrm{min}({\\mathbf{x}})_{[B,1,C]})-1$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `x`: torch.Tensor shape [batch, time, channels].<br>\n",
    "    `mask`: torch Tensor bool, shape  [batch, time] where `x` is valid and False\n",
    "            where `x` should be masked. Mask should not be all False in any column of\n",
    "            dimension dim to avoid NaNs from zero division.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `z`: torch.Tensor same shape as `x`, except scaled.\n",
    "    \"\"\"\n",
    "    max_mask = (mask==0)[:,:,None] * (-1e12)\n",
    "    min_mask = (mask==0)[:,:,None] * (1e12)\n",
    "    x_max = torch.max(x + max_mask, dim=1, keepdim=True)[0]\n",
    "    x_min = torch.min(x + min_mask, dim=1, keepdim=True)[0]\n",
    "\n",
    "    x = (x - x_min) / ((x_max - x_min) + eps)\n",
    "    z = x * (2) - 1\n",
    "    return z, x_min, x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def inv_minmax1_scaler(z, x_min, x_max, eps=1e-8):\n",
    "    z = (z + 1) / 2\n",
    "    return z * ((x_max - x_min) + eps) + x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ccb77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(minmax1_scaler, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c187a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def std_scaler(x, mask, eps=1e-8):\n",
    "    \"\"\" Standard Scaler\n",
    "\n",
    "    Standardizes temporal features by removing the mean and scaling\n",
    "    to unit variance in the temporal dimension. \n",
    "    The scaled features are obtained as:\n",
    "\n",
    "    $$\\mathbf{z} = (\\mathbf{x}_{[B,T,C]}-\\\\bar{\\mathbf{x}}_{[B,1,C]})/\\hat{\\sigma}_{[B,1,C]}$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `x`: torch.Tensor shape [batch, time, channels].<br>\n",
    "    `mask`: torch Tensor bool, shape  [batch, time] where `x` is valid and False\n",
    "            where `x` should be masked. Mask should not be all False in any column of\n",
    "            dimension dim to avoid NaNs from zero division.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `z`: torch.Tensor same shape as `x`, except scaled.\n",
    "    \"\"\"\n",
    "    x_means = masked_mean(x, mask)\n",
    "    x_stds = torch.sqrt(masked_mean((x-x_means)**2, mask)) \n",
    "\n",
    "    z = (x - x_means) / (x_stds + eps)\n",
    "    return z, x_means, x_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a953e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def inv_std_scaler(z, x_mean, x_std, eps=1e-8):\n",
    "    return (z * (x_std + eps)) + x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(std_scaler, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def robust_scaler(x, mask):\n",
    "    \"\"\" Robust Median Scaler\n",
    "\n",
    "    Standardizes temporal features by removing the median and scaling\n",
    "    with the mean absolute deviation (mad) a robust estimator of variance.\n",
    "    This scaler is particularly useful with noisy data where outliers can \n",
    "    heavily influence the sample mean / variance in a negative way.\n",
    "    In these scenarios the median and amd give better results.\n",
    "    \n",
    "    The scaled features are obtained as:\n",
    "\n",
    "    $$\\mathbf{z} = (\\mathbf{x}_{[B,T,C]}-\\\\textrm{median}(\\mathbf{x})_{[B,1,C]})/\\\\textrm{mad}(\\mathbf{x})_{[B,1,C]}$$\n",
    "        \n",
    "    $$\\\\textrm{mad}(\\mathbf{x}) = \\\\frac{1}{N} \\sum_{}|\\mathbf{x} - \\mathrm{median}(x)|$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `x`: torch.Tensor shape [batch, time, channels].<br>\n",
    "    `mask`: torch Tensor bool, shape  [batch, time] where `x` is valid and False\n",
    "            where `x` should be masked. Mask should not be all False in any column of\n",
    "            dimension dim to avoid NaNs from zero division.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `z`: torch.Tensor same shape as `x`, except scaled.\n",
    "    \"\"\"\n",
    "    x_median = masked_median(x, mask)\n",
    "    x_mad = masked_median(torch.abs(x-x_median), mask)\n",
    "\n",
    "    # Protect x_mad=0 values\n",
    "    x_means = masked_mean(x, mask)\n",
    "    x_stds = torch.sqrt(masked_mean((x-x_means)**2, mask))        \n",
    "    x_mad_aux = x_stds / 0.6744897501960817\n",
    "    x_mad = x_mad * (x_mad>0) + x_mad_aux * (x_mad==0)\n",
    "\n",
    "    z = (x - x_median) / x_mad\n",
    "    return z, x_median, x_mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3476556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def inv_robust_scaler(z, x_median, x_mad):\n",
    "    return z * x_mad + x_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(robust_scaler, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def invariant_scaler(x, mask):\n",
    "    \"\"\" Invariant Median Scaler\n",
    "\n",
    "    Standardizes temporal features by removing the median and scaling\n",
    "    with the mean absolute deviation (mad) a robust estimator of variance.\n",
    "    Aditionally it complements the transformation with the arcsinh transformation.\n",
    "    The scaled features are obtained as:\n",
    "\n",
    "    $$\\mathbf{z} = (\\mathbf{x}_{[B,T,C]}-\\\\textrm{median}(\\mathbf{x})_{[B,1,C]})/\\\\textrm{mad}(\\mathbf{x})_{[B,1,C]}$$\n",
    "\n",
    "    $$\\mathbf{z} = \\\\textrm{arcsinh}(\\mathbf{z})$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `x`: torch.Tensor shape [batch, time, channels].<br>\n",
    "    `mask`: torch Tensor bool, shape  [batch, time] where `x` is valid and False\n",
    "            where `x` should be masked. Mask should not be all False in any column of\n",
    "            dimension dim to avoid NaNs from zero division.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `z`: torch.Tensor same shape as `x`, except scaled.\n",
    "    \"\"\"\n",
    "    x_median = masked_median(x, mask)\n",
    "    x_mad = masked_median(torch.abs(x-x_median), mask)\n",
    "\n",
    "    # Protect x_mad=0 values\n",
    "    x_means = masked_mean(x, mask)\n",
    "    x_stds = torch.sqrt(masked_mean((x-x_means)**2, mask))        \n",
    "    x_mad_aux = x_stds / 0.6744897501960817\n",
    "    x_mad = x_mad * (x_mad>0) + x_mad_aux * (x_mad==0)\n",
    "\n",
    "    z = torch.arcsinh((x - x_median) / x_mad)\n",
    "    return z, x_median, x_mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e44ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def inv_invariant_scaler(z, x_median, x_mad):\n",
    "    return torch.sinh(z) * x_mad + x_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b1b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(invariant_scaler, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e828c",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 3. TemporalNorm Module </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TemporalNorm(nn.Module):\n",
    "    \"\"\" Temporal Normalization\n",
    "\n",
    "    Standardization of the features is a common requirement for many \n",
    "    machine learning estimators, and it is commonly achieved by removing \n",
    "    the level and scaling its variance. The `TemporalNorm` module applies \n",
    "    temporal normalization over the batch of inputs as defined by the type of scaler.\n",
    "\n",
    "    $$\\mathbf{z}_{[B,T,C]} = \\\\textrm{Scaler}(\\mathbf{x}_{[B,T,C]})$$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `scaler_type`: str, defines the type of scaler used by TemporalNorm.\n",
    "                    available [`standard`, `robust`, `minmax`, `minmax1`, `invariant`].<br>\n",
    "    \"\"\"    \n",
    "    def __init__(self, scaler_type='robust'):\n",
    "        super().__init__()\n",
    "        scalers = dict(standard=std_scaler,\n",
    "                       robust=robust_scaler,\n",
    "                       minmax=minmax_scaler,\n",
    "                       minmax1=minmax1_scaler,\n",
    "                       invariant=invariant_scaler,\n",
    "                      )\n",
    "        inverse_scalers = dict(standard=inv_std_scaler,\n",
    "                               robust=inv_robust_scaler,\n",
    "                               minmax=inv_minmax_scaler,\n",
    "                               minmax1=inv_minmax1_scaler,\n",
    "                               invariant=inv_invariant_scaler,\n",
    "                              )\n",
    "        assert (scaler_type in scalers.keys()), f'{scaler_type} not defined'\n",
    "\n",
    "        self.scaler = scalers[scaler_type]\n",
    "        self.inverse_scaler = inverse_scalers[scaler_type]\n",
    "        self.scaler_type = scaler_type\n",
    "\n",
    "    #@torch.no_grad()\n",
    "    def transform(self, x, mask):\n",
    "        \"\"\" Center and scale the data.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `x`: torch.Tensor shape [batch, time, channels].<br>\n",
    "        `mask`: torch Tensor bool, shape  [batch, time] where `x` is valid and False\n",
    "                where `x` should be masked. Mask should not be all False in any column of\n",
    "                dimension dim to avoid NaNs from zero division.<br>\n",
    "        \n",
    "        **Returns:**<br>\n",
    "        `z`: torch.Tensor same shape as `x`, except scaled.        \n",
    "        \"\"\"\n",
    "        z, x_shift, x_scale = self.scaler(x=x, mask=mask)\n",
    "        self.x_shift = x_shift\n",
    "        self.x_scale = x_scale\n",
    "        return z\n",
    "\n",
    "    #@torch.no_grad()\n",
    "    def inverse_transform(self, z, x_shift=None, x_scale=None):\n",
    "        \"\"\" Scale back the data to the original representation.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `z`: torch.Tensor shape [batch, time, channels], scaled.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `x`: torch.Tensor original data.\n",
    "        \"\"\"\n",
    "        if x_shift is None:\n",
    "            x_shift = self.x_shift\n",
    "        if x_scale is None:\n",
    "            x_scale = self.x_scale\n",
    "\n",
    "        x = self.inverse_scaler(z, x_shift, x_scale)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TemporalNorm, name='TemporalNorm', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TemporalNorm.transform, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TemporalNorm.inverse_transform, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2968e0",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> Example </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99722125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fef46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare synthetic batch to normalize\n",
    "x1 = 10**0 * np.arange(36)[:, None]\n",
    "x2 = 10**1 * np.arange(36)[:, None]\n",
    "\n",
    "np_x = np.concatenate([x1, x2], axis=1)\n",
    "np_x = np.repeat(np_x[None, :,:], repeats=2, axis=0)\n",
    "np_x[0,:,:] = np_x[0,:,:] + 100\n",
    "\n",
    "np_mask = np.concatenate([np.ones(24), \n",
    "                          np.zeros(12)])\n",
    "np_mask = np.repeat(np_mask[None,:], repeats=2, axis=0)\n",
    "\n",
    "print(f'x.shape [batch, time, features]={np_x.shape}')\n",
    "print(f'mask.shape [batch, time]={np_mask.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1f93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate scalers\n",
    "x = torch.tensor(np_x)\n",
    "mask = torch.tensor(np_mask)\n",
    "scaler = TemporalNorm(scaler_type='standard')\n",
    "x_scaled = scaler.transform(x=x, mask=mask)\n",
    "x_recovered = scaler.inverse_transform(x_scaled)\n",
    "\n",
    "plt.plot(x[0,:,0], label='x1', color='#78ACA8')\n",
    "plt.plot(x[0,:,1], label='x2',  color='#E3A39A')\n",
    "plt.title('Before TemporalNorm')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_scaled[0,:,0], label='x1', color='#78ACA8')\n",
    "plt.plot(x_scaled[0,:,1]+0.1, label='x2+0.1', color='#E3A39A')\n",
    "plt.title(f'TemporalNorm \\'{scaler.scaler_type}\\' ')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_recovered[0,:,0], label='x1', color='#78ACA8')\n",
    "plt.plot(x_recovered[0,:,1], label='x2', color='#E3A39A')\n",
    "plt.title('Recovered')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e49d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
