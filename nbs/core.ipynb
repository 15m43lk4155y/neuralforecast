{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkOrange\"> Core </span>\n",
    "> NeuralForecast contains two main components, PyTorch implementations deep learning predictive models, as well as parallelization and distributed computation utilities. The first component comprises low-level PyTorch model estimator classes like `models.NBEATS` and `models.RNN`. The second component is a high-level `core.NeuralForecast` wrapper class that operates with sets of time series data stored in pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45471e9f-4050-4a6c-bb05-4de144754da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _cv_dates(last_dates, freq, h, test_size, step_size=1):\n",
    "    #assuming step_size = 1\n",
    "    if (test_size - h) % step_size:\n",
    "        raise Exception('`test_size - h` should be module `step_size`')\n",
    "    n_windows = int((test_size - h) / step_size) + 1\n",
    "    if len(np.unique(last_dates)) == 1:\n",
    "        if issubclass(last_dates.dtype.type, np.integer):\n",
    "            total_dates = np.arange(last_dates[0] - test_size + 1, last_dates[0] + 1)\n",
    "            out = np.empty((h * n_windows, 2), dtype=last_dates.dtype)\n",
    "            freq = 1\n",
    "        else:\n",
    "            total_dates = pd.date_range(end=last_dates[0], periods=test_size, freq=freq)\n",
    "            out = np.empty((h * n_windows, 2), dtype='datetime64[s]')\n",
    "        for i_window, cutoff in enumerate(range(-test_size, -h + 1, step_size), start=0):\n",
    "            end_cutoff = cutoff + h\n",
    "            out[h * i_window : h * (i_window + 1), 0] = total_dates[cutoff:] if end_cutoff == 0 else total_dates[cutoff:end_cutoff]\n",
    "            out[h * i_window : h * (i_window + 1), 1] = np.tile(total_dates[cutoff] - freq, h)\n",
    "        dates = pd.DataFrame(np.tile(out, (len(last_dates), 1)), columns=['ds', 'cutoff'])\n",
    "    else:\n",
    "        dates = pd.concat([_cv_dates(np.array([ld]), freq, h, test_size, step_size) for ld in last_dates])\n",
    "        dates = dates.reset_index(drop=True)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff82c9-3cb5-407f-be34-141d265e8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "ds_int_cv_test = pd.DataFrame({\n",
    "    'ds': np.hstack([\n",
    "        [46, 47, 48],\n",
    "        [47, 48, 49],\n",
    "        [48, 49, 50]\n",
    "    ]),\n",
    "    'cutoff': [45] * 3 + [46] * 3 + [47] * 3\n",
    "}, dtype=np.int64)\n",
    "test_eq(ds_int_cv_test, _cv_dates(np.array([50], dtype=np.int64), 'D', 3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac141f22-ee59-408a-a5e8-4a5f72cb77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "ds_int_cv_test = pd.DataFrame({\n",
    "    'ds': np.hstack([\n",
    "        [46, 47, 48],\n",
    "        [48, 49, 50]\n",
    "    ]),\n",
    "    'cutoff': [45] * 3 + [47] * 3\n",
    "}, dtype=np.int64)\n",
    "test_eq(ds_int_cv_test, _cv_dates(np.array([50], dtype=np.int64), 'D', 3, 5, step_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2874985-3d14-40d2-a607-a774ed4abd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for e_e in [True, False]:\n",
    "    n_series = 2\n",
    "    ga, indices, dates, ds = TimeSeriesDataset.from_df(generate_series(n_series, equal_ends=e_e), sort_df=True)\n",
    "    freq = pd.tseries.frequencies.to_offset('D')\n",
    "    horizon = 3\n",
    "    test_size = 5\n",
    "    df_dates = _cv_dates(last_dates=dates, freq=freq, h=horizon, test_size=test_size)\n",
    "    test_eq(len(df_dates), n_series * horizon * (test_size - horizon + 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dae43c-4d11-4bbc-a431-ac33b004859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NeuralForecast:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 models: List[Any],\n",
    "                 freq: str):\n",
    "        \"\"\"\n",
    "        The `core.StatsForecast` class allows you to efficiently fit multiple `NeuralForecast` models \n",
    "        for large sets of time series. It operates with pandas DataFrame `df` that identifies series \n",
    "        and datestamps with the `unique_id` and `ds` columns. The `y` column denotes the target \n",
    "        time series variable.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `models`: List[typing.Any], list of instantiated objects models.StatsForecast.<br>\n",
    "        `freq`: str, frequency of the data, [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).<br>\n",
    "        `trainers`: List[typing.Any], optional list of instantiated pytorch lightning trainers.<br>\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "\n",
    "    def _prepare_fit(self, df, sort_df):\n",
    "        self.dataset, self.uids, self.last_dates, self.ds = TimeSeriesDataset.from_df(df=df, sort_df=sort_df)\n",
    "        self.sort_df = sort_df\n",
    "\n",
    "    def fit(self,\n",
    "            df: Optional[pd.DataFrame],\n",
    "            val_size: Optional[int] = 0,\n",
    "            sort_df: bool = True):\n",
    "        \"\"\"Fit the core.NeuralForecast.\n",
    "\n",
    "        Fit `models` to a large set of time series from DataFrame `df`.\n",
    "        and store fitted models for later inspection.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `df`: pandas.DataFrame, with columns [`unique_id`, `ds`, `y`] and exogenous.<br>\n",
    "        `val_size`: int, size of validation set.<br>\n",
    "        `sort_df`: bool, sort df before fitting.\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `self`: Returns with stored `NeuralForecast` fitted `models`.\n",
    "        \"\"\"\n",
    "        self._prepare_fit(df, sort_df)\n",
    "        #train + validation\n",
    "        for model in self.models:\n",
    "            model.fit(self.dataset, val_size=val_size)\n",
    "        #train with the full dataset\n",
    "\n",
    "    def _make_future_df(self, h: int):\n",
    "        if issubclass(self.last_dates.dtype.type, np.integer):\n",
    "            last_date_f = lambda x: np.arange(x + 1, x + 1 + h, dtype=self.last_dates.dtype)\n",
    "        else:\n",
    "            last_date_f = lambda x: pd.date_range(x + self.freq, periods=h, freq=self.freq)\n",
    "        if len(np.unique(self.last_dates)) == 1:\n",
    "            dates = np.tile(last_date_f(self.last_dates[0]), len(self.dataset))\n",
    "        else:\n",
    "            dates = np.hstack([last_date_f(last_date)\n",
    "                               for last_date in self.last_dates])\n",
    "        idx = pd.Index(np.repeat(self.uids, h), name='unique_id')\n",
    "        df = pd.DataFrame({'ds': dates}, index=idx)\n",
    "        return df\n",
    "\n",
    "    def predict(self, df: Optional[pd.DataFrame] = None, **data_kwargs):\n",
    "        \"\"\"Predict with core.NeuralForecast.\n",
    "\n",
    "        Use stored fitted `models` to predict large set of time series from DataFrame `df`.        \n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `df`: pandas.DataFrame, with [`unique_id`, `ds`] columns and `df`'s future exogenous.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `fcsts_df`: pandas.DataFrame, with `models` columns for point predictions.<br>\n",
    "        \"\"\"\n",
    "        cols = []\n",
    "        for model in self.models:\n",
    "            cols += [type(model).__name__ + n for n in model.loss.output_names]\n",
    "\n",
    "        col_idx = 0\n",
    "        h = self.models[0].h\n",
    "        fcsts = np.full((h * len(self.uids), len(cols)), fill_value=np.nan)\n",
    "        for model in self.models:\n",
    "            model_fcsts = model.predict(dataset=self.dataset, **data_kwargs)\n",
    "            # Append predictions in memory placeholder\n",
    "            output_length = len(model.loss.output_names)\n",
    "            fcsts[:,col_idx:col_idx+output_length] = model_fcsts\n",
    "            col_idx += output_length\n",
    "\n",
    "        # Declare predictions pd.DataFrame\n",
    "        fcsts_df = self._make_future_df(h=h)\n",
    "        fcsts = pd.DataFrame.from_records(fcsts, columns=cols, \n",
    "                                          index=fcsts_df.index)\n",
    "        fcsts_df = pd.concat([fcsts_df, fcsts], axis=1)\n",
    "\n",
    "        return fcsts_df\n",
    "    \n",
    "    def cross_validation(self,\n",
    "                         df: pd.DataFrame = None,\n",
    "                         n_windows: int = 1,\n",
    "                         step_size: int = 1,\n",
    "                         val_size: Optional[int] = 0, \n",
    "                         test_size: Optional[int] = None,\n",
    "                         sort_df: bool = True,\n",
    "                         **data_kwargs):\n",
    "        \"\"\"Temporal Cross-Validation with core.NeuralForecast.\n",
    "\n",
    "        `core.NeuralForecast`'s cross-validation efficiently fits a list of NeuralForecast \n",
    "        models through multiple windows, in either chained or rolled manner.\n",
    "\n",
    "        *Parameters:*<br>\n",
    "        `df`: pandas.DataFrame, with columns [`unique_id`, `ds`, `y`] and exogenous.<br>\n",
    "        `n_windows`: int, number of windows used for cross validation.<br>\n",
    "        `step_size`: int = 1, step size between each window.<br>\n",
    "        `val_size`: Optional[int] = None, length of validation size. If passed, set `n_windows=None`.<br>\n",
    "        `test_size`: Optional[int] = None, length of test size. If passed, set `n_windows=None`.<br>\n",
    "\n",
    "        *Returns:*<br>\n",
    "        `fcsts_df`: pandas.DataFrame, with insample `models` columns for point predictions and probabilistic\n",
    "        predictions for all fitted `models`.<br>        \n",
    "        \"\"\"\n",
    "        cols = []\n",
    "        for model in self.models:\n",
    "            cols += [type(model).__name__ + n for n in model.loss.output_names]            \n",
    "\n",
    "        h = self.models[0].h\n",
    "        if test_size is None:\n",
    "            test_size = h + step_size * (n_windows - 1)\n",
    "        elif n_windows is None:\n",
    "            if (test_size - h) % step_size:\n",
    "                raise Exception('`test_size - h` should be module `step_size`')\n",
    "            n_windows = int((test_size - h) / step_size) + 1\n",
    "        elif (n_windows is None) and (test_size is None):\n",
    "            raise Exception('you must define `n_windows` or `test_size`')\n",
    "        else:\n",
    "            raise Exception('you must define `n_windows` or `test_size` but not both')\n",
    "\n",
    "        # Declare predictions pd.DataFrame\n",
    "        self._prepare_fit(df=df, sort_df=sort_df)\n",
    "        fcsts_df = _cv_dates(last_dates=self.last_dates, freq=self.freq, \n",
    "                             h=h, test_size=test_size, step_size=step_size)\n",
    "        idx = pd.Index(np.repeat(self.uids, h * n_windows), name='unique_id')\n",
    "        fcsts_df.index = idx\n",
    "\n",
    "        col_idx = 0\n",
    "        fcsts = np.full((self.dataset.n_groups * h * n_windows, len(cols)),\n",
    "                         np.nan, dtype=np.float32)\n",
    "        for model in self.models:\n",
    "            model.fit(dataset=self.dataset,\n",
    "                      val_size=val_size, \n",
    "                      test_size=test_size)            \n",
    "            model_fcsts = model.predict(self.dataset, step_size=step_size, **data_kwargs)\n",
    "\n",
    "            # Append predictions in memory placeholder\n",
    "            output_length = len(model.loss.output_names)\n",
    "            fcsts[:,col_idx:(col_idx + output_length)] = model_fcsts\n",
    "            col_idx += output_length                \n",
    "\n",
    "        # Add predictions to forecasts DataFrame\n",
    "        fcsts = pd.DataFrame.from_records(fcsts, columns=cols, \n",
    "                                          index=fcsts_df.index)\n",
    "        fcsts_df = pd.concat([fcsts_df, fcsts], axis=1)\n",
    "\n",
    "        # Add original input df's y to forecasts DataFrame\n",
    "        fcsts_df = fcsts_df.merge(df, how='left', on=['unique_id', 'ds'])\n",
    "        return fcsts_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898e349-8000-4668-a1c5-52c03c69e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bede563-78c0-40ee-ba76-f06f329cd772",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NeuralForecast.fit, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90209f6-16da-40a6-8302-1c5c2f66c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NeuralForecast.predict, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8923a-f4f3-4e60-b9b9-a7088fc9bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NeuralForecast.cross_validation, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534d29d-eecc-43ba-8468-c23305fa24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "from neuralforecast.auto import (\n",
    "    AutoMLP, AutoNBEATS, AutoDilatedRNN\n",
    ")\n",
    "\n",
    "from neuralforecast.models.mlp import MLP\n",
    "from neuralforecast.models.tft import TFT\n",
    "from neuralforecast.models.nhits import NHITS\n",
    "from neuralforecast.models.nbeats import NBEATS\n",
    "from neuralforecast.models.dilated_rnn import DilatedRNN\n",
    "\n",
    "from neuralforecast.losses.pytorch import MQLoss\n",
    "from neuralforecast.utils import AirPassengersDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3779a6-2d03-4ac3-9f01-8bd5cb306845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "config = {'input_size': tune.choice([12, 24]), \n",
    "          'hidden_size': 256,\n",
    "          'max_epochs': 1,\n",
    "          'step_size': 12}\n",
    "\n",
    "config_drnn = {'input_size': tune.choice([12, 24]), \n",
    "               'state_hsize': tune.choice([50, 100]),\n",
    "               'max_epochs': 1,\n",
    "               'step_size': 1}\n",
    "\n",
    "fcst = NeuralForecast(\n",
    "    models=[\n",
    "        AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=2),\n",
    "        DilatedRNN(h=12, input_size=12, state_hsize=50, max_epochs=1),\n",
    "        AutoMLP(h=12, config=config, cpus=1, num_samples=2),\n",
    "        NHITS(h=12, input_size=24, loss=MQLoss(level=[80]), max_epochs=1),\n",
    "        NHITS(h=12, input_size=12, max_epochs=1),\n",
    "        TFT(h=12, input_size=24, max_epochs=1),\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "fcst.fit(AirPassengersDF)\n",
    "forecasts = fcst.predict()\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40038532-fd68-4375-b7da-ba5bc2491c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "plot_df = pd.concat([AirPassengersDF, forecasts]).set_index('ds')\n",
    "\n",
    "plot_df.drop('unique_id', axis=1).plot(ax=ax, linewidth=2)\n",
    "\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8162a-3d9d-48df-a314-3a2ce0377e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst = NeuralForecast(\n",
    "    models=[\n",
    "        DilatedRNN(h=12, input_size=12,  state_hsize=50, max_epochs=10),\n",
    "        AutoMLP(h=12, config=config, cpus=1, num_samples=1),\n",
    "        NHITS(h=12, input_size=12, max_epochs=10)\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "cv_df = fcst.cross_validation(AirPassengersDF, n_windows=3, step_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5ea12-ed87-4e46-ad04-3088e7167dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test cross validation no leakage\n",
    "def test_cross_validation(df, h, test_size):\n",
    "    if (test_size - h) % 1:\n",
    "        raise Exception(\"`test_size - h` should be module `step_size`\")\n",
    "    \n",
    "    n_windows = int((test_size - h) / 1) + 1\n",
    "    Y_test_df = df.groupby('unique_id').tail(test_size)\n",
    "    Y_train_df = df.drop(Y_test_df.index)\n",
    "    config = {'input_size': tune.choice([12, 24]), 'h': h, \n",
    "              'step_size': 12, 'hidden_size': 256, 'max_epochs': 1}\n",
    "    config_drnn = {'input_size': tune.choice([12, 24]), 'state_hsize': tune.choice([50, 100]),\n",
    "                   'h': 12, 'max_epochs': 1, 'step_size': 1}\n",
    "    fcst = NeuralForecast(\n",
    "        models=[\n",
    "            AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=1),\n",
    "            DilatedRNN(h=12, input_size=12, state_hsize=50, max_epochs=1),\n",
    "            AutoMLP(h=12, config=config, cpus=1, num_samples=1),\n",
    "            NHITS(h=12, input_size=12, max_epochs=1, normalize=True),\n",
    "            NHITS(h=12, input_size=12, loss=MQLoss(level=[80]), max_epochs=1),\n",
    "            TFT(h=12, input_size=24, max_epochs=1, normalize=True),\n",
    "        ],\n",
    "        freq='M'\n",
    "    )\n",
    "    fcst.fit(Y_train_df)\n",
    "    Y_hat_df = fcst.predict()\n",
    "    Y_hat_df = Y_hat_df.merge(Y_test_df, how='left', on=['unique_id', 'ds'])\n",
    "    last_dates = Y_train_df.groupby('unique_id').tail(1)\n",
    "    last_dates = last_dates[['unique_id', 'ds']].rename(columns={'ds': 'cutoff'})\n",
    "    Y_hat_df = Y_hat_df.merge(last_dates, how='left', on='unique_id')\n",
    "    \n",
    "    #cross validation\n",
    "    fcst = NeuralForecast(\n",
    "        models=[\n",
    "            AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=1),\n",
    "            DilatedRNN(h=12, input_size=12, state_hsize=50, max_epochs=1),\n",
    "            AutoMLP(h=12, config=config, cpus=1, num_samples=1),\n",
    "            NHITS(h=12, input_size=12, max_epochs=1, normalize=True),\n",
    "            NHITS(h=12, input_size=12, loss=MQLoss(level=[80]), max_epochs=1),\n",
    "            TFT(h=12, input_size=24, max_epochs=1, normalize=True),\n",
    "        ],\n",
    "        freq='M'\n",
    "    )\n",
    "    Y_hat_df_cv = fcst.cross_validation(df, test_size=test_size, \n",
    "                                        n_windows=None)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        Y_hat_df[Y_hat_df_cv.columns],\n",
    "        Y_hat_df_cv,\n",
    "        check_dtype=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0467904-748e-42ec-99cc-bac514626304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_cross_validation(AirPassengersDF, h=12, test_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d784c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "neuralforecast"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
