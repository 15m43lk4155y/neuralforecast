{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkOrange\"> Core </span>\n",
    "> NeuralForecast contains two main components, PyTorch implementations deep learning predictive models, as well as parallelization and distributed computation utilities. The first component comprises low-level PyTorch model estimator classes like `models.NBEATS` and `models.RNN`. The second component is a high-level `core.NeuralForecast` wrapper class that operates with sets of time series data stored in pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.models import (DilatedRNN, GMM_TFT, TFT, GRU, LSTM,\n",
    "                                   RNN, NBEATS, NBEATSx, NHITS, MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45471e9f-4050-4a6c-bb05-4de144754da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _cv_dates(last_dates, freq, h, test_size, step_size=1):\n",
    "    #assuming step_size = 1\n",
    "    if (test_size - h) % step_size:\n",
    "        raise Exception('`test_size - h` should be module `step_size`')\n",
    "    n_windows = int((test_size - h) / step_size) + 1\n",
    "    if len(np.unique(last_dates)) == 1:\n",
    "        if issubclass(last_dates.dtype.type, np.integer):\n",
    "            total_dates = np.arange(last_dates[0] - test_size + 1, last_dates[0] + 1)\n",
    "            out = np.empty((h * n_windows, 2), dtype=last_dates.dtype)\n",
    "            freq = 1\n",
    "        else:\n",
    "            total_dates = pd.date_range(end=last_dates[0], periods=test_size, freq=freq)\n",
    "            out = np.empty((h * n_windows, 2), dtype='datetime64[s]')\n",
    "        for i_window, cutoff in enumerate(range(-test_size, -h + 1, step_size), start=0):\n",
    "            end_cutoff = cutoff + h\n",
    "            out[h * i_window : h * (i_window + 1), 0] = total_dates[cutoff:] if end_cutoff == 0 else total_dates[cutoff:end_cutoff]\n",
    "            out[h * i_window : h * (i_window + 1), 1] = np.tile(total_dates[cutoff] - freq, h)\n",
    "        dates = pd.DataFrame(np.tile(out, (len(last_dates), 1)), columns=['ds', 'cutoff'])\n",
    "    else:\n",
    "        dates = pd.concat([_cv_dates(np.array([ld]), freq, h, test_size, step_size) for ld in last_dates])\n",
    "        dates = dates.reset_index(drop=True)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff82c9-3cb5-407f-be34-141d265e8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "ds_int_cv_test = pd.DataFrame({\n",
    "    'ds': np.hstack([\n",
    "        [46, 47, 48],\n",
    "        [47, 48, 49],\n",
    "        [48, 49, 50]\n",
    "    ]),\n",
    "    'cutoff': [45] * 3 + [46] * 3 + [47] * 3\n",
    "}, dtype=np.int64)\n",
    "test_eq(ds_int_cv_test, _cv_dates(np.array([50], dtype=np.int64), 'D', 3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac141f22-ee59-408a-a5e8-4a5f72cb77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "ds_int_cv_test = pd.DataFrame({\n",
    "    'ds': np.hstack([\n",
    "        [46, 47, 48],\n",
    "        [48, 49, 50]\n",
    "    ]),\n",
    "    'cutoff': [45] * 3 + [47] * 3\n",
    "}, dtype=np.int64)\n",
    "test_eq(ds_int_cv_test, _cv_dates(np.array([50], dtype=np.int64), 'D', 3, 5, step_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2874985-3d14-40d2-a607-a774ed4abd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for e_e in [True, False]:\n",
    "    n_series = 2\n",
    "    ga, indices, dates, ds = TimeSeriesDataset.from_df(generate_series(n_series, equal_ends=e_e), sort_df=True)\n",
    "    freq = pd.tseries.frequencies.to_offset('D')\n",
    "    horizon = 3\n",
    "    test_size = 5\n",
    "    df_dates = _cv_dates(last_dates=dates, freq=freq, h=horizon, test_size=test_size)\n",
    "    test_eq(len(df_dates), n_series * horizon * (test_size - horizon + 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "MODEL_FILENAME_DICT = {'dilatedrnn': DilatedRNN, 'gmm_tft': GMM_TFT, 'gru': GRU, 'lstm': LSTM,\n",
    "                       'mlp': MLP, 'nbeats': NBEATS, 'nbeatsx': NBEATSx, 'nhits': NHITS, 'rnn': RNN, 'tft': TFT,\n",
    "                       'autodilatedrnn': DilatedRNN, 'autogru': GRU, 'autolstm': LSTM,\n",
    "                       'automlp': MLP, 'autonbeats': NBEATS, 'autonhits': NHITS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dae43c-4d11-4bbc-a431-ac33b004859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NeuralForecast:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 models: List[Any],\n",
    "                 freq: str):\n",
    "        \"\"\"\n",
    "        The `core.StatsForecast` class allows you to efficiently fit multiple `NeuralForecast` models \n",
    "        for large sets of time series. It operates with pandas DataFrame `df` that identifies series \n",
    "        and datestamps with the `unique_id` and `ds` columns. The `y` column denotes the target \n",
    "        time series variable.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `h`: int, forecast horizon.<br>\n",
    "        `models`: List[typing.Any], instantiated `neuralforecast.models` see [collection here](https://nixtla.github.io/neuralforecast/models.html).<br>\n",
    "        `freq`: str, frequency of the data, [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).<br>\n",
    "        `trainers`: List[typing.Any], optional list of instantiated pytorch lightning trainers.<br>\n",
    "        \"\"\"\n",
    "        assert all(model.h == models[0].h for model in models), 'All models should have the same horizon'\n",
    "\n",
    "        self.h = models[0].h\n",
    "        self.models = models\n",
    "        self.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "\n",
    "        # Flags and attributes\n",
    "        self._fitted = False\n",
    "        self._dataset_stored = False\n",
    "\n",
    "    def _prepare_fit(self, df, sort_df):\n",
    "        #TODO: uids, last_dates and ds should be properties of the dataset class. See github issue.\n",
    "        self.dataset, self.uids, self.last_dates, self.ds = TimeSeriesDataset.from_df(df=df, sort_df=sort_df)\n",
    "        self.sort_df = sort_df\n",
    "        self._dataset_stored = True\n",
    "\n",
    "    def fit(self,\n",
    "            df: Optional[pd.DataFrame] = None,\n",
    "            val_size: Optional[int] = 0,\n",
    "            sort_df: bool = True):\n",
    "        \"\"\"Fit the core.NeuralForecast.\n",
    "\n",
    "        Fit `models` to a large set of time series from DataFrame `df`.\n",
    "        and store fitted models for later inspection.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `df`: pandas.DataFrame, with columns [`unique_id`, `ds`, `y`] and exogenous.<br>\n",
    "        `val_size`: int, size of validation set.<br>\n",
    "        `sort_df`: bool, sort df before fitting.\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `self`: Returns with stored `NeuralForecast` fitted `models`.\n",
    "        \"\"\"\n",
    "        assert (df is not None) or (self._dataset_stored), 'You need to provide a df or have a stored dataset'\n",
    "\n",
    "        # Process and save new dataset (in self)\n",
    "        if df is not None:\n",
    "            self._prepare_fit(df=df, sort_df=sort_df)\n",
    "        else:\n",
    "            print('Using stored dataset.')\n",
    "\n",
    "        #train + validation\n",
    "        for model in self.models:\n",
    "            model.fit(self.dataset, val_size=val_size)\n",
    "        #train with the full dataset\n",
    "\n",
    "        self._fitted = True\n",
    "\n",
    "    def _make_future_df(self, h: int):\n",
    "        if issubclass(self.last_dates.dtype.type, np.integer):\n",
    "            last_date_f = lambda x: np.arange(x + 1, x + 1 + h, dtype=self.last_dates.dtype)\n",
    "        else:\n",
    "            last_date_f = lambda x: pd.date_range(x + self.freq, periods=h, freq=self.freq)\n",
    "        if len(np.unique(self.last_dates)) == 1:\n",
    "            dates = np.tile(last_date_f(self.last_dates[0]), len(self.dataset))\n",
    "        else:\n",
    "            dates = np.hstack([last_date_f(last_date)\n",
    "                               for last_date in self.last_dates])\n",
    "        idx = pd.Index(np.repeat(self.uids, h), name='unique_id')\n",
    "        df = pd.DataFrame({'ds': dates}, index=idx)\n",
    "        return df\n",
    "\n",
    "    def predict(self,\n",
    "                df: Optional[pd.DataFrame] = None,\n",
    "                futr_df: Optional[pd.DataFrame] = None,\n",
    "                sort_df: bool = True,\n",
    "                **data_kwargs):\n",
    "        \"\"\"Predict with core.NeuralForecast.\n",
    "\n",
    "        Use stored fitted `models` to predict large set of time series from DataFrame `df`.        \n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `futr_df`: pandas.DataFrame, with [`unique_id`, `ds`] columns and `df`'s future exogenous.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `fcsts_df`: pandas.DataFrame, with `models` columns for point predictions.<br>\n",
    "        \"\"\"\n",
    "        assert (df is not None) or (self._dataset_stored), 'You need to provide a df or have a stored dataset'\n",
    "\n",
    "        # Process and save new dataset (in self)\n",
    "        if df is not None:\n",
    "            self._prepare_fit(df=df, sort_df=sort_df)\n",
    "        else:\n",
    "            print('Using stored dataset.')\n",
    "\n",
    "        cols = []\n",
    "        count_names = {'model': 0}\n",
    "        for model in self.models:\n",
    "            model_name = type(model).__name__\n",
    "            count_names[model_name] = count_names.get(model_name, -1) + 1\n",
    "            if count_names[model_name] > 0:\n",
    "                model_name += str(count_names[model_name])\n",
    "            cols += [model_name + n for n in model.loss.output_names]\n",
    "\n",
    "        # Placeholder dataframe for predictions with unique_id and ds\n",
    "        fcsts_df = self._make_future_df(h=self.h)\n",
    "\n",
    "        # Update and define new forecasting dataset\n",
    "        if futr_df is not None:\n",
    "            dataset = TimeSeriesDataset.update_dataset(dataset=self.dataset, future_df=futr_df)\n",
    "        else:\n",
    "            dataset = TimeSeriesDataset.update_dataset(dataset=self.dataset, future_df=fcsts_df.reset_index())\n",
    "\n",
    "        col_idx = 0\n",
    "        fcsts = np.full((self.h * len(self.uids), len(cols)), fill_value=np.nan)\n",
    "        for model in self.models:\n",
    "            model.set_test_size(self.h) # To predict h steps ahead\n",
    "            model_fcsts = model.predict(dataset=dataset, **data_kwargs)\n",
    "            # Append predictions in memory placeholder\n",
    "            output_length = len(model.loss.output_names)\n",
    "            fcsts[:,col_idx:col_idx+output_length] = model_fcsts\n",
    "            col_idx += output_length\n",
    "\n",
    "        # Declare predictions pd.DataFrame\n",
    "        fcsts = pd.DataFrame.from_records(fcsts, columns=cols, \n",
    "                                          index=fcsts_df.index)\n",
    "        fcsts_df = pd.concat([fcsts_df, fcsts], axis=1)\n",
    "\n",
    "        return fcsts_df\n",
    "    \n",
    "    def cross_validation(self,\n",
    "                         df: pd.DataFrame = None,\n",
    "                         n_windows: int = 1,\n",
    "                         step_size: int = 1,\n",
    "                         val_size: Optional[int] = 0, \n",
    "                         test_size: Optional[int] = None,\n",
    "                         sort_df: bool = True,\n",
    "                         **data_kwargs):\n",
    "        \"\"\"Temporal Cross-Validation with core.NeuralForecast.\n",
    "\n",
    "        `core.NeuralForecast`'s cross-validation efficiently fits a list of NeuralForecast \n",
    "        models through multiple windows, in either chained or rolled manner.\n",
    "\n",
    "        *Parameters:*<br>\n",
    "        `df`: pandas.DataFrame, with columns [`unique_id`, `ds`, `y`] and exogenous.<br>\n",
    "        `n_windows`: int, number of windows used for cross validation.<br>\n",
    "        `step_size`: int = 1, step size between each window.<br>\n",
    "        `val_size`: Optional[int] = None, length of validation size. If passed, set `n_windows=None`.<br>\n",
    "        `test_size`: Optional[int] = None, length of test size. If passed, set `n_windows=None`.<br>\n",
    "\n",
    "        *Returns:*<br>\n",
    "        `fcsts_df`: pandas.DataFrame, with insample `models` columns for point predictions and probabilistic\n",
    "        predictions for all fitted `models`.<br>        \n",
    "        \"\"\"\n",
    "        assert (df is not None) or (self._dataset_stored), 'You need to provide a df or have a stored dataset'\n",
    "\n",
    "        # Declare predictions pd.DataFrame\n",
    "        if df is not None:\n",
    "            self._prepare_fit(df=df, sort_df=sort_df)\n",
    "        else:\n",
    "            print('Using stored dataset.')\n",
    "\n",
    "        cols = []\n",
    "        count_names = {'model': 0}\n",
    "        for model in self.models:\n",
    "            model_name = type(model).__name__\n",
    "            count_names[model_name] = count_names.get(model_name, -1) + 1\n",
    "            if count_names[model_name] > 0:\n",
    "                model_name += str(count_names[model_name])\n",
    "            cols += [model_name + n for n in model.loss.output_names]            \n",
    "\n",
    "        h = self.models[0].h\n",
    "        if test_size is None:\n",
    "            test_size = h + step_size * (n_windows - 1)\n",
    "        elif n_windows is None:\n",
    "            if (test_size - h) % step_size:\n",
    "                raise Exception('`test_size - h` should be module `step_size`')\n",
    "            n_windows = int((test_size - h) / step_size) + 1\n",
    "        elif (n_windows is None) and (test_size is None):\n",
    "            raise Exception('you must define `n_windows` or `test_size`')\n",
    "        else:\n",
    "            raise Exception('you must define `n_windows` or `test_size` but not both')\n",
    "\n",
    "        fcsts_df = _cv_dates(last_dates=self.last_dates, freq=self.freq, \n",
    "                             h=h, test_size=test_size, step_size=step_size)\n",
    "        idx = pd.Index(np.repeat(self.uids, h * n_windows), name='unique_id')\n",
    "        fcsts_df.index = idx\n",
    "\n",
    "        col_idx = 0\n",
    "        fcsts = np.full((self.dataset.n_groups * h * n_windows, len(cols)),\n",
    "                         np.nan, dtype=np.float32)\n",
    "        for model in self.models:\n",
    "            model.fit(dataset=self.dataset,\n",
    "                      val_size=val_size, \n",
    "                      test_size=test_size)            \n",
    "            model_fcsts = model.predict(self.dataset, step_size=step_size, **data_kwargs)\n",
    "\n",
    "            # Append predictions in memory placeholder\n",
    "            output_length = len(model.loss.output_names)\n",
    "            fcsts[:,col_idx:(col_idx + output_length)] = model_fcsts\n",
    "            col_idx += output_length                \n",
    "\n",
    "        # Add predictions to forecasts DataFrame\n",
    "        fcsts = pd.DataFrame.from_records(fcsts, columns=cols, \n",
    "                                          index=fcsts_df.index)\n",
    "        fcsts_df = pd.concat([fcsts_df, fcsts], axis=1)\n",
    "\n",
    "        # Add original input df's y to forecasts DataFrame\n",
    "        fcsts_df = fcsts_df.merge(df, how='left', on=['unique_id', 'ds'])\n",
    "        return fcsts_df\n",
    "        \n",
    "    # Save list of models with pytorch lightning save_checkpoint function\n",
    "    def save(self, path: str, model_index: Optional[List]=None, save_dataset: bool=True, overwrite: bool=False):\n",
    "        \"\"\" Save NeuralForecast core class.\n",
    "\n",
    "        `core.NeuralForecast`'s method to save current status of models, dataset, and configuration.\n",
    "\n",
    "        *Parameters:*<br>\n",
    "        `path`: str, directory to save current status.<br>\n",
    "        `model_index`: Optional[List] = None, optional list to specify which models from list of self.models to save.<br>\n",
    "        `save_dataset`: bool = True, whether to save dataset or not.<br>\n",
    "        `overwrite`: bool = False, whether to overwrite files or not.<br>\n",
    "        \"\"\"\n",
    "        # Standarize path without '/'\n",
    "        if path[-1] == '/':\n",
    "            path = path[:-1]\n",
    "\n",
    "        # Model index list\n",
    "        if model_index is None:\n",
    "            model_index = list(range(len(self.models)))\n",
    "\n",
    "        # Create directory if not exists\n",
    "        os.makedirs(path, exist_ok = True)\n",
    "\n",
    "        # Check if directory is empty to protect overwriting files\n",
    "        dir = os.listdir(path)\n",
    "        \n",
    "        # Checking if the list is empty or not\n",
    "        if (len(dir) > 0) and (not overwrite):\n",
    "            raise Exception('Directory is not empty. Set `overwrite=True` to overwrite files.')\n",
    "\n",
    "        # Save models\n",
    "        count_names = {'model': 0}\n",
    "        for i, model in enumerate(self.models):\n",
    "            # Skip model if not in list\n",
    "            if i not in model_index:\n",
    "                continue\n",
    "\n",
    "            model_name = type(model).__name__.lower().replace('_', '')\n",
    "            count_names[model_name] = count_names.get(model_name, -1) + 1\n",
    "            model.save(f\"{path}/{model_name}_{count_names[model_name]}.ckpt\")\n",
    "\n",
    "        # Save dataset\n",
    "        if (save_dataset) and (self._dataset_stored):\n",
    "            with open(f\"{path}/dataset.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.dataset, f)\n",
    "        elif save_dataset:\n",
    "            raise Exception('You need to have a stored dataset to save it, \\\n",
    "                             set `save_dataset=False` to skip saving dataset.')\n",
    "\n",
    "        # Save configuration and parameters\n",
    "        config_dict = {'h': self.h,\n",
    "                       'freq': self.freq,\n",
    "                       'uids': self.uids,\n",
    "                       'last_dates': self.last_dates,\n",
    "                       'ds': self.ds,\n",
    "                       'sort_df': self.sort_df,\n",
    "                       '_fitted': self._fitted}\n",
    "\n",
    "        with open(f\"{path}/configuration.pkl\", \"wb\") as f:\n",
    "                pickle.dump(config_dict, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \"\"\" Load NeuralForecast\n",
    "\n",
    "        `core.NeuralForecast`'s method to load checkpoint from path.\n",
    "\n",
    "        *Parameters:*<br>\n",
    "        `path`: str, directory to save current status.<br>\n",
    "        \"\"\"\n",
    "        files = [f for f in os.listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "        # Load models\n",
    "        models_ckpt = [f for f in files if f.endswith('.ckpt')]\n",
    "        if len(models_ckpt) == 0:\n",
    "            raise Exception('No model found in directory.') \n",
    "        \n",
    "        print(10 * '-' + ' Loading models ' + 10 * '-')\n",
    "        models = []\n",
    "        for model in models_ckpt:\n",
    "            model_name = model.split('_')[0]\n",
    "            models.append(MODEL_FILENAME_DICT[model_name].load_from_checkpoint(f\"{path}/{model}\"))\n",
    "            print(f\"Model {model_name} loaded.\")\n",
    "\n",
    "        print(f\"Loaded {len(models)} models.\")\n",
    "\n",
    "        print(10*'-' + ' Loading dataset ' + 10*'-')\n",
    "        # Load dataset\n",
    "        if 'dataset.pkl' in files:\n",
    "            with open(f\"{path}/dataset.pkl\", \"rb\") as f:\n",
    "                dataset = pickle.load(f)\n",
    "            print('Dataset loaded.')\n",
    "        else:\n",
    "            dataset = None\n",
    "            print('No dataset found in directory.')\n",
    "        \n",
    "        print(10*'-' + ' Loading configuration ' + 10*'-')\n",
    "        # Load configuration\n",
    "        if 'configuration.pkl' in files:\n",
    "            with open(f\"{path}/configuration.pkl\", \"rb\") as f:\n",
    "                config_dict = pickle.load(f)\n",
    "            print('Configuration loaded.')\n",
    "        else:\n",
    "            raise Exception('No configuration found in directory.')\n",
    "\n",
    "        # Create NeuralForecast object\n",
    "        neuralforecast = NeuralForecast(models=models, freq=config_dict['freq'])\n",
    "\n",
    "        # Dataset\n",
    "        if dataset is not None:\n",
    "            neuralforecast.dataset = dataset\n",
    "            neuralforecast.uids = config_dict['uids']\n",
    "            neuralforecast.last_dates = config_dict['last_dates']\n",
    "            neuralforecast.ds = config_dict['ds']\n",
    "            neuralforecast.sort_df = config_dict['sort_df']\n",
    "            neuralforecast._dataset_stored = True\n",
    "\n",
    "        # Fitted flag\n",
    "        neuralforecast._fitted = config_dict['_fitted']\n",
    "\n",
    "        return neuralforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898e349-8000-4668-a1c5-52c03c69e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bede563-78c0-40ee-ba76-f06f329cd772",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NeuralForecast.fit, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90209f6-16da-40a6-8302-1c5c2f66c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NeuralForecast.predict, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8923a-f4f3-4e60-b9b9-a7088fc9bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NeuralForecast.cross_validation, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534d29d-eecc-43ba-8468-c23305fa24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "from neuralforecast.auto import (\n",
    "    AutoMLP, AutoNBEATS, AutoDilatedRNN\n",
    ")\n",
    "\n",
    "from neuralforecast.models.mlp import MLP\n",
    "from neuralforecast.models.tft import TFT\n",
    "from neuralforecast.models.nhits import NHITS\n",
    "from neuralforecast.models.nbeats import NBEATS\n",
    "from neuralforecast.models.nbeatsx import NBEATSx\n",
    "from neuralforecast.models.dilated_rnn import DilatedRNN\n",
    "from neuralforecast.models.rnn import RNN\n",
    "\n",
    "from neuralforecast.losses.pytorch import MQLoss\n",
    "from neuralforecast.utils import AirPassengersDF, AirPassengersPanel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "AirPassengersPanel_train = AirPassengersPanel[AirPassengersPanel['ds'] < AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\n",
    "AirPassengersPanel_test = AirPassengersPanel[AirPassengersPanel['ds'] >= AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\n",
    "AirPassengersPanel_test['y'] = np.nan\n",
    "AirPassengersPanel_test['y_[lag12]'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3779a6-2d03-4ac3-9f01-8bd5cb306845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "config = {'input_size': tune.choice([12, 24]), \n",
    "          'hidden_size': 256,\n",
    "          'max_epochs': 1,\n",
    "          'step_size': 12}\n",
    "\n",
    "config_drnn = {'input_size': tune.choice([12, 24]), \n",
    "               'state_hsize': tune.choice([50, 100]),\n",
    "               'max_epochs': 1,\n",
    "               'step_size': 1}\n",
    "\n",
    "fcst = NeuralForecast(\n",
    "    models=[\n",
    "        AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=2),\n",
    "        DilatedRNN(h=12, input_size=12, state_hsize=50, max_epochs=1,\n",
    "                   futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        RNN(h=12, input_size=12, state_hsize=50, max_epochs=1,\n",
    "            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        AutoMLP(h=12, config=config, cpus=1, num_samples=2),\n",
    "        NBEATSx(h=12, input_size=12, max_epochs=1,\n",
    "                futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        NHITS(h=12, input_size=24, loss=MQLoss(level=[80]), max_epochs=1),\n",
    "        NHITS(h=12, input_size=12, max_epochs=1,\n",
    "              futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        MLP(h=12, input_size=12, max_epochs=1,\n",
    "            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "        TFT(h=12, input_size=24, max_epochs=1),\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "fcst.fit(AirPassengersPanel_train)\n",
    "forecasts = fcst.predict(futr_df=AirPassengersPanel_test)\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40038532-fd68-4375-b7da-ba5bc2491c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "plot_df = pd.concat([AirPassengersPanel_train, forecasts.reset_index()]).set_index('ds')\n",
    "\n",
    "plot_df[plot_df['unique_id']=='Airline1'].drop(['unique_id','trend','y_[lag12]'], axis=1).plot(ax=ax, linewidth=2)\n",
    "\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "plot_df = pd.concat([AirPassengersPanel_train, forecasts.reset_index()]).set_index('ds')\n",
    "\n",
    "plot_df[plot_df['unique_id']=='Airline2'].drop(['unique_id','trend','y_[lag12]'], axis=1).plot(ax=ax, linewidth=2)\n",
    "\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8162a-3d9d-48df-a314-3a2ce0377e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst = NeuralForecast(\n",
    "    models=[\n",
    "        DilatedRNN(h=12, input_size=12,  state_hsize=50, max_epochs=10),\n",
    "        AutoMLP(h=12, config=config, cpus=1, num_samples=1),\n",
    "        NHITS(h=12, input_size=12, max_epochs=10)\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "cv_df = fcst.cross_validation(AirPassengersPanel, n_windows=3, step_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5ea12-ed87-4e46-ad04-3088e7167dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test cross validation no leakage\n",
    "def test_cross_validation(df, h, test_size):\n",
    "    if (test_size - h) % 1:\n",
    "        raise Exception(\"`test_size - h` should be module `step_size`\")\n",
    "    \n",
    "    n_windows = int((test_size - h) / 1) + 1\n",
    "    Y_test_df = df.groupby('unique_id').tail(test_size)\n",
    "    Y_train_df = df.drop(Y_test_df.index)\n",
    "    config = {'input_size': tune.choice([12, 24]), 'h': h, \n",
    "              'step_size': 12, 'hidden_size': 256, 'max_epochs': 1}\n",
    "    config_drnn = {'input_size': tune.choice([12, 24]), 'state_hsize': tune.choice([50, 100]),\n",
    "                   'h': 12, 'max_epochs': 1, 'step_size': 1}\n",
    "    fcst = NeuralForecast(\n",
    "        models=[\n",
    "            AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=1),\n",
    "            DilatedRNN(h=12, input_size=12, state_hsize=50, max_epochs=1),\n",
    "            RNN(h=12, input_size=12, state_hsize=50, max_epochs=1,\n",
    "            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "            AutoMLP(h=12, config=config, cpus=1, num_samples=1),\n",
    "            MLP(h=12, input_size=12, max_epochs=1, scaler_type='robust'),\n",
    "            NBEATSx(h=12, input_size=12, max_epochs=1,\n",
    "                futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "            NHITS(h=12, input_size=12, max_epochs=1, scaler_type='robust'),\n",
    "            NHITS(h=12, input_size=12, loss=MQLoss(level=[80]), max_epochs=1),\n",
    "            TFT(h=12, input_size=24, max_epochs=1, scaler_type='robust')\n",
    "        ],\n",
    "        freq='M'\n",
    "    )\n",
    "    fcst.fit(Y_train_df)\n",
    "    Y_hat_df = fcst.predict(futr_df=Y_test_df)\n",
    "    Y_hat_df = Y_hat_df.merge(Y_test_df, how='left', on=['unique_id', 'ds'])\n",
    "    last_dates = Y_train_df.groupby('unique_id').tail(1)\n",
    "    last_dates = last_dates[['unique_id', 'ds']].rename(columns={'ds': 'cutoff'})\n",
    "    Y_hat_df = Y_hat_df.merge(last_dates, how='left', on='unique_id')\n",
    "    \n",
    "    #cross validation\n",
    "    fcst = NeuralForecast(\n",
    "        models=[\n",
    "            AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=1),\n",
    "            DilatedRNN(h=12, input_size=12, state_hsize=50, max_epochs=1),\n",
    "            RNN(h=12, input_size=12, state_hsize=50, max_epochs=1,\n",
    "            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "            AutoMLP(h=12, config=config, cpus=1, num_samples=1),\n",
    "            MLP(h=12, input_size=12, max_epochs=1, scaler_type='robust'),\n",
    "            NBEATSx(h=12, input_size=12, max_epochs=1,\n",
    "                futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n",
    "            NHITS(h=12, input_size=12, max_epochs=1, scaler_type='robust'),\n",
    "            NHITS(h=12, input_size=12, loss=MQLoss(level=[80]), max_epochs=1),\n",
    "            TFT(h=12, input_size=24, max_epochs=1, scaler_type='robust')\n",
    "        ],\n",
    "        freq='M'\n",
    "    )\n",
    "    Y_hat_df_cv = fcst.cross_validation(df, test_size=test_size, \n",
    "                                        n_windows=None)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        Y_hat_df[Y_hat_df_cv.columns],\n",
    "        Y_hat_df_cv,\n",
    "        check_dtype=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0467904-748e-42ec-99cc-bac514626304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_cross_validation(AirPassengersPanel, h=12, test_size=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "neuralforecast"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
