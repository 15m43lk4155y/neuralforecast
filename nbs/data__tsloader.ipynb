{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.tsloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeSeriesLoader\n",
    "> Data Loader for Time Series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import copy\n",
    "import logging\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Collection, Dict, List, Optional, Tuple, Union\n",
    "from typing_extensions import Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TimeSeriesLoader(object):\n",
    "    \"\"\"\n",
    "    DataLoader for Time Series data.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    ts_dataset: TimeSeriesDataset\n",
    "        Object of class TimeSeriesDataset.\n",
    "    t_cols: list\n",
    "        List of temporal variables (mask variables included).\n",
    "    f_cols: list\n",
    "        List of exogenous variables of the future.\n",
    "    model: str\n",
    "        Model to be used.\n",
    "        One of ['nbeats', 'esrnn'].\n",
    "    window_sampling_limit: int\n",
    "        Max size of observations to consider, including output_size.\n",
    "    input_size: int\n",
    "        Size of the training sets.\n",
    "    output_size: int\n",
    "        Forecast horizon.\n",
    "    idx_to_sample_freq: int\n",
    "        Step size to construct windows.\n",
    "        Ej. if idx_to_sample_freq=7, each 7 timestamps\n",
    "        a window will be constructed.\n",
    "    batch_size: int\n",
    "        Number of samples considered in each iteration.\n",
    "    complete_inputs: bool\n",
    "        Whether consider only windows of length equals to input_size.\n",
    "    shuffle: bool\n",
    "        Shuffled batch.\n",
    "        If False, batch size will be ignored and\n",
    "        all windows will be used when training.\n",
    "    len_sample_chunks: Optional[int] = None\n",
    "        Size of complete windows.\n",
    "        Only used for model = 'esrnn'!\n",
    "        Default None, equls to input_size + ouput_size.\n",
    "    n_series_per_batch: Optional[int] = None\n",
    "        Number of time series per batch.\n",
    "    verbose: bool = False\n",
    "        Whether display informative messages.\n",
    "    windows_size: int\n",
    "        Size of the windows.\n",
    "        For model='nbeats', window_size=input_size + output_size.\n",
    "        For model='esrnn', window_size=len_sample_chunks.\n",
    "    padding: Tuple[int, int]\n",
    "        Tuple of left and right sizes of the padding.\n",
    "        Used to pad the ts_tensor with 0.\n",
    "        For model='nbeats', padding=(input_size, output_size).\n",
    "        For model='esrnn', padding=(0, 0).\n",
    "    sampleable_ts_idxs: np.ndarray\n",
    "        Indexes of sampleable time series.\n",
    "    n_sampleable_ts_idxs: int\n",
    "        Number of sampleable time series.\n",
    "    n_batches:\n",
    "        Number of batches given conditions.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    get_n_variables()\n",
    "        Returns Tuple of number of exogenous and static variables.\n",
    "    get_n_series()\n",
    "        Returns number of time series.\n",
    "    get_max_len()\n",
    "        Returns max length of the time series.\n",
    "    get_n_channels()\n",
    "        Returns number of channels.\n",
    "    get_frequency()\n",
    "        Returns infered frequency.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 ts_dataset: TimeSeriesDataset,\n",
    "                 model: Literal['nbeats', 'esrnn'],\n",
    "                 window_sampling_limit: int, \n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 idx_to_sample_freq: int,\n",
    "                 batch_size: int,\n",
    "                 complete_inputs: bool,\n",
    "                 shuffle: bool,\n",
    "                 len_sample_chunks: Optional[int] = None,\n",
    "                 n_series_per_batch: Optional[int] = None,\n",
    "                 verbose: bool = False) -> 'TimeSeriesLoader':\n",
    "        \"\"\"Instatiates loader for TimeSeriesDataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ts_dataset: TimeSeriesDataset\n",
    "            Object of class TimeSeriesDataset.\n",
    "        model: str\n",
    "            Model to be used.\n",
    "            One of ['nbeats', 'esrnn'].\n",
    "        window_sampling_limit: int\n",
    "            Max size of observations to consider, including output_size.\n",
    "        input_size: int\n",
    "            Size of the training sets.\n",
    "        output_size: int\n",
    "            Forecast horizon.\n",
    "        idx_to_sample_freq: int\n",
    "            Step size to construct windows.\n",
    "            Ej. if idx_to_sample_freq=7, each 7 timestamps\n",
    "            a window will be constructed.\n",
    "        batch_size: int\n",
    "            Number of samples considered in each iteration.\n",
    "        complete_inputs: bool\n",
    "            If complete_input=True\n",
    "            return all windows which its ouput_size \n",
    "            has complete sample_mask and its input_size\n",
    "            has complete available_mask.\n",
    "            If complete_input=False\n",
    "            returns all windows which its\n",
    "            output_size has complete sample_mask. \n",
    "            This avoids leakage.\n",
    "        shuffle: bool\n",
    "            Shuffled batch.\n",
    "            If False, batch size will be ignored and\n",
    "            all windows will be used when training.\n",
    "        len_sample_chunks: Optional[int] = None\n",
    "            Size of complete windows.\n",
    "            Only used for model = 'esrnn'!\n",
    "            Default None, equls to input_size + ouput_size.\n",
    "        n_series_per_batch: Optional[int] = None\n",
    "            Number of time series per batch.\n",
    "        verbose: bool = False\n",
    "            Whether display informative messages.\n",
    "        \"\"\"\n",
    "        # Dataloader attributes\n",
    "        self.model = model\n",
    "        self.window_sampling_limit = window_sampling_limit\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.complete_inputs = complete_inputs\n",
    "        self.idx_to_sample_freq = idx_to_sample_freq\n",
    "        self.ts_dataset = ts_dataset\n",
    "        self.t_cols = self.ts_dataset.t_cols\n",
    "        self.f_idxs = self.ts_dataset.f_idxs\n",
    "        if n_series_per_batch is not None:\n",
    "            self.n_series_per_batch = n_series_per_batch \n",
    "        else:\n",
    "            self.n_series_per_batch = min(batch_size, self.ts_dataset.n_series)\n",
    "            \n",
    "        if len_sample_chunks is not None:\n",
    "            if len_sample_chunks < self.input_size + self.output_size:\n",
    "                raise Exception(f'Insufficient len of sample chunks {len_sample_chunks}')\n",
    "            self.len_sample_chunks = len_sample_chunks\n",
    "        else:\n",
    "            self.len_sample_chunks = input_size + output_size\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        if not shuffle:\n",
    "            logging.warning('Batch size will be ignored (shuffle=False). '\n",
    "                            'All constructed windows will be used to train.')\n",
    "        \n",
    "        # Dataloader protections\n",
    "        assert self.batch_size % self.n_series_per_batch == 0, (\n",
    "            f'batch_size {self.batch_size} must be multiple of '\n",
    "            f'n_series_per_batch {self.n_series_per_batch}'\n",
    "        )\n",
    "        assert self.n_series_per_batch <= self.ts_dataset.n_series, (\n",
    "            f'n_series_per_batch {n_series_per_batch} needs '\n",
    "            f'to be smaller than n_series {self.ts_dataset.n_series}'\n",
    "        )\n",
    "                \n",
    "        # Defining windows attributes by model\n",
    "        self.windows_size: int\n",
    "        self.padding: Tuple[int, int]\n",
    "            \n",
    "        self._define_attributes_by_model()\n",
    "        \n",
    "        # Defining sampleable time series\n",
    "        self.sampleable_ts_idxs: np.ndarray\n",
    "        self.n_sampleable_ts: int\n",
    "            \n",
    "        self._define_sampleable_ts_idxs()\n",
    "        \n",
    "         # Loader iterations attributes\n",
    "        self.n_batches = int(np.ceil(self.n_sampleable_ts / self.n_series_per_batch)) # Must be multiple of batch_size for paralel gpu        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def _define_attributes_by_model(self: TimeSeriesLoader):\n",
    "    if self.model in ['nbeats']:\n",
    "        self.windows_size = self.input_size + self.output_size\n",
    "        self.padding = (self.input_size, self.output_size)\n",
    "    elif self.model in ['esrnn']:\n",
    "        self.windows_size = self.len_sample_chunks\n",
    "        self.padding = (0, 0)\n",
    "    else:\n",
    "        raise Exception(f'There is no batch strategy for {self.model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def _define_sampleable_ts_idxs(self: TimeSeriesLoader):\n",
    "    sum_sample_mask = self.ts_dataset.ts_tensor[:, self.t_cols.index('sample_mask')] \\\n",
    "                              .sum(axis=1)\n",
    "    if self.complete_inputs:\n",
    "        min_mask = self.windows_size\n",
    "    else:\n",
    "        min_mask = self.output_size\n",
    "    self.sampleable_ts_idxs = np.argwhere(sum_sample_mask > min_mask).reshape(1, -1)[0]\n",
    "    self.n_sampleable_ts = self.sampleable_ts_idxs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def _get_sampleable_windows_idxs(self: TimeSeriesLoader, \n",
    "                                 ts_windows_flatten: t.Tensor) -> np.ndarray:\n",
    "    \"\"\"Gets indexes of windows that fulfills conditions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ts_windows_flatten: t.Tensor\n",
    "        Tensor of shape (windows, n_channels, windows_size)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Numpy array of indexes of ts_windows_flatten that \n",
    "    fulfills conditions.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    [1] If complete_input=True\n",
    "    return all windows which its ouput_size \n",
    "    has complete sample_mask and its input_size\n",
    "    has complete available_mask.\n",
    "    [2] If complete_input=False\n",
    "    returns all windows which its\n",
    "    output_size has complete sample_mask. \n",
    "    This avoids leakage.\n",
    "    \"\"\"\n",
    "    sample_condition = t.sum(ts_windows_flatten[:, self.t_cols.index('sample_mask'), -self.output_size:], axis=1)\n",
    "    sample_condition = (sample_condition == self.output_size) * 1\n",
    "    if self.complete_inputs:\n",
    "        available_condition = t.sum(ts_windows_flatten[:, self.t_cols.index('available_mask'), :-self.output_size], axis=1)\n",
    "        available_condition = (available_condition == self.windows_size - self.output_size) * 1\n",
    "        sampling_idx = t.nonzero(available_condition * sample_condition > 0)\n",
    "    else:\n",
    "        sampling_idx = t.nonzero(sample_condition)\n",
    "\n",
    "    sampling_idx = sampling_idx.flatten().numpy()\n",
    "    assert sampling_idx.size > 0, (\n",
    "        'Check the data and masks as sample_idxs are empty, '\n",
    "        'check window_sampling_limit, input_size, output_size, masks'\n",
    "    )\n",
    "    \n",
    "    return sampling_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def _create_windows_tensor(self: TimeSeriesLoader, \n",
    "                           index: Optional[np.ndarray] = None) -> Tuple[t.Tensor, \n",
    "                                                                        t.Tensor,\n",
    "                                                                        t.Tensor]:\n",
    "    \"\"\"Creates windows of size windows_size from\n",
    "    the ts_tensor of the TimeSeriesDataset filtered by\n",
    "    window_sampling_limit and ts_idxs. The step of each window\n",
    "    is defined by idx_to_sample_freq.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    index: Optional[np.ndarray]\n",
    "        Indexes of time series to consider.\n",
    "        Default None: returns all ts.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of three elements:\n",
    "        - Windows tensor of shape (windows, channels, input_size + output_size)\n",
    "        - Static variables tensor of shape (windows * series, n_static)\n",
    "        - Time Series indexes for each window.\n",
    "    \"\"\"\n",
    "    # Default ts_idxs=ts_idxs sends all the data, otherwise filters series      \n",
    "    tensor, _ = self.ts_dataset \\\n",
    "                    .get_filtered_ts_tensor(output_size=self.output_size,\n",
    "                                            window_sampling_limit=self.window_sampling_limit,\n",
    "                                            ts_idxs=index)\n",
    "    tensor = t.Tensor(tensor)\n",
    "\n",
    "    padder = t.nn.ConstantPad1d(padding=self.padding, value=0)\n",
    "    tensor = padder(tensor)\n",
    "\n",
    "    # Creating rolling windows and 'flattens' them\n",
    "    windows = tensor.unfold(dimension=-1, \n",
    "                            size=self.windows_size, \n",
    "                            step=self.idx_to_sample_freq)\n",
    "    # n_serie, n_channel, n_time, window_size -> n_serie, n_time, n_channel, window_size\n",
    "    windows = windows.permute(0, 2, 1, 3)\n",
    "    windows = windows.reshape(-1, self.ts_dataset.n_channels, self.windows_size)\n",
    "    \n",
    "    # Broadcast s_matrix: This works because unfold in windows_tensor, orders: serie, time\n",
    "    s_matrix = self.ts_dataset.s_matrix[index]\n",
    "    n_ts = self.ts_dataset.n_series if index is None else len(index)\n",
    "    windows_per_serie = len(windows) / n_ts\n",
    "    s_matrix = s_matrix.repeat(repeats=windows_per_serie, axis=0)\n",
    "    ts_idxs = index.repeat(repeats=windows_per_serie)\n",
    "    \n",
    "    s_matrix = t.Tensor(s_matrix)\n",
    "    ts_idxs = t.as_tensor(ts_idxs, dtype=t.long)\n",
    "\n",
    "\n",
    "    return windows, s_matrix, ts_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def _windows_batch(self: TimeSeriesLoader, \n",
    "                   index: np.ndarray) -> Dict[str, t.Tensor]:\n",
    "    \"\"\"Creates batch based on index.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    index: np.ndarray\n",
    "        Indexes of time series to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary with keys:\n",
    "        - S\n",
    "        - Y\n",
    "        - X\n",
    "        - available_mask\n",
    "        - sample_mask\n",
    "        - idxs\n",
    "    \"\"\"\n",
    "\n",
    "    # Create windows for each sampled ts and sample random unmasked windows from each ts\n",
    "    windows, s_matrix, ts_idxs = self._create_windows_tensor(index=index)\n",
    "    sampleable_windows = self._get_sampleable_windows_idxs(ts_windows_flatten=windows)\n",
    "\n",
    "    # Get sample windows_idxs of batch\n",
    "    if self.shuffle:\n",
    "        windows_idxs = np.random.choice(sampleable_windows, self.batch_size, replace=True)\n",
    "    else:\n",
    "        windows_idxs = sampleable_windows\n",
    "\n",
    "    # Index the windows and s_matrix tensors of batch\n",
    "    windows = windows[windows_idxs]\n",
    "    S = s_matrix[windows_idxs]\n",
    "    ts_idxs = ts_idxs[windows_idxs]\n",
    "\n",
    "    # Parse windows to elements of batch\n",
    "    Y = windows[:, self.t_cols.index('y'), :]\n",
    "    X = windows[:, (self.t_cols.index('y') + 1):self.t_cols.index('available_mask'), :]\n",
    "    available_mask = windows[:, self.t_cols.index('available_mask'), :]\n",
    "    sample_mask = windows[:, self.t_cols.index('sample_mask'), :]\n",
    "\n",
    "    batch = {'S': S, 'Y': Y, 'X': X,\n",
    "             'available_mask': available_mask,\n",
    "             'sample_mask': sample_mask,\n",
    "             'idxs': ts_idxs}\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def __getitem__(self: TimeSeriesLoader, \n",
    "                index: Union[Collection[int], np.ndarray]) -> Dict[str, t.Tensor]:\n",
    "    \"\"\"Gets batch based on index.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    index: Collection[int]\n",
    "        Indexes of time series to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Batch corresponding to index.\n",
    "    \"\"\"\n",
    "    \n",
    "    return self._windows_batch(index=np.array(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def __iter__(self: TimeSeriesLoader) -> Dict[str, t.Tensor]:\n",
    "    \"\"\"Batch iterator.\"\"\"\n",
    "    # Hierarchical sampling\n",
    "    # 1. Sampling series\n",
    "    if self.shuffle:\n",
    "        sample_idxs = np.random.choice(a=self.sampleable_ts_idxs, \n",
    "                                       size=self.n_sampleable_ts, \n",
    "                                       replace=False)\n",
    "    else:\n",
    "        sample_idxs = np.array(self.sampleable_ts_idxs)\n",
    "\n",
    "    for idx in range(self.n_batches):\n",
    "        ts_idxs = sample_idxs[(idx * self.n_series_per_batch) : (idx + 1) * self.n_series_per_batch]\n",
    "        # 2. Sampling windows\n",
    "        batch = self[ts_idxs]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def get_n_variables(self: TimeSeriesLoader) -> Tuple[int, int]:\n",
    "    \"\"\"Gets number of exogenous and static variables.\"\"\"\n",
    "    return self.ts_dataset.n_x, self.ts_dataset.n_s\n",
    "\n",
    "@patch\n",
    "def get_n_series(self: TimeSeriesLoader) -> int:\n",
    "    \"\"\"Gets number of time series.\"\"\"\n",
    "    return self.ts_dataset.n_series\n",
    "\n",
    "@patch\n",
    "def get_max_len(self: TimeSeriesLoader) -> int:\n",
    "    \"\"\"Gets max len of time series.\"\"\"\n",
    "    return self.ts_dataset.max_len\n",
    "\n",
    "@patch\n",
    "def get_n_channels(self: TimeSeriesLoader) -> int:\n",
    "    \"\"\"Gets number of channels considered.\"\"\"\n",
    "    return self.ts_dataset.n_channels\n",
    "\n",
    "@patch\n",
    "def get_frequency(self: TimeSeriesLoader) -> str:\n",
    "    \"\"\"Gets infered frequency.\"\"\"\n",
    "    return self.ts_dataset.frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for TimeSeriesLoader's methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampleable ts idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtla.data.utils import create_synthetic_tsdata\n",
    "\n",
    "n_ts = 64\n",
    "Y_df, X_df, S_df = create_synthetic_tsdata(n_ts=n_ts, sort=True)\n",
    "ds_in_test = 10\n",
    "is_test = False\n",
    "batch_size = 8\n",
    "len_sample_chunks = 20 #only for ESRNN\n",
    "\n",
    "dataset = TimeSeriesDataset(Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                            ds_in_test=ds_in_test, \n",
    "                            is_test=is_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sampleable_ts(dataset, model, input_size, output_size, \n",
    "                       len_sample_chunks,\n",
    "                       complete_inputs, ds_in_test):\n",
    "    \"\"\"This function checks that the sample conditions of the loader \n",
    "    match the expected ones.\n",
    "    Assumptions for the test:\n",
    "    - Full sample mask.\n",
    "    - Positive time series.\n",
    "    These assumptions are satisfied by the function create_synthetic_tsdata.\n",
    "    \"\"\"\n",
    "    loader = TimeSeriesLoader(ts_dataset=dataset,\n",
    "                              model=model,\n",
    "                              window_sampling_limit=dataset.max_len,\n",
    "                              input_size=input_size,\n",
    "                              output_size=output_size,\n",
    "                              idx_to_sample_freq=1,\n",
    "                              batch_size=batch_size,\n",
    "                              complete_inputs=complete_inputs,\n",
    "                              len_sample_chunks=len_sample_chunks,\n",
    "                              shuffle=True)\n",
    "    \n",
    "    sizes = dataset.len_series\n",
    "    if complete_inputs:\n",
    "        if model in ['esrnn']:\n",
    "            min_size = len_sample_chunks or input_size + output_size\n",
    "        else:\n",
    "            min_size = input_size + output_size\n",
    "    else:\n",
    "        min_size = output_size\n",
    "    e_sampleable_ts_idxs = np.where(sizes > min_size + ds_in_test)[0]\n",
    "    \n",
    "    assert np.array_equal(loader.sampleable_ts_idxs, e_sampleable_ts_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampleable_ts(dataset=dataset, \n",
    "                   model='esrnn', input_size=5, output_size=5,\n",
    "                   len_sample_chunks=None, complete_inputs=False, \n",
    "                   ds_in_test=ds_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampleable_ts(dataset=dataset, \n",
    "                   model='esrnn', input_size=5, output_size=5,\n",
    "                   len_sample_chunks=len_sample_chunks, complete_inputs=True, \n",
    "                   ds_in_test=ds_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampleable_ts(dataset=dataset, \n",
    "                   model='nbeats', input_size=5, output_size=5,\n",
    "                   len_sample_chunks=None, complete_inputs=True, \n",
    "                   ds_in_test=ds_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampleable_ts(dataset=dataset, \n",
    "                   model='nbeats', input_size=5, output_size=5,\n",
    "                   len_sample_chunks=None, complete_inputs=False, \n",
    "                   ds_in_test=ds_in_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following test checks failure for not sampleable time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TimeSeriesLoader(ts_dataset=dataset,\n",
    "                          model='esrnn',\n",
    "                          window_sampling_limit=dataset.max_len,\n",
    "                          input_size=5,\n",
    "                          output_size=5,\n",
    "                          idx_to_sample_freq=1,\n",
    "                          batch_size=8,\n",
    "                          complete_inputs=False,\n",
    "                          len_sample_chunks=15,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fail_not_sampleable_ts(): return loader[[1]]\n",
    "test_fail(_fail_not_sampleable_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches size\n",
    "\n",
    "Tests for `__iter__` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following test checks the correct size of batches. Note that the data loader only returns batches of ` batch_size` when suffle is True.\n",
    "For `nbeats` the windows size should be `input_size + output_size`. Meanwhile for `esrnn` the windows size should be `len_sample_chunks`. When `len_sample_chunks=None` then `len_sample_chunks=input_size + output_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtla.data.utils import create_synthetic_tsdata\n",
    "\n",
    "n_ts = 64\n",
    "Y_df, X_df, S_df = create_synthetic_tsdata(n_ts=n_ts, sort=True)\n",
    "ds_in_test = 10\n",
    "is_test = False\n",
    "batch_size = 8\n",
    "len_sample_chunks = 20 #only for ESRNN\n",
    "\n",
    "dataset = TimeSeriesDataset(Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                            ds_in_test=ds_in_test, \n",
    "                            is_test=is_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch_size(dataset, model, input_size, output_size, len_sample_chunks,\n",
    "                    expected_window_size):\n",
    "    \"\"\"This test checks the correct size of batches.\"\"\"\n",
    "    loader = TimeSeriesLoader(ts_dataset=dataset,\n",
    "                              model=model,\n",
    "                              window_sampling_limit=dataset.max_len,\n",
    "                              input_size=input_size,\n",
    "                              output_size=output_size,\n",
    "                              idx_to_sample_freq=1,\n",
    "                              batch_size=batch_size,\n",
    "                              complete_inputs=False,\n",
    "                              len_sample_chunks=len_sample_chunks,\n",
    "                              shuffle=True)\n",
    "    for batch in loader:\n",
    "        S = batch['S']\n",
    "        Y = batch['Y']\n",
    "        X = batch['X']\n",
    "        available_mask = batch['available_mask']\n",
    "        sample_mask = batch['sample_mask']\n",
    "        idxs = batch['idxs']\n",
    "\n",
    "        test_eq(S.shape, (batch_size, dataset.n_s))\n",
    "        test_eq(Y.shape, (batch_size, expected_window_size))\n",
    "        test_eq(X.shape, (batch_size, dataset.n_x, expected_window_size))\n",
    "        test_eq(available_mask.shape, (batch_size, expected_window_size))\n",
    "        test_eq(sample_mask.shape, (batch_size, expected_window_size))\n",
    "        test_eq(idxs.shape, (batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch_iteration(dataset, model, input_size, output_size, \n",
    "                         len_sample_chunks, complete_inputs,\n",
    "                         shuffle):\n",
    "    \"\"\"This test checks proper iteration when batch_size=1.\"\"\"\n",
    "    loader = TimeSeriesLoader(ts_dataset=dataset,\n",
    "                              model=model,\n",
    "                              window_sampling_limit=dataset.max_len,\n",
    "                              input_size=input_size,\n",
    "                              output_size=output_size,\n",
    "                              idx_to_sample_freq=1,\n",
    "                              batch_size=1,\n",
    "                              complete_inputs=complete_inputs,\n",
    "                              len_sample_chunks=len_sample_chunks,\n",
    "                              shuffle=shuffle)\n",
    "    \n",
    "    batches = [batch for batch in loader]\n",
    "    assert len(batches) == loader.n_sampleable_ts, (\n",
    "        'Expected and actual batches are different '\n",
    "        'for batch_size=1'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_sample_chunks = 20\n",
    "test_batch_size(dataset, 'esrnn', input_size=5, output_size=5, \n",
    "                len_sample_chunks=len_sample_chunks, \n",
    "                expected_window_size=len_sample_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_iteration(dataset, 'esrnn', input_size=5, output_size=5,\n",
    "                     len_sample_chunks=len_sample_chunks,\n",
    "                     complete_inputs=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_iteration(dataset, 'esrnn', input_size=5, output_size=5,\n",
    "                     len_sample_chunks=None,\n",
    "                     complete_inputs=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_iteration(dataset, 'esrnn', input_size=5, output_size=5,\n",
    "                     len_sample_chunks=None,\n",
    "                     complete_inputs=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch size will be ignored (shuffle=False). All constructed windows will be used to train.\n"
     ]
    }
   ],
   "source": [
    "test_batch_iteration(dataset, 'esrnn', input_size=5, output_size=5,\n",
    "                     len_sample_chunks=None,\n",
    "                     complete_inputs=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "output_size = 5\n",
    "test_batch_size(dataset, 'nbeats', input_size=input_size, output_size=output_size, \n",
    "                len_sample_chunks=20, expected_window_size=input_size + output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_iteration(dataset, 'nbeats', input_size=5, output_size=5,\n",
    "                     len_sample_chunks=None,\n",
    "                     complete_inputs=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_iteration(dataset, 'nbeats', input_size=5, output_size=5,\n",
    "                     len_sample_chunks=None,\n",
    "                     complete_inputs=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch size will be ignored (shuffle=False). All constructed windows will be used to train.\n"
     ]
    }
   ],
   "source": [
    "test_batch_iteration(dataset, 'nbeats', input_size=5, output_size=5,\n",
    "                     len_sample_chunks=None,\n",
    "                     complete_inputs=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper batch construction\n",
    "\n",
    "Tests for `__getitem__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtla.data.utils import create_synthetic_tsdata\n",
    "\n",
    "n_ts = 64\n",
    "Y_df, X_df, S_df = create_synthetic_tsdata(n_ts=n_ts, sort=True)\n",
    "ds_in_test = 5\n",
    "is_test = False\n",
    "\n",
    "dataset = TimeSeriesDataset(Y_df=Y_df, X_df=X_df, S_df=S_df,\n",
    "                            ds_in_test=ds_in_test, \n",
    "                            is_test=is_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "# This test only works for the dataset constructed\n",
    "# before and for the 21 time series\n",
    "def test_batch_construction(dataset, model, input_size, output_size, \n",
    "                            len_sample_chunks, window_step, \n",
    "                            complete_inputs, ds_in_test):\n",
    "    \"\"\"Final test to verify that the batch (of windows) is well constructed.\"\"\"\n",
    "    \n",
    "    wsl = dataset.max_len\n",
    "    \n",
    "    if model == 'nbeats':\n",
    "        windows_size = input_size + output_size\n",
    "    elif model == 'esrnn':\n",
    "        windows_size = len_sample_chunks or input_size + output_size\n",
    "\n",
    "    loader = TimeSeriesLoader(ts_dataset=dataset,\n",
    "                              model=model,\n",
    "                              window_sampling_limit=wsl,\n",
    "                              input_size=input_size,\n",
    "                              output_size=output_size,\n",
    "                              idx_to_sample_freq=window_step,\n",
    "                              batch_size=1,\n",
    "                              complete_inputs=complete_inputs,\n",
    "                              len_sample_chunks=len_sample_chunks,\n",
    "                              shuffle=False)\n",
    "    windows = loader[[20]]['Y']\n",
    "    \n",
    "    #Expected windows\n",
    "    uid = Y_df['unique_id'].unique()[20]\n",
    "    Y_original = Y_df.query('unique_id == @uid')['y'].values\n",
    "    size = Y_original.size\n",
    "    Y_sample = np.zeros(wsl)\n",
    "    Y_sample[-size:] = Y_original\n",
    "    if model == 'nbeats':\n",
    "        Y_sample = np.pad(Y_sample, (input_size, output_size))\n",
    "        \n",
    "    e_windows = sliding_window_view(Y_sample, window_shape=windows_size)\n",
    "    if model == 'nbeats':\n",
    "        e_windows = e_windows[0:-(ds_in_test + output_size):window_step]\n",
    "    elif model == 'esrnn':\n",
    "        e_windows = e_windows[0:-ds_in_test:window_step]\n",
    "    # This test works assuming there are no series with values of zero.\n",
    "    if complete_inputs:\n",
    "        sampleable_windows_idxs = np.where((e_windows > 0).sum(1) == windows_size)[0]\n",
    "    else:\n",
    "        sampleable_windows_idxs = np.where((e_windows > 0).sum(1) >= ds_in_test)[0]\n",
    "        \n",
    "    e_windows = e_windows[sampleable_windows_idxs]\n",
    "    \n",
    "    #Comparison\n",
    "    assert np.array_equal(windows, e_windows), (\n",
    "        'Expected and actual windows are different'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch size will be ignored (shuffle=False). All constructed windows will be used to train.\n"
     ]
    }
   ],
   "source": [
    "test_batch_construction(dataset=dataset, \n",
    "                        model='nbeats', input_size=5, output_size=5, \n",
    "                        len_sample_chunks=None, window_step=2,\n",
    "                        complete_inputs=False,\n",
    "                        ds_in_test=ds_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch size will be ignored (shuffle=False). All constructed windows will be used to train.\n"
     ]
    }
   ],
   "source": [
    "test_batch_construction(dataset=dataset, \n",
    "                        model='esrnn', input_size=5, output_size=5, \n",
    "                        len_sample_chunks=15, window_step=2,\n",
    "                        complete_inputs=False, ds_in_test=ds_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch size will be ignored (shuffle=False). All constructed windows will be used to train.\n"
     ]
    }
   ],
   "source": [
    "test_batch_construction(dataset=dataset, \n",
    "                        model='esrnn', input_size=5, output_size=5, \n",
    "                        len_sample_chunks=15, window_step=2,\n",
    "                        complete_inputs=True, ds_in_test=ds_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch size will be ignored (shuffle=False). All constructed windows will be used to train.\n"
     ]
    }
   ],
   "source": [
    "test_batch_construction(dataset=dataset, \n",
    "                        model='nbeats', input_size=5, output_size=5, \n",
    "                        len_sample_chunks=None, window_step=3,\n",
    "                        complete_inputs=True,\n",
    "                        ds_in_test=ds_in_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
