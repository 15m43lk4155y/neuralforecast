{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1a69c-8605-47b1-842b-005b8f0e89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.datasets.m5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3dbb4c-07aa-4421-a8f1-3cc36f4c9777",
   "metadata": {},
   "source": [
    "# M5 dataset\n",
    "\n",
    "> Download and evaluate the M5 dataset.\n",
    "\n",
    "The 2020 M5 competition was organized by the Makridakis Open Forecasting Center was organized by University of Nicosia and hosted in Kaggle. The main task consisted in providing hierarchical point and probabilistic predictions for 28 days ahead for the sales of Walmart stores in three US States (California, Texas, and Wisconsin). The dataset is organized at the item, department, product and store level details. Additionally it contains temporal covariates like price, promotions, special event and calendar variables.\n",
    "\n",
    "\n",
    "[University of Nicosia. The M5 competition: Estimate the unit sales of Walmart retail goods. Kaggle Competition, 2020.](https://www.kaggle.com/c/m5-forecasting-accuracy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c83ec4-30a2-4229-9d54-b836f762f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nixtlats.data.datasets.utils import download_file, Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0cfa5-fe76-43fb-b835-cefc69ececd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class M5:\n",
    "    \n",
    "    # original data available from Kaggle directly\n",
    "    # pip install kaggle --upgrade\n",
    "    # kaggle competitions download -c m5-forecasting-accuracy\n",
    "    source_url = 'https://www.dropbox.com/s/u5e0fo5yn76eri8/m5-forecasting-accuracy.zip?dl=1'\n",
    "    \n",
    "    @staticmethod\n",
    "    def download(directory: str) -> None:\n",
    "        \"\"\"Downloads M5 Competition Dataset.\"\"\"\n",
    "        path = f'{directory}/m5/datasets'\n",
    "        if not os.path.exists(path):\n",
    "            download_file(directory=path,\n",
    "                          source_url=M5.source_url,\n",
    "                          decompress=True)\n",
    "            \n",
    "    @staticmethod\n",
    "    def load(directory: str, cache: bool = True) -> Tuple[pd.DataFrame, \n",
    "                                                          pd.DataFrame, \n",
    "                                                          pd.DataFrame]:\n",
    "        \"\"\"Downloads and loads M5 data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory: str\n",
    "            Directory where data will be downloaded.\n",
    "        cache: bool\n",
    "            If `True` saves and loads \n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        [1] Returns train+test sets.\n",
    "        [2] Based on https://www.kaggle.com/lemuz90/m5-preprocess.\n",
    "        \"\"\"\n",
    "        path = f'{directory}/m5/datasets'\n",
    "        file_cache = f'{path}/m5.p'\n",
    "        \n",
    "        if os.path.exists(file_cache) and cache:\n",
    "            Y_df, X_df, S_df = pd.read_pickle(file_cache)\n",
    "            \n",
    "            return Y_df, X_df, S_df\n",
    "        \n",
    "        M5.download(directory)\n",
    "        # Calendar data\n",
    "        cal_dtypes = {\n",
    "            'd': 'category',\n",
    "            'wm_yr_wk': np.uint16,\n",
    "            'event_name_1': 'category',\n",
    "            'event_type_1': 'category',\n",
    "            'event_name_2': 'category',\n",
    "            'event_type_2': 'category',\n",
    "            'snap_CA': np.uint8,\n",
    "            'snap_TX': np.uint8,\n",
    "            'snap_WI': np.uint8,\n",
    "        }\n",
    "        cal = pd.read_csv(f'{path}/calendar.csv', \n",
    "                          dtype=cal_dtypes, \n",
    "                          usecols=list(cal_dtypes.keys()) + ['date'], \n",
    "                          parse_dates=['date'])\n",
    "        \n",
    "        event_cols = [k for k in cal_dtypes if k.startswith('event')]\n",
    "        for col in event_cols:\n",
    "            cal[col] = cal[col].cat.add_categories('nan').fillna('nan')\n",
    "        \n",
    "        # Prices\n",
    "        prices_dtypes = {\n",
    "            'store_id': 'category',\n",
    "            'item_id': 'category',\n",
    "            'wm_yr_wk': np.uint16,\n",
    "            'sell_price': np.float32\n",
    "        }\n",
    "\n",
    "        prices = pd.read_csv(f'{path}/sell_prices.csv', \n",
    "                             dtype=prices_dtypes)\n",
    "        \n",
    "        # Sales\n",
    "        sales_dtypes = {\n",
    "            'id': 'category',\n",
    "            'item_id': prices.item_id.dtype,\n",
    "            'dept_id': 'category',\n",
    "            'cat_id': 'category',\n",
    "            'store_id': 'category',\n",
    "            'state_id': 'category',\n",
    "            **{f'd_{i+1}': np.float32 for i in range(1913)}\n",
    "        }\n",
    "        sales = pd.read_csv(f'{path}/sales_train_validation.csv', \n",
    "                            dtype=sales_dtypes)\n",
    "        long = sales.melt(id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "                          var_name='d', value_name='y')\n",
    "        \n",
    "        long['d'] = long['d'].astype(cal.d.dtype)\n",
    "        long = long.merge(cal, on=['d'])\n",
    "        long = long.merge(prices, on=['store_id', 'item_id', 'wm_yr_wk'])\n",
    "        long = long.drop(columns=['d', 'wm_yr_wk'])\n",
    "        \n",
    "        @njit\n",
    "        def first_nz_mask(x):\n",
    "            \"\"\"Return a boolean mask where the True starts at the first non-zero value.\"\"\"\n",
    "            mask = np.full(x.size, True)\n",
    "            for idx, value in enumerate(x):\n",
    "                if value == 0:\n",
    "                    mask[idx] = False\n",
    "                else:\n",
    "                    break\n",
    "            return mask\n",
    "        \n",
    "        long = long.sort_values(['id', 'date'], ignore_index=True)\n",
    "        keep_mask = long.groupby('id')['y'].transform(lambda x: first_nz_mask(x.values))\n",
    "        long = long[keep_mask]\n",
    "        long.rename(columns={'id': 'unique_id', 'date': 'ds'}, inplace=True)\n",
    "        Y_df = long.filter(items=['unique_id', 'ds', 'y'])\n",
    "        cats = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "        S_df = long.filter(items=['unique_id'] + cats)\n",
    "        S_df = S_df.drop_duplicates()\n",
    "        X_df = long.drop(columns=['y'] + cats)\n",
    "        \n",
    "        if cache:\n",
    "            pd.to_pickle((Y_df, X_df, S_df), file_cache)\n",
    "        \n",
    "        return Y_df, X_df, S_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ef7c6-4878-48e8-bc88-a17b23a91d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df, X_df, S_df = M5.load('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f409f99-0306-4a71-bbb5-c4ce19a273e2",
   "metadata": {},
   "source": [
    "## Test number of series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920d220-646d-410a-be2c-010b0e7ec089",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_series = 30_490\n",
    "assert Y_df['unique_id'].unique().size == n_series\n",
    "assert X_df['unique_id'].unique().size == n_series\n",
    "assert S_df.shape[0] == 30_490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a576b88-13e5-485d-8696-a331e0b6909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e3e6a0-3aef-46ff-aafe-ae016dd030ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7326a877-309b-4cf7-8b8c-1d07fbd2ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ada69-0da2-41a7-92d6-e27bcaac6752",
   "metadata": {},
   "source": [
    "# Kaggle-Competition-M5 References\n",
    "\n",
    "The evaluation metric of the Favorita Kaggle competition was the normalized weighted root mean squared logarithmic error (NWRMSLE).\n",
    "Perishable items have a score weight of 1.25; otherwise, the weight is 1.0.\n",
    "\n",
    "$$ NWRMSLE = \\sqrt{\\frac{\\sum^{n}_{i=1} w_{i}\\left(log(\\hat{y}_{i}+1)  - log(y_{i}+1)\\right)^{2}}{\\sum^{n}_{i=1} w_{i}}}$$\n",
    "\n",
    "Kaggle Competition Forecasting Methods                                                              | 16D ahead NWRMSLE\n",
    ":-------------------------------------------------------------------------------------------------: | :-------: \n",
    "[LGBM](https://www.kaggle.com/shixw125/1st-place-lgb-model-public-0-506-private-0-511/comments) [1] | 0.5091   | \n",
    "[Seq2Seq WaveNet](https://arxiv.org/abs/1803.04037) [2]                                             | 0.5129   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813fbb27-c5f2-44e6-a3a0-8cef208d4c73",
   "metadata": {},
   "source": [
    "1.\t[Corporación Favorita. Corporación favorita grocery sales forecasting. Kaggle Competition Leaderboard, 2018.](https://www.kaggle.com/c/favorita-grocery-sales-forecasting/leaderboard)\n",
    "2.\t[Glib Kechyn, Lucius Yu, Yangguang Zang, and Svyatoslav Kechyn.  Sales forecasting using wavenet within the framework of the Favorita Kaggle competition. Computing Research Repository, abs/1803.04037, 2018](https://arxiv.org/abs/1803.04037)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
