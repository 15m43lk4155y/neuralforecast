{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.datasets.tourismL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TourismLarge dataset.\n",
    "\n",
    "## Dataset Description\n",
    "This detailed Australian Tourism Dataset comes from the National Visitor Survey, managed by the Tourism Research Australia, it is composed of 555 monthly series from 1998 to 2016, it is organized geographically, and purpose of travel. The natural geographical hierarchy comprises seven states, divided further in 27 zones and 76 regions. The purpose of travel categories are holiday, visiting friends and relatives (VFR), business and other. The MinT (Wickramasuriya et al., 2019), among other hierarchical forecasting studies has used the dataset it in the past. The dataset can be accessed in the [MinT reconciliation webpage](https://robjhyndman.com/publications/mint/), official sources are also available.\n",
    "\n",
    "Table of Contents\n",
    "1.   [Auxiliary Functions](#cell-1)\n",
    "2.   [TourismL Class](#cell-2)\n",
    "3.   [TourismL Validation](#cell-3)\n",
    "4.   [HierarchicalDataset Class](#cell-4)\n",
    "5.   [HierarchicalDataset Validation](#cell-5)\n",
    "\n",
    "\n",
    "Summary of TourismL's hierarchical structure\n",
    "\n",
    "| Geographical Division | Number of series per division | Number of series per purpose | Total |\n",
    "|          ---          |               ---             |              ---             |  ---  |\n",
    "|  Australia            |              1                |               4              |   5   |\n",
    "|  States               |              7                |              28              |  35   |\n",
    "|  Zones                |             27                |              108             |  135  |\n",
    "|  Regions              |             76                |              304             |  380  |\n",
    "|  Total                |            111                |              444             |  555  |\n",
    "\n",
    "\n",
    "Evaluation levels:\n",
    "\n",
    "| Level                 | Number of Series       |\n",
    "|---                    |---                     |\n",
    "|  1. Overall           |  555                   |\n",
    "|  2. Country           |  1                     |\n",
    "|  3. States            |  7                     |\n",
    "|  4. Zones             |  27                    |\n",
    "|  5. Regions           |  76                    |\n",
    "|  6. Country (purpose) |  4                     |\n",
    "|  7. States (purpose)  |  28                    |\n",
    "|  8. Zones (purpose)   |  108                   |\n",
    "|  9. Regions (purpose) |  304                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import timeit\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from neuralforecast.data.datasets.utils import (\n",
    "    download_file, Info, TimeSeriesDataclass\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pylab as plt\n",
    "from pylab import rcParams\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "from matplotlib import rcParams\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "FONTSIZE = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-1\"></a>\n",
    "## 1. Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CodeTimer:\n",
    "    def __init__(self, name=None, verbose=True):\n",
    "        self.name = \" '\"  + name + \"'\" if name else ''\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = timeit.default_timer()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.took = (timeit.default_timer() - self.start)\n",
    "        if self.verbose:\n",
    "            print('Code block' + self.name + \\\n",
    "                  ' took:\\t{0:.5f}'.format(self.took) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def one_hot_encoding(df, index_col):    \n",
    "    encoder = OneHotEncoder()\n",
    "    columns = list(df.columns)\n",
    "    columns.remove(index_col)\n",
    "    one_hot_concat_df = pd.DataFrame(df[index_col].values, columns=[index_col])\n",
    "    for col in columns:\n",
    "        dummy_columns = [f'{col}_[{x}]' for x in list(df[col].unique())]\n",
    "        dummy_values  = encoder.fit_transform(df[col].values.reshape(-1,1)).toarray()\n",
    "        one_hot_df    = pd.DataFrame(dummy_values, columns=dummy_columns)        \n",
    "        one_hot_concat_df = pd.concat([one_hot_concat_df, one_hot_df], axis=1)\n",
    "    return one_hot_concat_df\n",
    "\n",
    "def nonzero_indexes_by_row(M):\n",
    "    return [np.nonzero(M[row,:])[0] for row in range(len(M))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-2\"></a>\n",
    "## 2. TourismL Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class TourismL(TimeSeriesDataclass):\n",
    "    \n",
    "    url = 'https://robjhyndman.com/publications/mint/'\n",
    "    source_url = 'https://robjhyndman.com/data/TourismData_v3.csv'\n",
    "    H = 12\n",
    "\n",
    "    @staticmethod\n",
    "    def load(directory: str):\n",
    "        \"\"\"\n",
    "        Downloads and loads TourismL data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory: str\n",
    "            Directory where data will be downloaded.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        df: pd.DataFrame\n",
    "            Target time series with columns ['unique_id', 'ds', 'y'].\n",
    "        \"\"\"\n",
    "        \n",
    "        TourismL.download(directory)\n",
    "        \n",
    "        file = f'{directory}/TourismData_v3.csv'\n",
    "        if not os.path.exists(file):\n",
    "            raise ValueError(\n",
    "                \"TourismL zip file not found in {}!\".format(file) +\n",
    "                \" Please manually download from {}\".format(TourismL.url))\n",
    "        \n",
    "        # Fixing original dataset dates\n",
    "        df = pd.read_csv(file)\n",
    "        df['ds'] = np.arange(\"1998-01-01\",\"2017-01-01\",dtype='datetime64[M]')\n",
    "        df = df.drop(labels=['Year', 'Month'], axis=1)\n",
    "        \n",
    "        # Add aditional H steps for predictions\n",
    "        # declare skipping ds from column names\n",
    "        extra = np.empty((12,len(df.columns)-1,))\n",
    "        extra[:] = np.nan\n",
    "        extra_df = pd.DataFrame.from_records(extra, columns=df.columns[1:]) \n",
    "        extra_df['ds'] = np.arange(\"2017-01-01\",\"2018-01-01\",dtype='datetime64[M]')\n",
    "        \n",
    "        raw_df = pd.concat([df, extra_df], axis=0).reset_index(drop=True)\n",
    "        \n",
    "        return raw_df\n",
    "        \n",
    "    @staticmethod\n",
    "    def download(directory: str) -> None:\n",
    "        \"\"\"\n",
    "        Download M3 Dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        directory: str\n",
    "            Directory path to download dataset.\n",
    "        \"\"\"\n",
    "        path = f'{directory}'\n",
    "        if not os.path.exists(path):\n",
    "            download_file(path, TourismL.source_url)\n",
    "            \n",
    "    @staticmethod\n",
    "    def preprocess_data(directory: str, verbose=False) -> None:\n",
    "        # Read raw data and create ids for later use\n",
    "        raw_df = TourismL.load(directory)\n",
    "        dates = raw_df.ds        \n",
    "        raw_df.drop(['ds'], axis=1, inplace=True)\n",
    "        unique_ids  = np.array(list(raw_df.columns)) \n",
    "        region_ids  = np.array([u_id[:3] for u_id in unique_ids]) #AAAHol->AAA\n",
    "        purpose_ids = np.array([u_id[3:] for u_id in unique_ids]) #AAAHol->Hol\n",
    "        \n",
    "        with CodeTimer('Create static_agg', verbose):\n",
    "            # Create static features df, \n",
    "            # 1 country, 7 States, 27 Zones, 76 Regions\n",
    "            H_df = pd.DataFrame({'unique_id': unique_ids})\n",
    "            H_df['country'] = 'Australia'                 # 1\n",
    "            H_df['state']   = H_df['unique_id'].str[:1]   # 7\n",
    "            H_df['zone']    = H_df['unique_id'].str[:2]   # 27\n",
    "            H_df['region']  = H_df['unique_id'].str[:3]   # 76\n",
    "\n",
    "            # GeographyXpurpose fixed effects\n",
    "            # 4 countryXpurpose, 28 StatesXpurpose, \n",
    "            # 108 ZonesXpurpose, 304 RegionsXpurpose (unique_id)\n",
    "            H_df['purpose']       = H_df['unique_id'].str[-3:]      # 4\n",
    "            H_df['state_purpose'] = H_df['state'] + H_df['purpose'] # 28\n",
    "            H_df['zone_purpose']  = H_df['zone'] + H_df['purpose']  # 108\n",
    "\n",
    "            # Hierarchical aggregation matrix\n",
    "            Hencoded = one_hot_encoding(df=H_df, index_col='unique_id')\n",
    "            Hsum = Hencoded.values[:, 1:].T # Eliminate unique_id index\n",
    "            H = np.concatenate((Hsum, np.eye(len(unique_ids))), axis=0)\n",
    "\n",
    "            # Final wrangling + save\n",
    "            # Create aggregate(region) level dummy variables\n",
    "            H_df_agg  = H_df[['region', 'country', 'state', 'zone']]\n",
    "            H_df_agg  = H_df_agg.drop_duplicates()\n",
    "            static_agg = one_hot_encoding(df=H_df_agg, index_col='region')\n",
    "\n",
    "        with CodeTimer('Create temporal_agg', verbose):\n",
    "            #----------------------- Y Variables -----------------------#\n",
    "            \n",
    "            # Hierarchical Y for Y_agg features\n",
    "            Y_bottom = raw_df.values.T\n",
    "            Y_hier    = H @ Y_bottom      # [n_agg+n_bottom, n_bottom] x [n_bottom, T]\n",
    "\n",
    "            # [Total, State, Zone, Region, TotalP, StateP, ZoneP] [0, 1, 2, 3, 4, 5, 6]\n",
    "            # [Total, State, Zone, Region] <--> [0, 1, 2, 3]\n",
    "            # [TotalP, StateP, ZoneP, RegionP] <--> [4, 5, 6]\n",
    "            Y_agg_idxs = nonzero_indexes_by_row(Hsum.T)\n",
    "            Y_agg  = np.stack([Y_hier[idx, :].T for idx in Y_agg_idxs ])\n",
    "            Y_agg  = Y_agg[:,:,[0, 1, 2, 3]]\n",
    "\n",
    "            # Keep total geographic visits, erase redundant purpose\n",
    "            n_groups = len(H_df.region.unique())\n",
    "            n_purposes = len(H_df.purpose.unique())\n",
    "            n_time = len(dates)\n",
    "            n_agg = Y_agg.shape[-1]\n",
    "            Y_agg    = Y_agg.reshape((n_groups,n_purposes,n_time,n_agg))\n",
    "            Y_agg    = Y_agg[:,0,:,:] # Skip redundant purpose dimension\n",
    "\n",
    "            #----------------------- X variables -----------------------#\n",
    "            # Calendar variables December distance\n",
    "            calendar_df = pd.DataFrame()\n",
    "            calendar_df['ds'] = dates\n",
    "            calendar_df['distance_month'] = calendar_df.ds.dt.month\n",
    "            # December/January? spike match and normalization of month distance\n",
    "            calendar_df['distance_month'] = (calendar_df['distance_month'] - 3) % 12\n",
    "            calendar_df['distance_month'] = (calendar_df.distance_month) / 11.0 - 0.5\n",
    "            distance_month = calendar_df.distance_month.values\n",
    "            distance_month = np.tile(distance_month, (76,1))\n",
    "            X_agg = np.tile(calendar_df.values, (76,1,1))\n",
    "            \n",
    "            # Final wrangling + save\n",
    "            temporal_agg = np.concatenate([X_agg, Y_agg], axis=2)\n",
    "            temporal_agg = temporal_agg.reshape(-1, temporal_agg.shape[-1])\n",
    "            temporal_agg = pd.DataFrame.from_records(temporal_agg,\n",
    "                                     columns=['ds', 'distance_month',\n",
    "                                              'y_[total]', 'y_[state]', \n",
    "                                              'y_[zone]', 'y_[region]'])\n",
    "            temporal_agg['region'] = np.tile(static_agg.region.values[:,None], (240,1))\n",
    "            \n",
    "        with CodeTimer('Create static_bottom', verbose):\n",
    "            # This version intends to help the model learn in this low sample task\n",
    "            # Providing it with precomputed level/spread will model to \"level\" predictions\n",
    "            # Some ideas to try:\n",
    "            # 1) seasonal levels\n",
    "            # 2) robust vs non robust levels and spreads\n",
    "            # 3) like moving average, compute this metrics filtering the available data.\n",
    "            \n",
    "            # Create S_bottom matrix (previous static features)\n",
    "            # S_bottom   = H[:n_agg, :].T\n",
    "\n",
    "            # Avoid leakage into statistics and collapse time dimension, \n",
    "            # using median (it is robust to outliers)    \n",
    "            Y_available = Y_bottom[:,:-12-12] # skip test and validation\n",
    "\n",
    "            # [n_regions*n_purpose, n_time] --> [n_regions*n_purpose]\n",
    "            level  = np.median(Y_available, axis=1)\n",
    "            spread = np.median(np.abs(Y_available - level[:,None]), axis=1)\n",
    "\n",
    "            # Seasonal levels\n",
    "            Y_december = Y_available.reshape(304,18,12)[:,:,0]\n",
    "            Y_january  = Y_available.reshape(304,18,12)[:,:,-1]\n",
    "            december_level = np.median(Y_december/(level[:,None]+1), axis=1)\n",
    "            january_level  = np.median(Y_january/(level[:,None]+1), axis=1)\n",
    "\n",
    "            # Final wrangling + save\n",
    "            static_bottom = np.concatenate([level[:,None], spread[:,None],\n",
    "                                           december_level[:,None], \n",
    "                                           january_level[:,None]], axis=1)\n",
    "            static_bottom = pd.DataFrame.from_records(static_bottom, \n",
    "                                        columns=['level', 'spread', \n",
    "                                                 'level_[december]', \n",
    "                                                 'level_[january]'])\n",
    "            static_bottom['unique_id'] = unique_ids\n",
    "            static_bottom['region']    = region_ids\n",
    "            static_bottom['purpose']   = purpose_ids\n",
    "\n",
    "        with CodeTimer('Create temporal_bottom', verbose):\n",
    "            # To help the model learn seasonalities(12 months)\n",
    "            # we compute lag 12 for 304 regions x purposes\n",
    "            # Elements that roll beyond the last position are re-introduced at the first.\n",
    "            Y_lags = np.roll(Y_bottom, shift=12, axis=1)\n",
    "            Y_lags[:,:12] = Y_lags[:,12:24] # Clean raw_df NAs from first 12 entries\n",
    "\n",
    "            # December/January dummy variables\n",
    "            encoder = OneHotEncoder()\n",
    "            month_cols = [f'month_[{str(x+1)}]' for x in range(12)]\n",
    "            month_df = calendar_df['distance_month'].values.reshape(-1,1)\n",
    "            month_df = encoder.fit_transform(month_df).toarray()\n",
    "            month_df = pd.DataFrame(month_df, columns=month_cols)\n",
    "            month_df['ds'] = calendar_df['ds']\n",
    "            \n",
    "            # Available and sample masks\n",
    "            month_df['available_mask'] = 1\n",
    "            month_df['sample_mask']    = 1\n",
    "            \n",
    "            month_df = month_df[['ds', 'month_[11]', 'month_[12]',\n",
    "                                 'available_mask', 'sample_mask']]       # Filter, Dec/Jan\n",
    "            month_bottom = np.tile(month_df.values[None,:,:], (304,1,1)) # Dummies for each serie\n",
    "            \n",
    "            # Final wrangling + save\n",
    "            temporal_bottom = np.concatenate([month_bottom, Y_lags[:,:,None]], axis=2)\n",
    "            temporal_bottom = np.concatenate([temporal_bottom, Y_bottom[:,:,None]], axis=2)\n",
    "            temporal_bottom = temporal_bottom.reshape(-1, temporal_bottom.shape[-1])\n",
    "            temporal_bottom = pd.DataFrame.from_records(temporal_bottom, \n",
    "                                        columns=list(month_df.columns) + \\\n",
    "                                                ['y_[lag12]', 'y',])\n",
    "            temporal_bottom['unique_id'] = np.repeat(unique_ids, 240)\n",
    "            temporal_bottom['region']    = np.repeat(region_ids, 240)\n",
    "            temporal_bottom['purpose']   = np.repeat(purpose_ids,240)\n",
    "        \n",
    "        # Checking dtypes correctness\n",
    "        if verbose:\n",
    "            print('1. static_agg.dtypes \\n', static_agg.dtypes)\n",
    "            print('2. temporal_agg.dtypes \\n', temporal_agg.dtypes)\n",
    "            print('3. static_bottom.dtypes \\n', static_bottom.dtypes)\n",
    "            print('4. temporal_bottom.dtypes \\n', temporal_bottom.dtypes)\n",
    "        \n",
    "        # Save feathers for fast access\n",
    "        H_df.to_csv(f'{directory}/H_df.csv', index=False)\n",
    "        static_agg.to_csv(f'{directory}/static_agg.csv', index=False)\n",
    "        temporal_agg.to_csv(f'{directory}/temporal_agg.csv', index=False)\n",
    "        static_bottom.to_csv(f'{directory}/static_bottom.csv', index=False)\n",
    "        temporal_bottom.to_csv(f'{directory}/temporal_bottom.csv', index=False)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_process(directory=str, verbose=False) -> None:\n",
    "\n",
    "        with CodeTimer('Reading data   ', verbose):\n",
    "            static_agg    = pd.read_csv(f'{directory}/static_agg.csv')\n",
    "            temporal_agg    = pd.read_csv(f'{directory}/temporal_agg.csv')\n",
    "\n",
    "            static_bottom = pd.read_csv(f'{directory}/static_bottom.csv')\n",
    "            temporal_bottom = pd.read_csv(f'{directory}/temporal_bottom.csv')\n",
    "            \n",
    "            # Extract datasets dimensions for later use\n",
    "            dates = pd.to_datetime(temporal_agg.ds.unique())\n",
    "            unique_ids  = static_bottom.unique_id.values\n",
    "            region_ids  = static_bottom.region.values\n",
    "            purpose_ids = static_bottom.purpose.values\n",
    "            n_time   = len(dates)                             # 228\n",
    "            n_group  = len(static_bottom.region.unique())     # 76\n",
    "            n_series = len(static_bottom.purpose.unique())    # 4\n",
    "        \n",
    "        #-------------------------------------- S/X/Y_agg    --------------------------------------#\n",
    "        with CodeTimer('Process temporal_agg', verbose):\n",
    "            # Drop observation indexes and obtain column indexes\n",
    "            temporal_agg.drop(['region', 'ds'], axis=1, inplace=True)\n",
    "            xcols_agg       = temporal_agg.columns\n",
    "            xcols_hist_agg  = ['y_[total]', 'y_[state]', 'y_[zone]', 'y_[region]']\n",
    "            xcols_futr_agg  = ['distance_month']\n",
    "        \n",
    "            X_agg  = temporal_agg.values\n",
    "            Y_agg  = temporal_agg[['y_[total]', 'y_[state]', \n",
    "                                'y_[zone]', 'y_[region]']].values\n",
    "\n",
    "            X_agg  = X_agg.reshape((n_group,n_time,temporal_agg.shape[1]))\n",
    "            Y_agg  = Y_agg.reshape((n_group,n_time,4))\n",
    "            \n",
    "        with CodeTimer('Process static_agg', verbose):\n",
    "            # Drop observation indexes and obtain column indexes\n",
    "            S_agg = static_agg.drop(['region'], axis=1)\n",
    "            scols_agg = S_agg.columns\n",
    "            \n",
    "            S_agg = S_agg.values\n",
    "            \n",
    "        del temporal_agg, static_agg\n",
    "        gc.collect()\n",
    "        \n",
    "        #--------------------------------------- S/X/Y_bottom ---------------------------------------#\n",
    "        with CodeTimer('Process temporal_bottom', verbose):\n",
    "            # Drop observation indexes and obtain column indexes\n",
    "            temporal_bottom.drop(['unique_id', 'region', 'purpose', 'ds'], axis=1, inplace=True)\n",
    "            xcols      = temporal_bottom.columns\n",
    "            xcols_hist = ['y']\n",
    "            xcols_futr = ['month_[11]', 'month_[12]', 'y_[lag12]']\n",
    "            \n",
    "            X  = temporal_bottom.values\n",
    "            Y  = temporal_bottom['y'].values\n",
    "            \n",
    "            # temporal_bottom assumes to be balanced ie len(Y) = n_group*n_series*n_time\n",
    "            n_features = temporal_bottom.shape[1]\n",
    "            X  = X.reshape((n_group,n_series,n_time,n_features))\n",
    "            Y  = Y.reshape((n_group,n_series,n_time))\n",
    "            \n",
    "            #Reshape to match with the Mixture's inputs\n",
    "            #n_groups, n_series, n_time, n_features -> n_group, n_time, n_series, n_features\n",
    "            X = np.transpose(X, (0, 2, 1, 3))\n",
    "            Y = np.transpose(Y, (0, 2, 1))\n",
    "            \n",
    "        with CodeTimer('Process static_bottom', verbose):\n",
    "            # Drop observation indexes and obtain column indexes\n",
    "            static_bottom.drop(['unique_id', 'region', 'purpose'], axis=1, inplace=True)\n",
    "            scols = static_bottom.columns\n",
    "            \n",
    "            S = static_bottom.values\n",
    "            S = S.reshape(n_group,n_series,static_bottom.shape[1])\n",
    "        \n",
    "        with CodeTimer('Hier constraints', verbose):\n",
    "            # Read hierarchical constraints dataframe\n",
    "            # Create hierarchical aggregation numpy\n",
    "            H_df = pd.read_csv(f'{directory}/H_df.csv')\n",
    "            Hencoded = one_hot_encoding(df=H_df, index_col='unique_id')\n",
    "            Hsum = Hencoded.values[:, 1:].T # Eliminate stores index\n",
    "            H = np.concatenate((Hsum, np.eye(len(unique_ids))), axis=0)\n",
    "            \n",
    "            # Hierarchical dataset and evaluation utility\n",
    "            Y_flat  = temporal_bottom['y'].values\n",
    "            Y_flat  = Y_flat.reshape((n_group*n_series,n_time))\n",
    "            \n",
    "            Y_hier      = H @ Y_flat\n",
    "            hier_labels = list(Hencoded.columns[1:]) + list(H_df.unique_id)\n",
    "            hier_labels = np.array(hier_labels) # Numpy for easy list indexing\n",
    "            hier_idxs   = [range(555),\n",
    "                           range(0, 1), range(1, 1+7), range(8, 8+27), range(35, 111),\n",
    "                           range(111, 111+4), range(115, 115+28), \n",
    "                           range(143,143+108), range(251, 251+304)] # Hardcoded hier_idxs\n",
    "            hier_linked_idxs = nonzero_indexes_by_row(H.T)\n",
    "            \n",
    "        del temporal_bottom, static_bottom\n",
    "        gc.collect()\n",
    "            \n",
    "        #---------------------------------- Logs and Output ----------------------------------#\n",
    "        with CodeTimer('Final processing', verbose):            \n",
    "            # NaiveSeasonal compatibility X_futr indexing\n",
    "            # Consider T0=0, L=8, H=3 and Y=[0,1,2,3,4,5,6,7|,8,9,10,11]\n",
    "            # T0=0 --> [0,1,2,3,4,5,6,7|,8,9,10]\n",
    "            # T0=1 --> [1,2,3,4,5,6,7,8|9,10,11]\n",
    "            # T0=1 uses T0+1 X_futr=[2,3,4,5,6,7,8,9|10,11,12] ???\n",
    "            # Seasonal Naive12 switch to [-1] for Naive1\n",
    "            X = np.concatenate((X, X[:,[-12],:]), axis=1)\n",
    "            X_agg = np.concatenate((X_agg, X_agg[:,[-12],:]), axis=1)\n",
    "            \n",
    "            # Reshape NumPy arrays for Pytorch inputs\n",
    "            # [0,1,2,3][G,T,N,C] -> [0,2,3,1][G,N,C,T]\n",
    "            S_agg = np.float32(S_agg)\n",
    "            Y_agg = np.float32(np.transpose(Y_agg, (0, 2, 1)))\n",
    "            X_agg = np.float32(np.transpose(X_agg, (0, 2, 1)))\n",
    "            \n",
    "            S = np.float32(S)\n",
    "            Y = np.float32(np.transpose(Y, (0, 2, 1)))\n",
    "            X = np.float32(np.transpose(X, (0, 2, 3, 1)))\n",
    "            \n",
    "            # Skip NAs from Y data [G,N,T]\n",
    "            Y = Y[:,:,:-12]\n",
    "            Y_agg = Y_agg[:,:,:-12]\n",
    "            Y_hier = Y_hier[:,:-12]\n",
    "            \n",
    "            # Assert that the variables are contained in the column indexes\n",
    "            assert all(c in list(xcols_agg) for c in xcols_hist_agg)\n",
    "            assert all(c in list(xcols_agg) for c in xcols_futr_agg)\n",
    "            assert all(c in list(xcols) for c in xcols_hist)\n",
    "            assert all(c in list(xcols) for c in xcols_futr)\n",
    "            \n",
    "            data = {# Bottom data\n",
    "                    'S': S, 'X': X, 'Y': Y,\n",
    "                    'scols': scols,\n",
    "                    'xcols': xcols,\n",
    "                    'xcols_hist': xcols_hist,\n",
    "                    'xcols_futr': xcols_futr,\n",
    "                    'xcols_sample_mask': 'sample_mask',\n",
    "                    'xcols_available_mask': 'available_mask',\n",
    "                    # Aggregate data\n",
    "                    'S_agg': S_agg, 'X_agg': X_agg, 'Y_agg': Y_agg,\n",
    "                    'scols_agg': scols_agg,\n",
    "                    'xcols_agg': xcols_agg,\n",
    "                    'xcols_hist_agg': xcols_hist_agg,\n",
    "                    'xcols_futr_agg': xcols_futr_agg,\n",
    "                    # Hierarchical data for evaluation\n",
    "                    'Y_hier': Y_hier,\n",
    "                    'hier_idxs': hier_idxs,\n",
    "                    'hier_labels': hier_labels,\n",
    "                    'hier_linked_idxs': hier_linked_idxs,\n",
    "                    # Shared data\n",
    "                    'H': H,\n",
    "                    'dates': dates,\n",
    "                    'unique_ids': unique_ids,\n",
    "                    'region_ids': region_ids,\n",
    "                    'pupose_ids': purpose_ids}\n",
    "            \n",
    "            if verbose:\n",
    "                print('\\n')\n",
    "                print(f'Total days {len(dates)}, train {n_time-12-12} validation {12}, test {12}')\n",
    "                print(f'Whole dates: \\t\\t [{min(dates)}, {max(dates)}]')\n",
    "                print(f'Validation dates: \\t '+\\\n",
    "                      f'[{min(dates[n_time-12-12:n_time-12])},{max(dates[n_time-12-12:n_time-12])}]')\n",
    "                print(f'Test dates: \\t\\t [{min(dates[n_time-12:])}, {max(dates[n_time-12:])}]')\n",
    "                print('\\n')\n",
    "                print(' '*35 +'BOTTOM')\n",
    "                print('S.shape (n_regions,n_purpose,n_features):        \\t' + str(data['S'].shape))\n",
    "                print('X.shape (n_regions,n_purpose,n_features,n_time): \\t' + str(data['X'].shape))\n",
    "                print('Y.shape (n_regions,n_purpose,n_time):            \\t' + str(data['Y'].shape))\n",
    "                print(f\"scols ({len(scols)}) {data['scols']}\")\n",
    "                print(f\"xcols ({len(xcols)}) {data['xcols']}\")\n",
    "                print(f\"xcols_hist ({len(xcols_hist)}) {data['xcols_hist']}\")\n",
    "                print(f\"xcols_futr ({len(xcols_futr)}) {data['xcols_futr']}\")\n",
    "                print(' '*35 +'AGGREGATE')\n",
    "                print('S_agg.shape (n_regions,n_features):           \\t\\t\\t' + str(data['S_agg'].shape))\n",
    "                print('X_agg.shape (n_regions,n_features,n_time):    \\t\\t\\t' + str(data['X_agg'].shape))\n",
    "                print('Y_agg.shape (n_regions,n_features,n_time):    \\t\\t\\t' + str(data['Y_agg'].shape))\n",
    "                print(f\"scols_agg ({len(scols_agg)}) {data['scols_agg'][:4]}\")\n",
    "                print(f\"xcols_agg ({len(xcols_agg)}) {data['xcols_agg']}\")\n",
    "                print(f\"xcols_hist_agg ({len(xcols_hist_agg)}) {data['xcols_hist_agg']}\")\n",
    "                print(f\"xcols_futr_agg ({len(xcols_futr_agg)}) {data['xcols_futr_agg']}\")\n",
    "                print(' '*35 +'HIERARCHICAL')\n",
    "                print('Y_hier.shape (n_regions,n_time):   \\t\\t\\t\\t' + str(data['Y_hier'].shape))\n",
    "                #print('hier_idxs', hier_idxs)\n",
    "                print('overall.shape \\t\\t',  data['Y_hier'][hier_idxs[0],:].shape)\n",
    "                print('country.shape \\t\\t',  data['Y_hier'][hier_idxs[1],:].shape)\n",
    "                print('states.shape  \\t\\t',  data['Y_hier'][hier_idxs[2],:].shape)\n",
    "                print('zones.shape   \\t\\t',  data['Y_hier'][hier_idxs[3],:].shape)\n",
    "                print('regions.shape \\t\\t',  data['Y_hier'][hier_idxs[4],:].shape)\n",
    "                print('country_purpose.shape \\t',  data['Y_hier'][hier_idxs[5],:].shape)\n",
    "                print('states_purpose.shape  \\t',  data['Y_hier'][hier_idxs[6],:].shape)\n",
    "                print('zones_purpose.shape   \\t',  data['Y_hier'][hier_idxs[7],:].shape)\n",
    "                print('regions_purpose.shape \\t',  data['Y_hier'][hier_idxs[8],:].shape)\n",
    "            \n",
    "        return data\n",
    "    \n",
    "# directory = './data/hierarchical/TourismL'\n",
    "# TourismL.preprocess_data(directory=directory)\n",
    "\n",
    "# data = TourismL.load_process(directory=directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-3\"></a>\n",
    "## 3. TourismL Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load and Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './data/hierarchical/TourismL'\n",
    "\n",
    "TourismL.preprocess_data(directory=directory, verbose=False)\n",
    "data = TourismL.load_process(directory=directory, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Validate data with plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Hierarchical aggregation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=1, figsize=(6, 15), dpi=80, facecolor='w')\n",
    "plt.spy(data['H'])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking pre-estimated levels seasons S_bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot S_bottom\n",
    "dates = data['dates']\n",
    "scols_bottom = data['scols']\n",
    "\n",
    "# n_groups,n_series,n_features,n_time\n",
    "Y_bottom = data['Y'].reshape(-1, 228)\n",
    "X_bottom = data['X']\n",
    "X_bottom = X_bottom.reshape(-1, X_bottom.shape[2], 241)\n",
    "S_bottom = data['S'].reshape(-1, data['S'].shape[1])\n",
    "\n",
    "print(\"data['scols']:=\", data['scols'])\n",
    "print(\"data['xcols']:=\", data['xcols'])\n",
    "\n",
    "b_idx = 0\n",
    "plt.figure(num=1, figsize=(10, 6), dpi=80, facecolor='w')\n",
    "plt.plot(dates[:228],Y_bottom[b_idx,:], label='signal')\n",
    "plt.axhline(y=S_bottom[b_idx,0], \n",
    "            label='level', color='black')\n",
    "plt.axhline(y=S_bottom[b_idx,0]*S_bottom[b_idx,2], \n",
    "            label='level_[december]', color='green')\n",
    "plt.axhline(y=S_bottom[b_idx,0]*S_bottom[b_idx,3],\n",
    "            label='level_[january]', color='red')\n",
    "plt.title('{} Visits'.format(data[\"unique_ids\"][b_idx]))\n",
    "plt.ylabel('Tourism Visits')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(bbox_to_anchor=(1, 1), \n",
    "           loc='upper left', ncol=1)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(num=1, figsize=(10, 6), dpi=80, facecolor='w')\n",
    "plt.plot(dates,X_bottom[b_idx,-1,:-1], label='signal') # 'y_[lag12]'\n",
    "plt.plot(dates,X_bottom[b_idx,4,:-1], label='y_[lag12]') # 'y_[lag12]'\n",
    "plt.plot(dates,X_bottom[b_idx,0,:-1]*2000, label='month_[11]') # 'month_[11]'\n",
    "plt.title('{} Visits'.format(data[\"unique_ids\"][b_idx]))\n",
    "plt.ylabel('Tourism Visits')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(bbox_to_anchor=(1, 1), \n",
    "           loc='upper left', ncol=1)\n",
    "plt.show()\n",
    "plt.close()\n",
    "# assert 1<0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data['scols_agg']:=\", data['scols_agg'][:4])\n",
    "print(\"data['xcols_agg']:=\", data['xcols_agg'])\n",
    "\n",
    "X_agg = data['X_agg']\n",
    "\n",
    "a_idx = 0\n",
    "plt.figure(num=1, figsize=(10, 6), dpi=80, facecolor='w')\n",
    "plt.plot(dates,X_agg[a_idx,1,:-1], label='y_[total]')\n",
    "plt.plot(dates,X_agg[a_idx,2,:-1], label='y_[state]')\n",
    "plt.plot(dates,X_agg[a_idx,3,:-1], label='y_[zone]')\n",
    "plt.plot(dates,X_agg[a_idx,4,:-1], label='y_[region]')\n",
    "plt.plot(dates,X_agg[a_idx,0,:-1]*20_000, label='distance_month')\n",
    "plt.legend()\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=1, figsize=(6, 10), dpi=80, facecolor='w')\n",
    "plt.spy(data['S_agg'].T)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Hierarchically linked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hierarchically_linked_data(data, b_idx, n_years=18, plot_file=None):\n",
    "    \n",
    "    # This plots validate the S/X/Y_bottom\n",
    "    # We check for the sinchronization of features with Y_bottom\n",
    "    # We do the same for Y_agg, to check correct linkage\n",
    "    assert b_idx < 304 and b_idx>=0\n",
    "    \n",
    "    # Hierarchically linked target data and labels\n",
    "    # [Total, State, Zone, Region, TotalP, StateP, ZoneP] [0, 1, 2, 3, 4, 5, 6]\n",
    "    # [Total, State, Zone, Region] <--> [0, 1, 2, 3]\n",
    "    linked_idxs = data['hier_linked_idxs'][b_idx][:4]\n",
    "    Y_hier = data['Y_hier']\n",
    "    hier_labels = data['hier_labels']\n",
    "    \n",
    "    # Base series and its label  n_agg=251 idx \\in {0,...,304}\n",
    "    Y_base  = data['Y_hier'][b_idx + 251]\n",
    "    Y_base2 = data['Y'][b_idx//4,b_idx % 4,:]\n",
    "    label   = data['hier_labels'][b_idx + 251]\n",
    "    \n",
    "    x_plot = data['dates'][:-12] # Skip future dates\n",
    "        \n",
    "    fig, axs = plt.subplots(len(linked_idxs)+1, 1, figsize=(12, 9))\n",
    "    for idx, linked_idx in enumerate(linked_idxs):\n",
    "        axs[idx].plot(x_plot[-12*n_years:], Y_hier[linked_idx,-12*n_years:]/1000,\n",
    "                      color = 'black', linewidth=1.5)\n",
    "        axs[idx].set_ylabel(f'{hier_labels[linked_idx]} \\nTourist Visits\\n [In millions]', \n",
    "                            fontsize=13)\n",
    "        axs[idx].set_xticks([])\n",
    "        axs[idx].tick_params(labelsize=13)\n",
    "    \n",
    "    # Checking that Y_bottom and Y_hier match\n",
    "    axs[4].plot(x_plot[-12*n_years:], Y_base[-12*n_years:]/1000,\n",
    "                color = 'blue', linewidth=1.5, label='base')\n",
    "    axs[4].plot(x_plot[-12*n_years:], Y_base2[-12*n_years:]/1000,\n",
    "                color = 'black', linewidth=1.5, label='base')\n",
    "    \n",
    "    axs[4].set_xlabel('Month', fontsize=13)\n",
    "    axs[4].set_ylabel(f'{label} \\nTourist Visits\\n [In millions]', fontsize=13)\n",
    "    axs[4].tick_params(labelsize=13)\n",
    "    \n",
    "    plt.suptitle(f'Hierarchically Linked Series for {label}', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "    if plot_file is not None:\n",
    "        plt.savefig(plot_file, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "b_idx = 0\n",
    "plot_hierarchically_linked_data(data=data, b_idx=b_idx, n_years=5, plot_file=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Geographically linked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_region_data(data, region_idx, plot_file=None):\n",
    "    x_plot = data['dates'][:-12]\n",
    "    Y_agg  = data['Y_agg'][region_idx] # [country,state,zone,region]\n",
    "    Y_base = data['Y'][region_idx]\n",
    "    \n",
    "    hier_idxs = data['hier_linked_idxs'][region_idx]\n",
    "    Y_agg_labels = data['hier_labels'][hier_idxs]\n",
    "    region = Y_agg_labels[3] #[country,state,zone,region,country_p,state_p,...]\n",
    "    \n",
    "    n_years = 5\n",
    "    fig, axs = plt.subplots(4,1, figsize=(12, 9))\n",
    "\n",
    "    for idx, purpose in enumerate(['Hol', 'Vis', 'Bus', 'Oth']):\n",
    "        axs[idx].plot(x_plot[-12*n_years:], Y_base[idx,-12*n_years:]/1000, \n",
    "                      color = 'black', linewidth=1.5, label=f'{idx}')\n",
    "        #axs[idx].set_xlabel('Month', fontsize=13)\n",
    "        axs[idx].set_ylabel(f'{region}{purpose} \\n Tourist Visits\\n (In millions)', fontsize=13)\n",
    "        if idx<3:\n",
    "            axs[idx].set_xticks([])\n",
    "        axs[idx].tick_params(labelsize=13)  \n",
    "    \n",
    "    axs[idx].plot(x_plot[-12*n_years:], Y_base[idx, -12*n_years:]/1000, \n",
    "                  color = 'black', linewidth=1.5, label=f'{idx}')\n",
    "    axs[3].set_xlabel('Month', fontsize=13)\n",
    "    axs[3].tick_params(labelsize=13)\n",
    "    \n",
    "    plt.suptitle(f'Purpose Grouped Series for {region}', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "    if plot_file is not None:\n",
    "        plt.savefig(plot_file, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "plot_region_data(data, region_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-4\"></a>\n",
    "## 4. HierTimeseriesDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HierTimeseriesDataset(Dataset):   \n",
    "    def __init__(self, \n",
    "                 # Bottom data\n",
    "                 Y, X, S,\n",
    "                 xcols,\n",
    "                 xcols_hist,\n",
    "                 xcols_futr,\n",
    "                 xcols_sample_mask,\n",
    "                 xcols_available_mask,\n",
    "                 dates,\n",
    "                 # Aggregated data\n",
    "                 Y_agg, X_agg, S_agg,\n",
    "                 xcols_agg,\n",
    "                 xcols_hist_agg,\n",
    "                 xcols_futr_agg,                 \n",
    "                 # Generator parameters\n",
    "                 T0, T, H,\n",
    "                 lastwindow_mask=False,\n",
    "                ):\n",
    "        \n",
    "        # Assert that raw data has correct series lens\n",
    "        series_len = [len(Y_agg), len(Y), len(X_agg), len(X), len(S_agg), len(S)]\n",
    "        assert len(np.unique(series_len))==1, f'Check your series length {series_len}'\n",
    "        \n",
    "        # Assert that raw data has correct time lens\n",
    "        assert Y.shape[-1]>=T0+T and X.shape[-1]>=T0+T+H+1,\\\n",
    "            f'Times (Y.shape={Y.shape[-1]},T0+T={T0+T}), (X.shape={X.shape[-1]}, T0+T+H+1={T0+T+H+1})'\n",
    "                \n",
    "        # Bottom data\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.S = S\n",
    "        \n",
    "        self.xcols      = xcols\n",
    "        self.xcols_hist = xcols_hist\n",
    "        self.xcols_futr = xcols_futr\n",
    "        self.xcols_sample_mask = xcols_sample_mask\n",
    "        self.xcols_available_mask = xcols_available_mask\n",
    "        \n",
    "        # Aggregated data\n",
    "        self.Y_agg = Y_agg\n",
    "        self.X_agg = X_agg\n",
    "        self.S_agg = S_agg\n",
    "        \n",
    "        self.xcols_agg = xcols_agg\n",
    "        self.xcols_hist_agg = xcols_hist_agg\n",
    "        self.xcols_futr_agg = xcols_futr_agg        \n",
    "        \n",
    "        # Feature indexes \n",
    "        self.hist_agg_col    = list(xcols_agg.get_indexer(xcols_hist_agg))\n",
    "        self.futr_agg_col    = list(xcols_agg.get_indexer(xcols_futr_agg))\n",
    "\n",
    "        self.hist_col        = list(xcols.get_indexer(xcols_hist))\n",
    "        self.futr_col        = list(xcols.get_indexer(xcols_futr))\n",
    "        self.sample_mask_col    = xcols.get_loc(xcols_sample_mask)\n",
    "        self.available_mask_col = xcols.get_loc(xcols_available_mask)\n",
    "        \n",
    "        self.dates = dates[T0+1:T0+T+H+1]\n",
    "        \n",
    "        # Copy sample mask to avoid overwriting it\n",
    "        self.sample_mask = self.X[:,:,self.sample_mask_col,:].copy()\n",
    "        if lastwindow_mask:\n",
    "            # Create dummy to identify observations of\n",
    "            # Y's last H steps (X's last H steps are forecast)\n",
    "            lastwindow_mask = np.zeros(self.X[:,:,self.sample_mask_col,:].shape)\n",
    "            lastwindow_mask[:,:,T0+T-H:T0+T] = 1\n",
    "            self.sample_mask = lastwindow_mask\n",
    "        \n",
    "        # Batch parameters\n",
    "        self.T0   = T0\n",
    "        self.T    = T\n",
    "        self.H    = H\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Y_agg) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Parse time indexes\n",
    "        T0 = self.T0\n",
    "        T  = self.T\n",
    "        H  = self.H\n",
    "        \n",
    "        # [G,N,C,T]\n",
    "        s = self.S[idx,:,:]\n",
    "        x = self.X[idx,:,:,T0:T0+T][:,self.hist_col,:]\n",
    "        f = self.X[idx,:,:,T0+1:T0+T+H+1][:,self.futr_col,:]\n",
    "        y = self.Y[idx,:,T0+1:T0+T]\n",
    "        \n",
    "        sample_mask    = self.sample_mask[idx,:,T0+1:T0+T]\n",
    "        available_mask = self.X[idx,:,:,T0+1:T0+T][:,self.available_mask_col,:]\n",
    "        \n",
    "        # [G,C,T] Shared features across N\n",
    "        s_agg    = self.S_agg[idx,:]\n",
    "        x_agg    = self.X_agg[idx,:,T0:T0+T,][self.hist_agg_col,:]\n",
    "        f_agg    = self.X_agg[idx,:,T0+1:T0+T+H+1][self.futr_agg_col,:]\n",
    "        y_agg    = self.Y_agg[idx,:,T0+1:T0+T]\n",
    "\n",
    "        batch = dict(# Bottom data\n",
    "                     Y=y, S=s, X=x, F=f,\n",
    "                     sample_mask=sample_mask,\n",
    "                     available_mask=available_mask,\n",
    "                     # Aggregated data\n",
    "                     Y_agg=y_agg, S_agg=s_agg, X_agg=x_agg, F_agg=f_agg)\n",
    "            \n",
    "        #print('\\n')\n",
    "        #print('================== BATCH ==============')\n",
    "        #for key in batch.keys():\n",
    "        #    print(f'{key}.shape:\\t{batch[key].shape}')\n",
    "        #print('\\n')\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-5\"></a>\n",
    "## 5. HierTimeseriesDataset Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuralforecast.data.datasets.tourismL import TourismL\n",
    "\n",
    "# directory = './data/hierarchical/TourismL'\n",
    "# TourismL.preprocess_data(directory=directory, verbose=False)\n",
    "# data = TourismL.load_process(directory=directory, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 228-12-12\n",
    "H = 12\n",
    "\n",
    "train_dataset = HierTimeseriesDataset(# Bottom data\n",
    "                                      X=data['X'], \n",
    "                                      S=data['S'], \n",
    "                                      Y=data['Y'],\n",
    "                                      xcols=data['xcols'],\n",
    "                                      xcols_hist=data['xcols_hist'],\n",
    "                                      xcols_futr=data['xcols_futr'],\n",
    "                                      xcols_sample_mask=data['xcols_sample_mask'],\n",
    "                                      xcols_available_mask=data['xcols_available_mask'],\n",
    "                                      dates = data['dates'],\n",
    "                                      # Aggregated data\n",
    "                                      X_agg=data['X_agg'], \n",
    "                                      S_agg=data['S_agg'], \n",
    "                                      Y_agg=data['Y_agg'],\n",
    "                                      xcols_agg=data['xcols_agg'],\n",
    "                                      xcols_hist_agg=data['xcols_hist_agg'],\n",
    "                                      xcols_futr_agg=data['xcols_futr_agg'],\n",
    "                                      # Generator parameters\n",
    "                                      T0=0,T=T,H=H)\n",
    "\n",
    "valid_dataset = HierTimeseriesDataset(# Bottom data\n",
    "                                      X=data['X'], \n",
    "                                      S=data['S'], \n",
    "                                      Y=data['Y'],\n",
    "                                      xcols=data['xcols'],\n",
    "                                      xcols_hist=data['xcols_hist'],\n",
    "                                      xcols_futr=data['xcols_futr'],\n",
    "                                      xcols_sample_mask=data['xcols_sample_mask'],\n",
    "                                      xcols_available_mask=data['xcols_available_mask'], \n",
    "                                      dates = data['dates'],\n",
    "                                      # Aggregated data\n",
    "                                      X_agg=data['X_agg'], \n",
    "                                      S_agg=data['S_agg'], \n",
    "                                      Y_agg=data['Y_agg'],\n",
    "                                      xcols_agg=data['xcols_agg'],\n",
    "                                      xcols_hist_agg=data['xcols_hist_agg'],\n",
    "                                      xcols_futr_agg=data['xcols_futr_agg'],\n",
    "                                      # Generator parameters\n",
    "                                      T0=24,T=T,H=H,\n",
    "                                      lastwindow_mask=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "train_batch = next(iter(train_dataloader))\n",
    "valid_batch = next(iter(valid_dataloader))\n",
    "\n",
    "for key in train_batch.keys():\n",
    "    print(f'{key}.shape', train_batch[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Examinate HierTimeSeriesDataset aggregate batch\n",
    "1. Examinate the hierarchically linked Y_agg series.\n",
    "2. Examinate the alignment between Y_agg and X_agg_futr.\n",
    "3. Validate the S_agg dummies generated from H constraints matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse batch and filter for first entry\n",
    "batch = valid_batch\n",
    "S_agg = batch['S_agg'][0]\n",
    "Y_agg = batch['Y_agg'][0]\n",
    "X_agg = batch['X_agg'][0]\n",
    "F_agg = batch['F_agg'][0]\n",
    "\n",
    "#---------------- Hierarchical Y_agg ----------------#\n",
    "plt.plot(Y_agg[0,:])\n",
    "plt.plot(Y_agg[1,:])\n",
    "plt.plot(Y_agg[2,:])\n",
    "plt.plot(Y_agg[3,:])\n",
    "plt.title('Y_agg')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#---------------- X_agg_futr ----------------#\n",
    "F_agg = np.maximum(F_agg[0,:]*80000,\n",
    "                   np.zeros(F_agg[0,:].shape))\n",
    "plt.plot(F_agg[-36:])\n",
    "plt.plot(Y_agg[0,-36:])\n",
    "plt.title('F_agg')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#---------------- S_agg ----------------#\n",
    "plt.figure(num=1, figsize=(5, 5), dpi=80, facecolor='w')\n",
    "plt.spy(S_agg[None,:])\n",
    "plt.title('S_agg')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Examinate HierTimeSeriesDataset bottom batch\n",
    "1. Examinate the alignment between Y_bottom, S_bottom and X_bottom_futr.\n",
    "2. Validate batch Seq2Seq structure between X_bottom vs Y_bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse batch and filter for first entry\n",
    "# batch = train_batch\n",
    "# dates = train_dataset.dates\n",
    "batch = valid_batch\n",
    "dates = valid_dataset.dates\n",
    "S_bottom = batch['S'][0]\n",
    "Y_bottom = batch['Y'][0]\n",
    "X_bottom = batch['X'][0]\n",
    "F_bottom = batch['F'][0]\n",
    "sample_mask = batch['sample_mask'][0]\n",
    "\n",
    "#---------------- Y/S/X_bottom alignment ----------------#\n",
    "n_years = 3\n",
    "fig, axs = plt.subplots(4,1, figsize=(12, 9))\n",
    "\n",
    "for idx, purpose in enumerate(['Hol', 'Vis', 'Bus', 'Oth']):    \n",
    "    axs[idx].plot(dates[1:n_years*12+1],Y_bottom[idx,:n_years*12], label='y')\n",
    "    axs[idx].axhline(y=S_bottom[idx,0],\n",
    "                label='level', color='black')\n",
    "    axs[idx].axhline(y=S_bottom[idx,0]*S_bottom[idx,2], \n",
    "                label='level_[december]', color='green')\n",
    "    axs[idx].axhline(y=S_bottom[idx,0]*S_bottom[idx,3],\n",
    "                label='level_[january]', color='red')\n",
    "    \n",
    "    axs[idx].plot(dates[1:n_years*12+1],\n",
    "                  F_bottom[idx,0,:n_years*12]*S_bottom[idx,0]*S_bottom[idx,2],\n",
    "                 label='dummy_[december]')\n",
    "    axs[idx].plot(dates[1:n_years*12+1],\n",
    "                  F_bottom[idx,1,:n_years*12]*S_bottom[idx,0]*S_bottom[idx,3],\n",
    "                 label='dummy_[january]')\n",
    "    axs[idx].set_ylabel(purpose)\n",
    "\n",
    "plt.suptitle('Y/S/X/F_bottom alignment ', y=0.91)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#---------------- Y/X_bottom Seq2Seq structure ----------------#\n",
    "idx = 0\n",
    "start = 155\n",
    "end = 203\n",
    "print('\\n')\n",
    "plt.figure(figsize=(12, 1.8))\n",
    "plt.plot(dates[start:end],     Y_bottom[idx,start:end]+100, label='y')\n",
    "plt.plot(dates[start-1:end-1], X_bottom[idx,0,start:end], label='y_[lag1]')\n",
    "plt.plot(dates[start:end+12],  F_bottom[idx,2,start:end+12], label='y_[lag12]')\n",
    "plt.plot(dates[start:end],     sample_mask[idx,start:end]*Y_bottom[idx,-1], label='sample_mask')\n",
    "plt.ylabel('Tourist Visits\\n [In millions]')\n",
    "plt.title('Y/X_bottom (Seq2Seq)')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "neuralforecast"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
