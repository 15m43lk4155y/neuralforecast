{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils\n",
    "> Set of functions to easily perform experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENV_VARS = dict(OMP_NUM_THREADS='2',\n",
    "                OPENBLAS_NUM_THREADS='2',\n",
    "                MKL_NUM_THREADS='3',\n",
    "                VECLIB_MAXIMUM_THREADS='2',\n",
    "                NUMEXPR_NUM_THREADS='3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import pickle\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ.update(ENV_VARS)\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch as t\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nixtlats.data.scalers import Scaler\n",
    "from nixtlats.data.tsdataset import TimeSeriesDataset, WindowsDataset, IterateWindowsDataset\n",
    "from nixtlats.data.tsloader import TimeSeriesLoader\n",
    "from nixtlats.models.esrnn.esrnn import ESRNN\n",
    "from nixtlats.models.esrnn.mqesrnn import MQESRNN\n",
    "from nixtlats.models.nbeats.nbeats import NBEATS\n",
    "from nixtlats.models.deepmidas.deepmidas import DeepMIDAS\n",
    "from nixtlats.models.transformer.autoformer import Autoformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mask_dfs(Y_df, ds_in_val, ds_in_test):\n",
    "    # train mask\n",
    "    train_mask_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    train_mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    train_mask_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_mask_df['sample_mask'] = 1\n",
    "    train_mask_df['available_mask'] = 1\n",
    "    \n",
    "    idx_out = train_mask_df.groupby('unique_id').tail(ds_in_val+ds_in_test).index\n",
    "    train_mask_df.loc[idx_out, 'sample_mask'] = 0\n",
    "    \n",
    "    # test mask\n",
    "    test_mask_df = train_mask_df.copy()\n",
    "    test_mask_df['sample_mask'] = 0\n",
    "    idx_test = test_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    test_mask_df.loc[idx_test, 'sample_mask'] = 1\n",
    "    \n",
    "    # validation mask\n",
    "    val_mask_df = train_mask_df.copy()\n",
    "    val_mask_df['sample_mask'] = 1\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - train_mask_df['sample_mask']\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - test_mask_df['sample_mask']\n",
    "\n",
    "    assert len(train_mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(train_mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_random_mask_dfs(Y_df, ds_in_test, \n",
    "                        n_val_windows, n_ds_val_window,\n",
    "                        n_uids, freq):\n",
    "    \"\"\"\n",
    "    Generates train, test and random validation mask.\n",
    "    Train mask begins by avoiding ds_in_test\n",
    "    \n",
    "    Validation mask: 1) samples n_uids unique ids\n",
    "                     2) creates windows of size n_ds_val_window\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    n_uids: int\n",
    "        Number of unique ids in validation.\n",
    "    n_val_windows: int\n",
    "        Number of windows for validation.\n",
    "    n_ds_val_window: int\n",
    "        Number of ds in each validation window.\n",
    "    periods: int  \n",
    "        ds_in_test multiplier.\n",
    "    freq: str\n",
    "        string that determines datestamp frequency, used in\n",
    "        random windows creation.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    #----------------------- Train mask -----------------------#\n",
    "    # Initialize masks\n",
    "    train_mask_df, val_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                            ds_in_val=0,\n",
    "                                                            ds_in_test=ds_in_test)\n",
    "    \n",
    "    assert val_mask_df['sample_mask'].sum()==0, 'Muerte'\n",
    "    \n",
    "    #----------------- Random Validation mask -----------------#\n",
    "    # Overwrite validation with random windows\n",
    "    uids = train_mask_df['unique_id'].unique()\n",
    "    val_uids = np.random.choice(uids, n_uids, replace=False)\n",
    "    \n",
    "    # Validation avoids test\n",
    "    idx_test = train_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    available_ds = train_mask_df.loc[~train_mask_df.index.isin(idx_test)]['ds'].unique()\n",
    "    val_init_ds = np.random.choice(available_ds, n_val_windows, replace=False)\n",
    "    \n",
    "    # Creates windows \n",
    "    val_ds = [pd.date_range(init, periods=n_ds_val_window, freq=freq) for init in val_init_ds]\n",
    "    val_ds = np.concatenate(val_ds)\n",
    "\n",
    "    # Cleans random windows from train mask\n",
    "    val_idx = train_mask_df.query('unique_id in @val_uids & ds in @val_ds').index\n",
    "    train_mask_df.loc[val_idx, 'sample_mask'] = 0\n",
    "    val_mask_df.loc[val_idx, 'sample_mask'] = 1\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def scale_data(Y_df, X_df, mask_df, normalizer_y, normalizer_x):\n",
    "    mask = mask_df['available_mask'].values * mask_df['sample_mask'].values\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "\n",
    "    if normalizer_x is not None:\n",
    "        X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        for col in X_cols:\n",
    "            scaler_x = Scaler(normalizer=normalizer_x)\n",
    "            X_df[col] = scaler_x.scale(x=X_df[col].values, mask=mask)\n",
    "\n",
    "    return Y_df, X_df, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_datasets(mc, S_df, Y_df, X_df, f_cols,\n",
    "                    ds_in_test, ds_in_val):\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    train_mask_df, valid_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                              ds_in_val=ds_in_val,\n",
    "                                                              ds_in_test=ds_in_test)\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "    Y_df, X_df, scaler_y = scale_data(Y_df=Y_df, X_df=X_df, mask_df=train_mask_df,\n",
    "                                      normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "\n",
    "    #----------------------------------------- Declare Dataset and Loaders ----------------------------------#\n",
    "    \n",
    "    if mc['mode'] == 'simple':\n",
    "        train_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       complete_windows=mc['complete_windows'],\n",
    "                                       verbose=True)\n",
    "        \n",
    "        valid_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                       complete_windows=True,\n",
    "                                       verbose=True)\n",
    "        \n",
    "        test_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                      mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                      input_size=int(mc['n_time_in']),\n",
    "                                      output_size=int(mc['n_time_out']),\n",
    "                                      sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                      complete_windows=True,\n",
    "                                      verbose=True)\n",
    "    if mc['mode'] == 'iterate_windows':\n",
    "        train_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                              mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                              input_size=int(mc['n_time_in']),\n",
    "                                              output_size=int(mc['n_time_out']),\n",
    "                                              verbose=True)\n",
    "        \n",
    "        valid_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                              mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                              input_size=int(mc['n_time_in']),\n",
    "                                              output_size=int(mc['n_time_out']),\n",
    "                                              verbose=True)\n",
    "        \n",
    "        test_dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                             mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                             input_size=int(mc['n_time_in']),\n",
    "                                             output_size=int(mc['n_time_out']),\n",
    "                                             verbose=True)   \n",
    "    \n",
    "    if mc['mode'] == 'full':\n",
    "        train_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          verbose=True)\n",
    "        \n",
    "        valid_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          verbose=True)\n",
    "        \n",
    "        test_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                         mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                         input_size=int(mc['n_time_in']),\n",
    "                                         output_size=int(mc['n_time_out']),\n",
    "                                         verbose=True)        \n",
    "    \n",
    "    if ds_in_test == 0:\n",
    "        test_dataset = None\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_loaders(mc, train_dataset, val_dataset, test_dataset):\n",
    "    if mc['mode'] in ['simple', 'full'] :\n",
    "        train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                        batch_size=int(mc['batch_size']),\n",
    "                                        n_windows=int(mc['n_windows']),\n",
    "                                        eq_batch_size=False,\n",
    "                                        shuffle=True)\n",
    "        if val_dataset is not None:\n",
    "            val_loader = TimeSeriesLoader(dataset=val_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        if test_dataset is not None:\n",
    "            test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)\n",
    "        else:\n",
    "            test_loader = None\n",
    "\n",
    "    elif mc['mode'] == 'iterate_windows':\n",
    "        train_loader =DataLoader(dataset=train_dataset,\n",
    "                                 batch_size=int(mc['batch_size']),\n",
    "                                 shuffle=True,\n",
    "                                 drop_last=True)\n",
    "\n",
    "        if val_dataset is not None:\n",
    "            val_loader = DataLoader(dataset=val_dataset,\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=False)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        if test_dataset is not None:\n",
    "            test_loader = DataLoader(dataset=test_dataset,\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=False)\n",
    "        else:\n",
    "            test_loader = None\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nbeats(mc):\n",
    "    mc['n_theta_hidden'] = len(mc['stack_types']) * [ [int(mc['n_hidden']), int(mc['n_hidden'])] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "\n",
    "    model = NBEATS(n_time_in=int(mc['n_time_in']),\n",
    "                   n_time_out=int(mc['n_time_out']),\n",
    "                   n_x=mc['n_x'],\n",
    "                   n_s=mc['n_s'],\n",
    "                   n_s_hidden=int(mc['n_s_hidden']),\n",
    "                   n_x_hidden=int(mc['n_x_hidden']),\n",
    "                   shared_weights=mc['shared_weights'],\n",
    "                   initialization=mc['initialization'],\n",
    "                   activation=mc['activation'],\n",
    "                   stack_types=mc['stack_types'],\n",
    "                   n_blocks=mc['n_blocks'],\n",
    "                   n_layers=mc['n_layers'],\n",
    "                   n_theta_hidden=mc['n_theta_hidden'],\n",
    "                   n_harmonics=int(mc['n_harmonics']),\n",
    "                   n_polynomials=int(mc['n_polynomials']),\n",
    "                   batch_normalization = mc['batch_normalization'],\n",
    "                   dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                   learning_rate=float(mc['learning_rate']),\n",
    "                   lr_decay=float(mc['lr_decay']),\n",
    "                   lr_decay_step_size=lr_decay_step_size,\n",
    "                   weight_decay=mc['weight_decay'],\n",
    "                   loss_train=mc['loss_train'],\n",
    "                   loss_hypar=float(mc['loss_hypar']),\n",
    "                   loss_valid=mc['loss_valid'],\n",
    "                   frequency=mc['frequency'],\n",
    "                   seasonality=int(mc['seasonality']),\n",
    "                   random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_esrnn(mc):    \n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "    model = ESRNN(# Architecture parameters\n",
    "                  n_series=mc['n_series'],\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  sample_freq=int(mc['sample_freq']),\n",
    "                  input_size=int(mc['n_time_in']),\n",
    "                  output_size=int(mc['n_time_out']),\n",
    "                  es_component=mc['es_component'],\n",
    "                  cell_type=mc['cell_type'],\n",
    "                  state_hsize=int(mc['state_hsize']),\n",
    "                  dilations=mc['dilations'],\n",
    "                  add_nl_layer=mc['add_nl_layer'],\n",
    "                  # Optimization parameters                \n",
    "                  learning_rate=mc['learning_rate'],\n",
    "                  lr_scheduler_step_size=lr_decay_step_size,\n",
    "                  lr_decay=mc['lr_decay'],\n",
    "                  per_series_lr_multip=mc['per_series_lr_multip'],\n",
    "                  gradient_eps=mc['gradient_eps'],\n",
    "                  gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                  rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                  noise_std=mc['noise_std'],\n",
    "                  level_variability_penalty=mc['level_variability_penalty'],\n",
    "                  testing_percentile=mc['testing_percentile'],\n",
    "                  training_percentile=mc['training_percentile'],\n",
    "                  loss=mc['loss_train'],\n",
    "                  val_loss=mc['loss_valid'],\n",
    "                  seasonality=mc['seasonality']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_mqesrnn(mc): \n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])   \n",
    "    model = MQESRNN(# Architecture parameters\n",
    "                    n_series=mc['n_series'],\n",
    "                    n_x=mc['n_x'],\n",
    "                    n_s=mc['n_s'],\n",
    "                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                    input_size=int(mc['n_time_in']),\n",
    "                    output_size=int(mc['n_time_out']),\n",
    "                    es_component=mc['es_component'],\n",
    "                    cell_type=mc['cell_type'],\n",
    "                    state_hsize=int(mc['state_hsize']),\n",
    "                    dilations=mc['dilations'],\n",
    "                    add_nl_layer=mc['add_nl_layer'],\n",
    "                    # Optimization parameters                 \n",
    "                    learning_rate=mc['learning_rate'],\n",
    "                    lr_scheduler_step_size=lr_decay_step_size,\n",
    "                    lr_decay=mc['lr_decay'],\n",
    "                    gradient_eps=mc['gradient_eps'],\n",
    "                    gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                    rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                    noise_std=mc['noise_std'],\n",
    "                    testing_percentiles=list(mc['testing_percentiles']),\n",
    "                    training_percentiles=list(mc['training_percentiles']),\n",
    "                    loss=mc['loss_train'],\n",
    "                    val_loss=mc['loss_valid']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_deepmidas(mc):\n",
    "    mc['n_theta_hidden'] = len(mc['stack_types']) * [ [int(mc['n_hidden']), int(mc['n_hidden'])] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "\n",
    "    model = DeepMIDAS(n_time_in=int(mc['n_time_in']),\n",
    "                      n_time_out=int(mc['n_time_out']),\n",
    "                      n_x=mc['n_x'],\n",
    "                      n_s=mc['n_s'],\n",
    "                      n_s_hidden=int(mc['n_s_hidden']),\n",
    "                      n_x_hidden=int(mc['n_x_hidden']),\n",
    "                      shared_weights = mc['shared_weights'],\n",
    "                      initialization=mc['initialization'],\n",
    "                      activation=mc['activation'],\n",
    "                      stack_types=mc['stack_types'],\n",
    "                      n_blocks=mc['n_blocks'],\n",
    "                      n_layers=mc['n_layers'],\n",
    "                      n_theta_hidden=mc['n_theta_hidden'],\n",
    "                      n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "                      n_freq_downsample=mc['n_freq_downsample'],\n",
    "                      interpolation_mode=mc['interpolation_mode'],\n",
    "                      batch_normalization = mc['batch_normalization'],\n",
    "                      dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                      learning_rate=float(mc['learning_rate']),\n",
    "                      lr_decay=float(mc['lr_decay']),\n",
    "                      lr_decay_step_size=lr_decay_step_size,\n",
    "                      weight_decay=mc['weight_decay'],\n",
    "                      loss_train=mc['loss_train'],\n",
    "                      loss_hypar=float(mc['loss_hypar']),\n",
    "                      loss_valid=mc['loss_valid'],\n",
    "                      frequency=mc['frequency'],\n",
    "                      seasonality=int(mc['seasonality']),\n",
    "                      random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_autoformer(mc):\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "    \n",
    "    model = Autoformer(seq_len=int(mc['seq_len']),\n",
    "                       label_len=int(mc['label_len']),\n",
    "                       pred_len=int(mc['pred_len']),\n",
    "                       output_attention=mc['output_attention'],\n",
    "                       enc_in=int(mc['enc_in']),\n",
    "                       dec_in=int(mc['dec_in']),\n",
    "                       d_model=int(mc['d_model']),\n",
    "                       c_out=int(mc['c_out']),\n",
    "                       embed = mc['embed'],\n",
    "                       freq=mc['freq'],\n",
    "                       dropout=mc['dropout'],\n",
    "                       factor=mc['factor'],\n",
    "                       n_heads=int(mc['n_heads']),\n",
    "                       d_ff=int(mc['d_ff']),\n",
    "                       moving_avg=int(mc['moving_avg']),\n",
    "                       activation=mc['activation'],\n",
    "                       e_layers=int(mc['e_layers']),\n",
    "                       d_layers=int(mc['d_layers']),\n",
    "                       learning_rate=float(mc['learning_rate']),\n",
    "                       lr_decay=float(mc['lr_decay']),\n",
    "                       lr_decay_step_size=lr_decay_step_size,\n",
    "                       weight_decay=mc['weight_decay'],\n",
    "                       loss_train=mc['loss_train'],\n",
    "                       loss_hypar=float(mc['loss_hypar']),\n",
    "                       loss_valid=mc['loss_valid'],\n",
    "                       random_seed=int(mc['random_seed']))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_model(mc):\n",
    "    MODEL_DICT = {'nbeats': instantiate_nbeats,\n",
    "                  'esrnn': instantiate_esrnn,\n",
    "                  'mqesrnn': instantiate_mqesrnn,\n",
    "                  'deepmidas': instantiate_deepmidas,\n",
    "                  'autoformer': instantiate_autoformer}\n",
    "    return MODEL_DICT[mc['model']](mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def predict(mc, model, trainer, loader, scaler_y):\n",
    "   outputs = trainer.predict(model, loader)\n",
    "   y_true, y_hat, mask = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "   meta_data = loader.dataset.meta_data\n",
    "\n",
    "   # Scale to original scale\n",
    "   if mc['normalizer_y'] is not None:\n",
    "        y_true_shape = y_true.shape\n",
    "        y_true = scaler_y.inv_scale(x=y_true.flatten())\n",
    "        y_true = np.reshape(y_true, y_true_shape)\n",
    "\n",
    "        y_hat = scaler_y.inv_scale(x=y_hat.flatten())\n",
    "        y_hat = np.reshape(y_hat, y_true_shape)\n",
    "\n",
    "   return y_true, y_hat, mask, meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_fit_predict(mc, S_df, Y_df, X_df, f_cols, ds_in_val, ds_in_test):\n",
    "    \n",
    "    # Protect inplace modifications\n",
    "    Y_df = Y_df.copy()\n",
    "    if X_df is not None:\n",
    "        X_df = X_df.copy()\n",
    "    if S_df is not None:\n",
    "        S_df = S_df.copy()        \n",
    "\n",
    "    #----------------------------------------------- Datasets -----------------------------------------------#\n",
    "    train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc,\n",
    "                                                                         S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                                                         f_cols=f_cols,\n",
    "                                                                         ds_in_val=ds_in_val,\n",
    "                                                                         ds_in_test=ds_in_test)\n",
    "    mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()\n",
    "\n",
    "    #------------------------------------------- Instantiate & fit -------------------------------------------#\n",
    "    train_loader, val_loader, test_loader = instantiate_loaders(mc=mc,\n",
    "                                                                train_dataset=train_dataset,\n",
    "                                                                val_dataset=val_dataset,\n",
    "                                                                test_dataset=test_dataset)\n",
    "    model = instantiate_model(mc=mc)\n",
    "    callbacks = []\n",
    "    if mc['early_stop_patience']:\n",
    "        early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, \n",
    "                                                    patience=mc['early_stop_patience'],\n",
    "                                                    verbose=True, \n",
    "                                                    mode='min') \n",
    "        callbacks=[early_stopping]\n",
    "\n",
    "    gpus = -1 if t.cuda.is_available() else 0\n",
    "    trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                         max_steps=mc['max_steps'],\n",
    "                         check_val_every_n_epoch=mc['eval_freq'],\n",
    "                         progress_bar_refresh_rate=1,\n",
    "                         gpus=gpus,\n",
    "                         callbacks=callbacks,\n",
    "                         checkpoint_callback=False,\n",
    "                         logger=False)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    #------------------------------------------------ Predict ------------------------------------------------#\n",
    "    results = {}\n",
    "\n",
    "    if ds_in_val > 0:\n",
    "        y_true, y_hat, mask, meta_data = predict(mc, model, trainer, val_loader, scaler_y)\n",
    "        val_values = (('val_y_true', y_true), ('val_y_hat', y_hat), ('val_mask', mask), ('val_meta_data', meta_data))\n",
    "        results.update(val_values)\n",
    "\n",
    "        print(f\"VAL y_true.shape: {y_true.shape}\")\n",
    "        print(f\"VAL y_hat.shape: {y_hat.shape}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Predict test if available\n",
    "    if ds_in_test > 0:\n",
    "        y_true, y_hat, mask, meta_data = predict(mc, model, trainer, test_loader, scaler_y)\n",
    "        test_values = (('test_y_true', y_true), ('test_y_hat', y_hat), ('test_mask', mask), ('test_meta_data', meta_data))\n",
    "        results.update(test_values)\n",
    "\n",
    "        print(f\"TEST y_true.shape: {y_true.shape}\")\n",
    "        print(f\"TEST y_hat.shape: {y_hat.shape}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_model(mc, loss_function_val, loss_functions_test, \n",
    "                   S_df, Y_df, X_df, f_cols,\n",
    "                   ds_in_val, ds_in_test,\n",
    "                   return_forecasts,\n",
    "                   save_progress,\n",
    "                   trials,\n",
    "                   results_file,\n",
    "                   loss_kwargs):\n",
    "\n",
    "    if (save_progress) and (len(trials) % 5 == 0):\n",
    "        with open(results_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "    \n",
    "    print(47*'=' + '\\n')\n",
    "    print(pd.Series(mc))\n",
    "    print(47*'=' + '\\n')\n",
    "    \n",
    "    # Some asserts due to work in progress\n",
    "    n_series = Y_df['unique_id'].nunique()\n",
    "    if n_series > 1:\n",
    "        assert mc['normalizer_y'] is None, 'Data scaling not implemented with multiple time series'\n",
    "        assert mc['normalizer_x'] is None, 'Data scaling not implemented with multiple time series'\n",
    "\n",
    "    assert ds_in_test % mc['val_idx_to_sample_freq']==0, 'outsample size should be multiple of val_idx_to_sample_freq'\n",
    "\n",
    "    # Make predictions\n",
    "    start = time.time()\n",
    "    results = model_fit_predict(mc=mc,\n",
    "                                S_df=S_df, \n",
    "                                Y_df=Y_df,\n",
    "                                X_df=X_df,\n",
    "                                f_cols=f_cols,\n",
    "                                ds_in_val=ds_in_val,\n",
    "                                ds_in_test=ds_in_test)\n",
    "    run_time = time.time() - start\n",
    "\n",
    "    # Evaluate predictions\n",
    "    val_loss = loss_function_val(y=results['val_y_true'], y_hat=results['val_y_hat'], weights=results['val_mask'], **loss_kwargs)\n",
    "\n",
    "    results_output = {'loss': val_loss,\n",
    "                      'mc': mc,\n",
    "                      'run_time': run_time,\n",
    "                      'status': STATUS_OK}\n",
    "\n",
    "    # Evaluation in test (if provided)\n",
    "    if ds_in_test > 0:\n",
    "        test_loss_dict = {}\n",
    "        for loss_name, loss_function in loss_functions_test.items():\n",
    "            test_loss_dict[loss_name] = loss_function(y=results['test_y_true'], y_hat=results['test_y_hat'], weights=results['test_mask'])\n",
    "        results_output['test_losses'] = test_loss_dict\n",
    "\n",
    "    if return_forecasts:\n",
    "        forecasts_test = {}\n",
    "        test_values = (('test_y_true', results['test_y_true']), ('test_y_hat', results['test_y_hat']),\n",
    "                        ('test_mask', results['test_mask']), ('test_meta_data', results['test_meta_data']))\n",
    "        forecasts_test.update(test_values)\n",
    "        results_output['forecasts_test'] = forecasts_test\n",
    "\n",
    "    return results_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hyperopt_tunning(space, hyperopt_max_evals, loss_function_val, loss_functions_test,\n",
    "                     S_df, Y_df, X_df, f_cols,\n",
    "                     ds_in_val, ds_in_test,\n",
    "                     return_forecasts,\n",
    "                     save_progress,\n",
    "                     results_file,\n",
    "                     loss_kwargs=None):\n",
    "    assert ds_in_val > 0, 'Validation set is needed for tunning!'\n",
    "\n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(evaluate_model, loss_function_val=loss_function_val, loss_functions_test=loss_functions_test,\n",
    "                             S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=f_cols,\n",
    "                             ds_in_val=ds_in_val, ds_in_test=ds_in_test,\n",
    "                             return_forecasts=return_forecasts, save_progress=save_progress, trials=trials,\n",
    "                             results_file=results_file,\n",
    "                             loss_kwargs=loss_kwargs or {})\n",
    "\n",
    "    fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=hyperopt_max_evals, trials=trials, verbose=True)\n",
    "\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from nixtlats.losses.numpy import mae, mape, smape, rmse, pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if t.cuda.is_available(): device = 'cuda'  \n",
    "\n",
    "deepmidas_space= {# Architecture parameters\n",
    "               'model':'deepmidas',\n",
    "               'mode': 'simple',\n",
    "               'n_time_in': hp.choice('n_time_in', [7*24]),\n",
    "               'n_time_out': hp.choice('n_time_out', [24]),\n",
    "               'n_x_hidden': hp.quniform('n_x_hidden', 1, 10, 1),\n",
    "               'n_s_hidden': hp.choice('n_s_hidden', [0]),\n",
    "               'shared_weights': hp.choice('shared_weights', [False]),\n",
    "               'activation': hp.choice('activation', ['SELU']),\n",
    "               'initialization':  hp.choice('initialization', ['glorot_normal','he_normal']),\n",
    "               'stack_types': hp.choice('stack_types', [2*['identity']]),\n",
    "               'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "               'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "               'n_hidden': hp.choice('n_hidden', [ 256 ]),\n",
    "               'n_pool_kernel_size': hp.choice('n_pool_kernel_size', [ [ 4, 1 ] ]),\n",
    "               'n_freq_downsample': hp.choice('n_freq_downsample', [ [ 24, 1 ] ]), \n",
    "               'interpolation_mode': hp.choice('interpolation_mode', ['linear']),\n",
    "               # Regularization and optimization parameters\n",
    "               'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "               'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 0.5),\n",
    "               'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "               'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.001)),\n",
    "               'lr_decay': hp.uniform('lr_decay', 0.3, 0.5),\n",
    "               'n_lr_decays': hp.choice('n_lr_decays', [3]), \n",
    "               'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "               'max_epochs': hp.choice('max_epochs', [10]), #'n_iterations': hp.choice('n_iterations', [10])\n",
    "               'max_steps': hp.choice('max_steps', [None]),\n",
    "               'early_stop_patience': hp.choice('early_stop_patience', [16]),\n",
    "               'eval_freq': hp.choice('eval_freq', [50]),\n",
    "               'loss_train': hp.choice('loss', ['MAE']),\n",
    "               'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "               'loss_valid': hp.choice('loss_valid', ['MAE']), #[args.val_loss]),\n",
    "               'l1_theta': hp.choice('l1_theta', [0]),\n",
    "               # Data parameters\n",
    "               'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "               'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "               'complete_windows': hp.choice('complete_windows', [False]),\n",
    "               'frequency': hp.choice('frequency', ['H']),\n",
    "               'seasonality': hp.choice('seasonality', [24]),      \n",
    "               'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "               'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [24]),\n",
    "               'batch_size': hp.choice('batch_size', [32]),\n",
    "               'n_windows': hp.choice('n_windows', [256]),\n",
    "               'random_seed': hp.quniform('random_seed', 10, 20, 1),\n",
    "               'device': hp.choice('device', [device])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.010948 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================      \n",
      "\n",
      "activation                                SELU       \n",
      "batch_normalization                      False\n",
      "batch_size                                  32\n",
      "complete_windows                         False\n",
      "device                                     cpu\n",
      "dropout_prob_exogenous                0.395977\n",
      "dropout_prob_theta                    0.258466\n",
      "early_stop_patience                         16\n",
      "eval_freq                                   50\n",
      "frequency                                    H\n",
      "idx_to_sample_freq                          24\n",
      "initialization                       he_normal\n",
      "interpolation_mode                      linear\n",
      "l1_theta                                     0\n",
      "learning_rate                         0.000662\n",
      "loss_hypar                                 0.5\n",
      "loss_train                                 MAE\n",
      "loss_valid                                 MAE\n",
      "lr_decay                              0.485504\n",
      "max_epochs                                  10\n",
      "max_steps                                 None\n",
      "mode                                    simple\n",
      "model                                deepmidas\n",
      "n_blocks                                (1, 1)\n",
      "n_freq_downsample                      (24, 1)\n",
      "n_hidden                                   256\n",
      "n_layers                                (2, 2)\n",
      "n_lr_decays                                  3\n",
      "n_pool_kernel_size                      (4, 1)\n",
      "n_s_hidden                                   0\n",
      "n_time_in                                  168\n",
      "n_time_out                                  24\n",
      "n_windows                                  256\n",
      "n_x_hidden                                 2.0\n",
      "normalizer_x                            median\n",
      "normalizer_y                              None\n",
      "random_seed                               14.0\n",
      "seasonality                                 24\n",
      "shared_weights                           False\n",
      "stack_types               (identity, identity)\n",
      "val_idx_to_sample_freq                      24\n",
      "weight_decay                          0.000069\n",
      "dtype: object\n",
      "===============================================      \n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | _DeepMIDAS | 376 K \n",
      "-------------------------------------\n",
      "376 K     Trainable params\n",
      "0         Non-trainable params\n",
      "376 K     Total params\n",
      "1.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]          \n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: -1it [00:00, ?it/s]                        \n",
      "Training:   0%|          | 0/1 [00:00<00:00, 27235.74it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<00:00, 408.72it/s]   \n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00, 57.83it/s, loss=4.83, train_loss_step=4.830]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<00:00, 25420.02it/s, loss=4.83, train_loss_step=4.830]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<00:00, 398.05it/s, loss=4.83, train_loss_step=4.830]  \n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00, 70.52it/s, loss=5.36, train_loss_step=5.880, train_loss_epoch=4.830]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<00:00, 25575.02it/s, loss=5.36, train_loss_step=5.880, train_loss_epoch=4.830]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<00:00, 401.56it/s, loss=5.36, train_loss_step=5.880, train_loss_epoch=4.830]  \n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00, 59.78it/s, loss=4.92, train_loss_step=4.050, train_loss_epoch=5.880] \n",
      "Epoch 2:   0%|          | 0/1 [00:00<00:00, 25575.02it/s, loss=4.92, train_loss_step=4.050, train_loss_epoch=5.880]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00, 393.24it/s, loss=4.92, train_loss_step=4.050, train_loss_epoch=5.880]  \n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00, 66.19it/s, loss=4.67, train_loss_step=3.940, train_loss_epoch=4.050] \n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00, 25575.02it/s, loss=4.67, train_loss_step=3.940, train_loss_epoch=4.050]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<00:00, 425.90it/s, loss=4.67, train_loss_step=3.940, train_loss_epoch=4.050]  \n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459064158/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00, 69.94it/s, loss=4.57, train_loss_step=4.150, train_loss_epoch=3.940] \n",
      "Epoch 4:   0%|          | 0/1 [00:00<00:00, 22192.08it/s, loss=4.57, train_loss_step=4.150, train_loss_epoch=3.940]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00, 444.22it/s, loss=4.57, train_loss_step=4.150, train_loss_epoch=3.940]  \n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00, 64.62it/s, loss=4.45, train_loss_step=3.880, train_loss_epoch=4.150] \n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00, 21845.33it/s, loss=4.45, train_loss_step=3.880, train_loss_epoch=4.150]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00, 394.65it/s, loss=4.45, train_loss_step=3.880, train_loss_epoch=4.150]  \n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00, 63.85it/s, loss=4.28, train_loss_step=3.240, train_loss_epoch=3.880] \n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00, 26546.23it/s, loss=4.28, train_loss_step=3.240, train_loss_epoch=3.880]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<00:00, 415.85it/s, loss=4.28, train_loss_step=3.240, train_loss_epoch=3.880]  \n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00, 66.42it/s, loss=4.15, train_loss_step=3.250, train_loss_epoch=3.240] \n",
      "Epoch 7:   0%|          | 0/1 [00:00<00:00, 30615.36it/s, loss=4.15, train_loss_step=3.250, train_loss_epoch=3.240]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00, 454.57it/s, loss=4.15, train_loss_step=3.250, train_loss_epoch=3.240]  \n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00, 65.53it/s, loss=4.06, train_loss_step=3.300, train_loss_epoch=3.250] \n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00, 26886.56it/s, loss=4.06, train_loss_step=3.300, train_loss_epoch=3.250]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<00:00, 464.28it/s, loss=4.06, train_loss_step=3.300, train_loss_epoch=3.250]  \n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 73.07it/s, loss=3.93, train_loss_step=2.790, train_loss_epoch=3.300] \n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 65.24it/s, loss=3.93, train_loss_step=2.790, train_loss_epoch=3.300]\n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 1it [00:00, ?it/s]                       \n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]    \n",
      "VAL y_true.shape: (7, 24)                            \n",
      "VAL y_hat.shape: (7, 24)                             \n",
      "Predicting: 1it [00:00, ?it/s]                       \n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]    \n",
      "TEST y_true.shape: (7, 24)                           \n",
      "TEST y_hat.shape: (7, 24)                            \n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.42s/trial, best loss: 6.462628364562988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.010047 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 6.462628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================                               \n",
      "\n",
      "activation                                SELU                                \n",
      "batch_normalization                      False\n",
      "batch_size                                  32\n",
      "complete_windows                         False\n",
      "device                                     cpu\n",
      "dropout_prob_exogenous                0.331015\n",
      "dropout_prob_theta                    0.383181\n",
      "early_stop_patience                         16\n",
      "eval_freq                                   50\n",
      "frequency                                    H\n",
      "idx_to_sample_freq                          24\n",
      "initialization                       he_normal\n",
      "interpolation_mode                      linear\n",
      "l1_theta                                     0\n",
      "learning_rate                          0.00061\n",
      "loss_hypar                                 0.5\n",
      "loss_train                                 MAE\n",
      "loss_valid                                 MAE\n",
      "lr_decay                              0.375691\n",
      "max_epochs                                  10\n",
      "max_steps                                 None\n",
      "mode                                    simple\n",
      "model                                deepmidas\n",
      "n_blocks                                (1, 1)\n",
      "n_freq_downsample                      (24, 1)\n",
      "n_hidden                                   256\n",
      "n_layers                                (2, 2)\n",
      "n_lr_decays                                  3\n",
      "n_pool_kernel_size                      (4, 1)\n",
      "n_s_hidden                                   0\n",
      "n_time_in                                  168\n",
      "n_time_out                                  24\n",
      "n_windows                                  256\n",
      "n_x_hidden                                 1.0\n",
      "normalizer_x                            median\n",
      "normalizer_y                              None\n",
      "random_seed                               11.0\n",
      "seasonality                                 24\n",
      "shared_weights                           False\n",
      "stack_types               (identity, identity)\n",
      "val_idx_to_sample_freq                      24\n",
      "weight_decay                          0.000052\n",
      "dtype: object\n",
      "===============================================                               \n",
      "\n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.42s/trial, best loss: 6.462628364562988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | _DeepMIDAS | 376 K \n",
      "-------------------------------------\n",
      "376 K     Trainable params\n",
      "0         Non-trainable params\n",
      "376 K     Total params\n",
      "1.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]                                   \n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]                \n",
      " 50%|█████     | 1/2 [00:02<00:01,  1.42s/trial, best loss: 6.462628364562988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: -1it [00:00, ?it/s]                                                 \n",
      "Training:   0%|          | 0/1 [00:00<00:00, 21732.15it/s]                    \n",
      "Epoch 0:   0%|          | 0/1 [00:00<00:00, 311.03it/s]                       \n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00, 54.48it/s, loss=4.81, train_loss_step=4.810]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<00:00, 13706.88it/s, loss=4.81, train_loss_step=4.810]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<00:00, 397.15it/s, loss=4.81, train_loss_step=4.810]  \n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00, 59.15it/s, loss=5.21, train_loss_step=5.620, train_loss_epoch=4.810]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<00:00, 22795.13it/s, loss=5.21, train_loss_step=5.620, train_loss_epoch=4.810]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<00:00, 376.51it/s, loss=5.21, train_loss_step=5.620, train_loss_epoch=4.810]  \n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00, 56.89it/s, loss=4.76, train_loss_step=3.850, train_loss_epoch=5.620] \n",
      "Epoch 2:   0%|          | 0/1 [00:00<00:00, 15141.89it/s, loss=4.76, train_loss_step=3.850, train_loss_epoch=5.620]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00, 77.02it/s, loss=4.76, train_loss_step=3.850, train_loss_epoch=5.620]   \n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00, 40.83it/s, loss=4.71, train_loss_step=4.560, train_loss_epoch=3.850]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00, 8004.40it/s, loss=4.71, train_loss_step=4.560, train_loss_epoch=3.850]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<00:00, 302.14it/s, loss=4.71, train_loss_step=4.560, train_loss_epoch=3.850] \n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00, 55.28it/s, loss=4.62, train_loss_step=4.260, train_loss_epoch=4.560] \n",
      "Epoch 4:   0%|          | 0/1 [00:00<00:00, 24966.10it/s, loss=4.62, train_loss_step=4.260, train_loss_epoch=4.560]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00, 428.12it/s, loss=4.62, train_loss_step=4.260, train_loss_epoch=4.560]  \n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00, 61.58it/s, loss=4.52, train_loss_step=4.030, train_loss_epoch=4.260] \n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00, 14665.40it/s, loss=4.52, train_loss_step=4.030, train_loss_epoch=4.260]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00, 347.93it/s, loss=4.52, train_loss_step=4.030, train_loss_epoch=4.260]  \n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00, 59.89it/s, loss=4.44, train_loss_step=3.940, train_loss_epoch=4.030] \n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00, 23301.69it/s, loss=4.44, train_loss_step=3.940, train_loss_epoch=4.030]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<00:00, 391.11it/s, loss=4.44, train_loss_step=3.940, train_loss_epoch=4.030]  \n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00, 56.26it/s, loss=4.28, train_loss_step=3.190, train_loss_epoch=3.940] \n",
      "Epoch 7:   0%|          | 0/1 [00:00<00:00, 22671.91it/s, loss=4.28, train_loss_step=3.190, train_loss_epoch=3.940]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00, 404.04it/s, loss=4.28, train_loss_step=3.190, train_loss_epoch=3.940]  \n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00, 67.33it/s, loss=4.17, train_loss_step=3.280, train_loss_epoch=3.190] \n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00, 25115.59it/s, loss=4.17, train_loss_step=3.280, train_loss_epoch=3.190]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<00:00, 437.50it/s, loss=4.17, train_loss_step=3.280, train_loss_epoch=3.190]  \n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 62.36it/s, loss=4.07, train_loss_step=3.170, train_loss_epoch=3.280] \n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 52.79it/s, loss=4.07, train_loss_step=3.170, train_loss_epoch=3.280]\n",
      " 50%|█████     | 1/2 [00:02<00:01,  1.42s/trial, best loss: 6.462628364562988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 1it [00:00, ?it/s]                                                \n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]                             \n",
      "VAL y_true.shape: (7, 24)                                                     \n",
      "VAL y_hat.shape: (7, 24)                                                      \n",
      "Predicting: 1it [00:00, ?it/s]                                                \n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]                             \n",
      "TEST y_true.shape: (7, 24)                                                    \n",
      "TEST y_hat.shape: (7, 24)                                                     \n",
      "100%|██████████| 2/2 [00:02<00:00,  1.40s/trial, best loss: 6.462628364562988]\n"
     ]
    }
   ],
   "source": [
    "from nixtlats.data.datasets.epf import EPF, EPFInfo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = ['NP']\n",
    "\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "\n",
    "X_df = X_df[['unique_id', 'ds', 'week_day']]\n",
    "Y_min = Y_df.y.min()\n",
    "\n",
    "trials = hyperopt_tunning(space=deepmidas_space, hyperopt_max_evals=2, loss_function_val=mae,\n",
    "                          loss_functions_test={'mae': mae, 'rmse': rmse},\n",
    "                          S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "                          ds_in_val=7*24, ds_in_test=7*24, return_forecasts=True, save_progress=False, results_file=None, loss_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtlats.data.datasets.ett import ETT\n",
    "\n",
    "Y_df, X_df, S_df = ETT.load(directory='./data', group='ETTm2')\n",
    "Y_df = Y_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoformer_space= {# Architecture parameters\n",
    "#                 'model':'autoformer',\n",
    "#                 'mode': 'iterate_windows',\n",
    "#                 'seq_len': hp.choice('seq_len', [96]),\n",
    "#                 'label_len': hp.choice('label_len', [48]),\n",
    "#                 'pred_len': hp.choice('pred_len', [24]),\n",
    "#                 'output_attention': hp.choice('output_attention', [False]),\n",
    "#                 'enc_in': hp.choice('enc_in', [7]),\n",
    "#                 'dec_in': hp.choice('dec_in', [7]),\n",
    "#                 'd_model': hp.choice('d_model', [512]),\n",
    "#                 'c_out': hp.choice('c_out', [7]),\n",
    "#                 'embed': hp.choice('embed', ['timeF']),\n",
    "#                 'freq': hp.choice('freq', ['h']),\n",
    "#                 'dropout': hp.choice('dropout', [0.05]),\n",
    "#                 'factor': hp.choice('factor', [1]),\n",
    "#                 'n_heads': hp.choice('n_heads', [8]),\n",
    "#                 'd_ff': hp.choice('d_ff', [2_048]),\n",
    "#                 'moving_avg': hp.choice('moving_avg', [25]),\n",
    "#                 'activation': hp.choice('activation', ['gelu']),\n",
    "#                 'e_layers': hp.choice('e_layers', [2]),\n",
    "#                 'd_layers': hp.choice('d_layers', [1]),\n",
    "#                 # Regularization and optimization parameters\n",
    "#                 'learning_rate': hp.choice('learning_rate', [0.001]),\n",
    "#                 'lr_decay': hp.choice('lr_decay', [0.5]),\n",
    "#                 'n_lr_decays': hp.choice('n_lr_decays', [3]), \n",
    "#                 'weight_decay': hp.choice('weight_decay', [0]), \n",
    "#                 'max_epochs': hp.choice('max_epochs', [10]),\n",
    "#                 'max_steps': hp.choice('max_steps', [None]),\n",
    "#                 'early_stop_patience': hp.choice('early_stop_patience', [20]),\n",
    "#                 'eval_freq': hp.choice('eval_freq', [50]),\n",
    "#                 'loss_train': hp.choice('loss', ['MAE']),\n",
    "#                 'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "#                 'loss_valid': hp.choice('loss_valid', ['MAE']),\n",
    "#                 # Data parameters\n",
    "#                 'n_time_in': hp.choice('n_time_in', [96]),\n",
    "#                 'n_time_out': hp.choice('n_time_out', [24]),\n",
    "#                 'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "#                 'normalizer_x': hp.choice('normalizer_x', [None]),\n",
    "#                 'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [1]),\n",
    "#                 'batch_size': hp.choice('batch_size', [32]),\n",
    "#                 'random_seed': hp.choice('random_seed', [1])}\n",
    "\n",
    "# trials = hyperopt_tunning(space=autoformer_space, hyperopt_max_evals=2, loss_function_val=mae,\n",
    "#             loss_functions_test={'mae': mae, 'rmse': rmse},\n",
    "#             S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "#             ds_in_val=11520, ds_in_test=11520, return_forecasts=True, save_progress=False, results_file=None, loss_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
