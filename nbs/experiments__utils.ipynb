{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils\n",
    "> Set of functions to easily perform experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENV_VARS = dict(OMP_NUM_THREADS='2',\n",
    "                OPENBLAS_NUM_THREADS='2',\n",
    "                MKL_NUM_THREADS='3',\n",
    "                VECLIB_MAXIMUM_THREADS='2',\n",
    "                NUMEXPR_NUM_THREADS='3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ.update(ENV_VARS)\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch as t\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "from nixtlats.data.scalers import Scaler\n",
    "from nixtlats.data.tsdataset import TimeSeriesDataset, WindowsDataset\n",
    "from nixtlats.data.tsloader import TimeSeriesLoader\n",
    "from nixtlats.models.esrnn.esrnn import ESRNN\n",
    "from nixtlats.models.esrnn.mqesrnn import MQESRNN\n",
    "from nixtlats.models.nbeats.nbeats import NBEATS\n",
    "from nixtlats.models.deepmidas.deepmidas import DeepMIDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mask_dfs(Y_df, ds_in_val, ds_in_test):\n",
    "    # train mask\n",
    "    train_mask_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    train_mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    train_mask_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_mask_df['sample_mask'] = 1\n",
    "    train_mask_df['available_mask'] = 1\n",
    "    \n",
    "    idx_out = train_mask_df.groupby('unique_id').tail(ds_in_val+ds_in_test).index\n",
    "    train_mask_df.loc[idx_out, 'sample_mask'] = 0\n",
    "    \n",
    "    # test mask\n",
    "    test_mask_df = train_mask_df.copy()\n",
    "    test_mask_df['sample_mask'] = 0\n",
    "    idx_test = test_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    test_mask_df.loc[idx_test, 'sample_mask'] = 1\n",
    "    \n",
    "    # validation mask\n",
    "    val_mask_df = train_mask_df.copy()\n",
    "    val_mask_df['sample_mask'] = 1\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - train_mask_df['sample_mask']\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - test_mask_df['sample_mask']\n",
    "\n",
    "    assert len(train_mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(train_mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_random_mask_dfs(Y_df, ds_in_test, \n",
    "                        n_val_windows, n_ds_val_window,\n",
    "                        n_uids, freq):\n",
    "    \"\"\"\n",
    "    Generates train, test and random validation mask.\n",
    "    Train mask begins by avoiding ds_in_test\n",
    "    \n",
    "    Validation mask: 1) samples n_uids unique ids\n",
    "                     2) creates windows of size n_ds_val_window\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    n_uids: int\n",
    "        Number of unique ids in validation.\n",
    "    n_val_windows: int\n",
    "        Number of windows for validation.\n",
    "    n_ds_val_window: int\n",
    "        Number of ds in each validation window.\n",
    "    periods: int  \n",
    "        ds_in_test multiplier.\n",
    "    freq: str\n",
    "        string that determines datestamp frequency, used in\n",
    "        random windows creation.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    #----------------------- Train mask -----------------------#\n",
    "    # Initialize masks\n",
    "    train_mask_df, val_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                            ds_in_val=0,\n",
    "                                                            ds_in_test=ds_in_test)\n",
    "    \n",
    "    assert val_mask_df['sample_mask'].sum()==0, 'Muerte'\n",
    "    \n",
    "    #----------------- Random Validation mask -----------------#\n",
    "    # Overwrite validation with random windows\n",
    "    uids = train_mask_df['unique_id'].unique()\n",
    "    val_uids = np.random.choice(uids, n_uids, replace=False)\n",
    "    \n",
    "    # Validation avoids test\n",
    "    idx_test = train_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    available_ds = train_mask_df.loc[~train_mask_df.index.isin(idx_test)]['ds'].unique()\n",
    "    val_init_ds = np.random.choice(available_ds, n_val_windows, replace=False)\n",
    "    \n",
    "    # Creates windows \n",
    "    val_ds = [pd.date_range(init, periods=n_ds_val_window, freq=freq) for init in val_init_ds]\n",
    "    val_ds = np.concatenate(val_ds)\n",
    "\n",
    "    # Cleans random windows from train mask\n",
    "    val_idx = train_mask_df.query('unique_id in @val_uids & ds in @val_ds').index\n",
    "    train_mask_df.loc[val_idx, 'sample_mask'] = 0\n",
    "    val_mask_df.loc[val_idx, 'sample_mask'] = 1\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def scale_data(Y_df, X_df, mask_df, normalizer_y, normalizer_x):\n",
    "    mask = mask_df['available_mask'].values * mask_df['sample_mask'].values\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "\n",
    "    if normalizer_x is not None:\n",
    "        X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        for col in X_cols:\n",
    "            scaler_x = Scaler(normalizer=normalizer_x)\n",
    "            X_df[col] = scaler_x.scale(x=X_df[col].values, mask=mask)\n",
    "\n",
    "    return Y_df, X_df, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_datasets(mc, S_df, Y_df, X_df, f_cols,\n",
    "                    ds_in_test, ds_in_val):\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    train_mask_df, valid_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                              ds_in_val=ds_in_val,\n",
    "                                                              ds_in_test=ds_in_test)\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "    Y_df, X_df, scaler_y = scale_data(Y_df=Y_df, X_df=X_df, mask_df=train_mask_df,\n",
    "                                      normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "\n",
    "    #----------------------------------------- Declare Dataset and Loaders ----------------------------------#\n",
    "    \n",
    "    if mc['mode'] == 'simple':\n",
    "        train_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       complete_windows=mc['complete_windows'],\n",
    "                                       verbose=True)\n",
    "        \n",
    "        valid_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                       complete_windows=True,\n",
    "                                       verbose=True)\n",
    "        \n",
    "        test_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                      mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                      input_size=int(mc['n_time_in']),\n",
    "                                      output_size=int(mc['n_time_out']),\n",
    "                                      sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                      complete_windows=True,\n",
    "                                      verbose=True)\n",
    "    \n",
    "    if mc['mode'] == 'full':\n",
    "        train_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          verbose=True)\n",
    "        \n",
    "        valid_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          verbose=True)\n",
    "        \n",
    "        test_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                         mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                         input_size=int(mc['n_time_in']),\n",
    "                                         output_size=int(mc['n_time_out']),\n",
    "                                         verbose=True)        \n",
    "    \n",
    "    if ds_in_test == 0:\n",
    "        test_dataset = None\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_loaders(mc, train_dataset, val_dataset, test_dataset):\n",
    "    train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                    batch_size=int(mc['batch_size']),\n",
    "                                    eq_batch_size=mc.get('eq_batch_size') or True,\n",
    "                                    shuffle=True)\n",
    "    if val_dataset is not None:\n",
    "        val_loader = TimeSeriesLoader(dataset=val_dataset,\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False)\n",
    "    else:\n",
    "        val_loader = None\n",
    "\n",
    "    if test_dataset is not None:\n",
    "        test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                                       batch_size=1,\n",
    "                                       shuffle=False)\n",
    "    else:\n",
    "        test_loader = None\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nbeats(mc):\n",
    "    mc['n_theta_hidden'] = len(mc['stack_types']) * [ [int(mc['n_hidden']), int(mc['n_hidden'])] ]\n",
    "    model = NBEATS(n_time_in=int(mc['n_time_in']),\n",
    "                   n_time_out=int(mc['n_time_out']),\n",
    "                   n_x=mc['n_x'],\n",
    "                   n_s=mc['n_s'],\n",
    "                   n_s_hidden=int(mc['n_s_hidden']),\n",
    "                   n_x_hidden=int(mc['n_x_hidden']),\n",
    "                   shared_weights=mc['shared_weights'],\n",
    "                   initialization=mc['initialization'],\n",
    "                   activation=mc['activation'],\n",
    "                   stack_types=mc['stack_types'],\n",
    "                   n_blocks=mc['n_blocks'],\n",
    "                   n_layers=mc['n_layers'],\n",
    "                   n_theta_hidden=mc['n_theta_hidden'],\n",
    "                   n_harmonics=int(mc['n_harmonics']),\n",
    "                   n_polynomials=int(mc['n_polynomials']),\n",
    "                   batch_normalization = mc['batch_normalization'],\n",
    "                   dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                   learning_rate=float(mc['learning_rate']),\n",
    "                   lr_decay=float(mc['lr_decay']),\n",
    "                   lr_decay_step_size=float(mc['lr_decay_step_size']),\n",
    "                   weight_decay=mc['weight_decay'],\n",
    "                   loss_train=mc['loss_train'],\n",
    "                   loss_hypar=float(mc['loss_hypar']),\n",
    "                   loss_valid=mc['loss_valid'],\n",
    "                   frequency=mc['frequency'],\n",
    "                   seasonality=int(mc['seasonality']),\n",
    "                   random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_esrnn(mc):    \n",
    "    model = ESRNN(# Architecture parameters\n",
    "                  n_series=mc['n_series'],\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  sample_freq=int(mc['sample_freq']),\n",
    "                  input_size=int(mc['n_time_in']),\n",
    "                  output_size=int(mc['n_time_out']),\n",
    "                  es_component=mc['es_component'],\n",
    "                  cell_type=mc['cell_type'],\n",
    "                  state_hsize=int(mc['state_hsize']),\n",
    "                  dilations=mc['dilations'],\n",
    "                  add_nl_layer=mc['add_nl_layer'],\n",
    "                  # Optimization parameters                \n",
    "                  learning_rate=mc['learning_rate'],\n",
    "                  lr_scheduler_step_size=int(mc['lr_decay_step_size']),\n",
    "                  lr_decay=mc['lr_decay'],\n",
    "                  per_series_lr_multip=mc['per_series_lr_multip'],\n",
    "                  gradient_eps=mc['gradient_eps'],\n",
    "                  gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                  rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                  noise_std=mc['noise_std'],\n",
    "                  level_variability_penalty=mc['level_variability_penalty'],\n",
    "                  testing_percentile=mc['testing_percentile'],\n",
    "                  training_percentile=mc['training_percentile'],\n",
    "                  loss=mc['loss_train'],\n",
    "                  val_loss=mc['loss_valid'],\n",
    "                  seasonality=mc['seasonality']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_mqesrnn(mc):    \n",
    "    model = MQESRNN(# Architecture parameters\n",
    "                    n_series=mc['n_series'],\n",
    "                    n_x=mc['n_x'],\n",
    "                    n_s=mc['n_s'],\n",
    "                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                    input_size=int(mc['n_time_in']),\n",
    "                    output_size=int(mc['n_time_out']),\n",
    "                    es_component=mc['es_component'],\n",
    "                    cell_type=mc['cell_type'],\n",
    "                    state_hsize=int(mc['state_hsize']),\n",
    "                    dilations=mc['dilations'],\n",
    "                    add_nl_layer=mc['add_nl_layer'],\n",
    "                    # Optimization parameters                 \n",
    "                    learning_rate=mc['learning_rate'],\n",
    "                    lr_scheduler_step_size=int(mc['lr_decay_step_size']),\n",
    "                    lr_decay=mc['lr_decay'],\n",
    "                    gradient_eps=mc['gradient_eps'],\n",
    "                    gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                    rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                    noise_std=mc['noise_std'],\n",
    "                    testing_percentiles=list(mc['testing_percentiles']),\n",
    "                    training_percentiles=list(mc['training_percentiles']),\n",
    "                    loss=mc['loss_train'],\n",
    "                    val_loss=mc['loss_valid']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_deepmidas(mc):\n",
    "    mc['n_theta_hidden'] = len(mc['stack_types']) * [ [int(mc['n_hidden']), int(mc['n_hidden'])] ]\n",
    "\n",
    "    model = DeepMIDAS(n_time_in=int(mc['n_time_in']),\n",
    "                      n_time_out=int(mc['n_time_out']),\n",
    "                      n_x=mc['n_x'],\n",
    "                      n_s=mc['n_s'],\n",
    "                      n_s_hidden=int(mc['n_s_hidden']),\n",
    "                      n_x_hidden=int(mc['n_x_hidden']),\n",
    "                      shared_weights = mc['shared_weights'],\n",
    "                      initialization=mc['initialization'],\n",
    "                      activation=mc['activation'],\n",
    "                      stack_types=mc['stack_types'],\n",
    "                      n_blocks=mc['n_blocks'],\n",
    "                      n_layers=mc['n_layers'],\n",
    "                      n_theta_hidden=mc['n_theta_hidden'],\n",
    "                      n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "                      n_freq_downsample=mc['n_freq_downsample'],\n",
    "                      batch_normalization = mc['batch_normalization'],\n",
    "                      dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                      learning_rate=float(mc['learning_rate']),\n",
    "                      lr_decay=float(mc['lr_decay']),\n",
    "                      lr_decay_step_size=float(mc['lr_decay_step_size']),\n",
    "                      weight_decay=mc['weight_decay'],\n",
    "                      loss_train=mc['loss_train'],\n",
    "                      loss_hypar=float(mc['loss_hypar']),\n",
    "                      loss_valid=mc['loss_valid'],\n",
    "                      frequency=mc['frequency'],\n",
    "                      seasonality=int(mc['seasonality']),\n",
    "                      random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_model(mc):\n",
    "    MODEL_DICT = {'nbeats': instantiate_nbeats,\n",
    "                  'esrnn': instantiate_esrnn,\n",
    "                  'mqesrnn': instantiate_mqesrnn,\n",
    "                  'deepmidas': instantiate_deepmidas}\n",
    "    return MODEL_DICT[mc['model']](mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def predict(mc, model, trainer, loader, scaler_y):\n",
    "   outputs = trainer.predict(model, loader)\n",
    "   y_true, y_hat, mask = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "   meta_data = loader.dataset.meta_data\n",
    "\n",
    "   # Scale to original scale\n",
    "   if mc['normalizer_y'] is not None:\n",
    "        y_true_shape = y_true.shape\n",
    "        y_true = scaler_y.inv_scale(x=y_true.flatten())\n",
    "        y_true = np.reshape(y_true, y_true_shape)\n",
    "\n",
    "        y_hat = scaler_y.inv_scale(x=y_hat.flatten())\n",
    "        y_hat = np.reshape(y_hat, y_true_shape)\n",
    "\n",
    "   return y_true, y_hat, mask, meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_fit_predict(mc, S_df, Y_df, X_df, f_cols, ds_in_val, ds_in_test):\n",
    "    \n",
    "    # Protect inplace modifications\n",
    "    Y_df = Y_df.copy()\n",
    "    if X_df is not None:\n",
    "        X_df = X_df.copy()\n",
    "    if S_df is not None:\n",
    "        S_df = S_df.copy()        \n",
    "\n",
    "    #----------------------------------------------- Datasets -----------------------------------------------#\n",
    "    train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc,\n",
    "                                                                         S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                                                         f_cols=f_cols,\n",
    "                                                                         ds_in_val=ds_in_val,\n",
    "                                                                         ds_in_test=ds_in_test)\n",
    "    mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()\n",
    "\n",
    "    #------------------------------------------- Instantiate & fit -------------------------------------------#\n",
    "    train_loader, val_loader, test_loader = instantiate_loaders(mc=mc,\n",
    "                                                                train_dataset=train_dataset,\n",
    "                                                                val_dataset=val_dataset,\n",
    "                                                                test_dataset=test_dataset)\n",
    "    model = instantiate_model(mc=mc)\n",
    "    callbacks = []\n",
    "    if mc['early_stop_patience']:\n",
    "        early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, \n",
    "                                                    patience=mc['early_stop_patience'],\n",
    "                                                    verbose=True, \n",
    "                                                    mode='min') \n",
    "        callbacks=[early_stopping]\n",
    "\n",
    "    gpus = -1 if t.cuda.is_available() else 0\n",
    "    trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                         max_steps=mc['max_steps'],\n",
    "                         check_val_every_n_epoch=mc['eval_freq'],\n",
    "                         progress_bar_refresh_rate=1,\n",
    "                         gpus=gpus,\n",
    "                         callbacks=callbacks)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    #------------------------------------------------ Predict ------------------------------------------------#\n",
    "    results = {}\n",
    "\n",
    "    if ds_in_val > 0:\n",
    "        y_true, y_hat, mask, meta_data = predict(mc, model, trainer, val_loader, scaler_y)\n",
    "        val_values = (('val_y_true', y_true), ('val_y_hat', y_hat), ('val_mask', mask), ('val_meta_data', meta_data))\n",
    "        results.update(val_values)\n",
    "\n",
    "        print(f\"VAL y_true.shape: {y_true.shape}\")\n",
    "        print(f\"VAL y_hat.shape: {y_hat.shape}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Predict test if available\n",
    "    if ds_in_test > 0:\n",
    "        y_true, y_hat, mask, meta_data = predict(mc, model, trainer, test_loader, scaler_y)\n",
    "        test_values = (('test_y_true', y_true), ('test_y_hat', y_hat), ('test_mask', mask), ('test_meta_data', meta_data))\n",
    "        results.update(test_values)\n",
    "\n",
    "        print(f\"TEST y_true.shape: {y_true.shape}\")\n",
    "        print(f\"TEST y_hat.shape: {y_hat.shape}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_model(mc, loss_function_val, loss_functions_test, \n",
    "                   S_df, Y_df, X_df, f_cols,\n",
    "                   ds_in_val, ds_in_test,\n",
    "                   return_forecasts,\n",
    "                   loss_kwargs):\n",
    "    \n",
    "    print(47*'=' + '\\n')\n",
    "    print(pd.Series(mc))\n",
    "    print(47*'=' + '\\n')\n",
    "    \n",
    "    # Some asserts due to work in progress\n",
    "    n_series = Y_df['unique_id'].nunique()\n",
    "    if n_series > 1:\n",
    "        assert mc['normalizer_y'] is None, 'Data scaling not implemented with multiple time series'\n",
    "        assert mc['normalizer_x'] is None, 'Data scaling not implemented with multiple time series'\n",
    "\n",
    "    assert ds_in_test % mc['val_idx_to_sample_freq']==0, 'outsample size should be multiple of val_idx_to_sample_freq'\n",
    "\n",
    "    # Make predictions\n",
    "    start = time.time()\n",
    "    results = model_fit_predict(mc=mc,\n",
    "                                S_df=S_df, \n",
    "                                Y_df=Y_df,\n",
    "                                X_df=X_df,\n",
    "                                f_cols=f_cols,\n",
    "                                ds_in_val=ds_in_val,\n",
    "                                ds_in_test=ds_in_test)\n",
    "    run_time = time.time() - start\n",
    "\n",
    "    # Evaluate predictions\n",
    "    val_loss = loss_function_val(y=results['val_y_true'], y_hat=results['val_y_hat'], weights=results['val_mask'], **loss_kwargs)\n",
    "\n",
    "    results_output = {'loss': val_loss,\n",
    "                      'mc': mc,\n",
    "                      'run_time': run_time,\n",
    "                      'status': STATUS_OK}\n",
    "\n",
    "    # Evaluation in test (if provided)\n",
    "    if ds_in_test > 0:\n",
    "        test_loss_dict = {}\n",
    "        for loss_name, loss_function in loss_functions_test.items():\n",
    "            test_loss_dict[loss_name] = loss_function(y=results['test_y_true'], y_hat=results['test_y_hat'], weights=results['test_mask'])\n",
    "        results_output['test_losses'] = test_loss_dict\n",
    "\n",
    "    if return_forecasts:\n",
    "        results_output['run_forecasts'] = results\n",
    "\n",
    "    return results_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hyperopt_tunning(space, hyperopt_max_evals, loss_function_val, loss_functions_test,\n",
    "                     S_df, Y_df, X_df, f_cols,\n",
    "                     ds_in_val, ds_in_test,\n",
    "                     return_forecasts,\n",
    "                     loss_kwargs=None):\n",
    "    assert ds_in_val > 0, 'Validation set is needed for tunning!'\n",
    "\n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(evaluate_model, loss_function_val=loss_function_val, loss_functions_test=loss_functions_test,\n",
    "                             S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=f_cols,\n",
    "                             ds_in_val=ds_in_val, ds_in_test=ds_in_test,\n",
    "                             return_forecasts=return_forecasts,\n",
    "                             loss_kwargs=loss_kwargs or {})\n",
    "\n",
    "    fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=hyperopt_max_evals, trials=trials, verbose=True)\n",
    "\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from nixtlats.losses.numpy import mae, mape, smape, rmse, pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if t.cuda.is_available(): device = 'cuda'  \n",
    "\n",
    "deepmidas_space= {# Architecture parameters\n",
    "               'model':'deepmidas',\n",
    "               'mode': 'simple',\n",
    "               'n_time_in': hp.choice('n_time_in', [7*24]),\n",
    "               'n_time_out': hp.choice('n_time_out', [24]),\n",
    "               'n_x_hidden': hp.quniform('n_x_hidden', 1, 10, 1),\n",
    "               'n_s_hidden': hp.choice('n_s_hidden', [0]),\n",
    "               'shared_weights': hp.choice('shared_weights', [False]),\n",
    "               'activation': hp.choice('activation', ['SELU']),\n",
    "               'initialization':  hp.choice('initialization', ['glorot_normal','he_normal']),\n",
    "               'stack_types': hp.choice('stack_types', [2*['identity']]),\n",
    "               'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "               'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "               'n_hidden': hp.choice('n_hidden', [ 256 ]),\n",
    "               'n_pool_kernel_size': hp.choice('n_pool_kernel_size', [ [ 4, 1 ] ]),\n",
    "               'n_freq_downsample': hp.choice('n_freq_downsample', [ [ 24, 1 ] ]),\n",
    "               # Regularization and optimization parameters\n",
    "               'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "               'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 0.5),\n",
    "               'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "               'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.001)),\n",
    "               'lr_decay': hp.uniform('lr_decay', 0.3, 0.5),\n",
    "               'lr_decay_step_size': hp.choice('lr_decay_step_size', [100]), \n",
    "               'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "               'max_epochs': hp.choice('max_epochs', [10]), #'n_iterations': hp.choice('n_iterations', [10])\n",
    "               'max_steps': hp.choice('max_steps', [None]),\n",
    "               'early_stop_patience': hp.choice('early_stop_patience', [16]),\n",
    "               'eval_freq': hp.choice('eval_freq', [50]),\n",
    "               'loss_train': hp.choice('loss', ['MAE']),\n",
    "               'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "               'loss_valid': hp.choice('loss_valid', ['MAE']), #[args.val_loss]),\n",
    "               'l1_theta': hp.choice('l1_theta', [0]),\n",
    "               # Data parameters\n",
    "               'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "               'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "               'complete_windows': hp.choice('complete_windows', [False]),\n",
    "               'frequency': hp.choice('frequency', ['H']),\n",
    "               'seasonality': hp.choice('seasonality', [24]),      \n",
    "               'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "               'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [24]),\n",
    "               'batch_size': hp.choice('batch_size', [256]),\n",
    "               'random_seed': hp.quniform('random_seed', 10, 20, 1),\n",
    "               'device': hp.choice('device', [device])}\n",
    "\n",
    "mc = {'model':'deepmidas',\n",
    "      # Architecture parameters\n",
    "      'n_time_in': 7*24,\n",
    "      'n_time_out': 24,\n",
    "      'n_x_hidden': 3,\n",
    "      'n_s_hidden': 0,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'SELU',\n",
    "      'initialization': 'he_normal',\n",
    "      'stack_types': ['identity', 'identity'],\n",
    "      'n_blocks': [1, 1],\n",
    "      'n_layers': [2, 2],\n",
    "      'n_pool_kernel_size': [4, 1],\n",
    "      'n_freq_downsample': [24, 1],\n",
    "      'n_hidden': 364,\n",
    "      # Regularization and optimization parameters\n",
    "      'max_epochs': 10, #'n_iterations': 100,\n",
    "      'max_steps': None,      \n",
    "      'early_stop_patience': 8,\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.2,\n",
    "      'learning_rate': 0.0005, #0.002,\n",
    "      'lr_decay': 0.64,\n",
    "      'lr_decay_step_size': 100,\n",
    "      'weight_decay': 0.00015,\n",
    "      'eval_freq': 50,\n",
    "      'loss_train': 'PINBALL',\n",
    "      'loss_hypar': 0.5, #0.49,\n",
    "      'loss_valid': 'MAE',\n",
    "      'l1_theta': 0,\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'window_sampling_limit': 100_000,\n",
    "      'complete_windows': False,\n",
    "      'frequency':'H',\n",
    "      'seasonality': 24,\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'val_idx_to_sample_freq': 24,\n",
    "      'batch_size': 256,\n",
    "      'n_series_per_batch': 1,\n",
    "      'random_seed': 10,\n",
    "      'device': 'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nixtlats.data.datasets.epf import EPF, EPFInfo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = ['NP']\n",
    "\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "\n",
    "X_df = X_df[['unique_id', 'ds', 'week_day']]\n",
    "Y_min = Y_df.y.min()\n",
    "#Y_df.y = Y_df.y - Y_min + 20\n",
    "\n",
    "plt.plot(Y_df.y.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: think about use of big windows during train for extremely long series\n",
    "# backpropagation trough time is slow\n",
    "# result = evaluate_model(loss_function=mae, mc=mc, \n",
    "#                         S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "#                         ds_in_test=0, ds_in_val=728*24,\n",
    "#                         n_uids=None, n_val_windows=None, freq=None,\n",
    "#                         is_val_random=False, loss_kwargs={})\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(result['y_hat'].flatten())\n",
    "# plt.plot(Y_df['y'][-728*24:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.016587 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "\n",
      "activation                                SELU\n",
      "batch_normalization                      False\n",
      "batch_size                                 256\n",
      "complete_windows                         False\n",
      "device                                     cpu\n",
      "dropout_prob_exogenous                0.382591\n",
      "dropout_prob_theta                    0.297799\n",
      "early_stop_patience                         16\n",
      "eval_freq                                   50\n",
      "frequency                                    H\n",
      "idx_to_sample_freq                          24\n",
      "initialization                       he_normal\n",
      "l1_theta                                     0\n",
      "learning_rate                         0.000787\n",
      "loss_hypar                                 0.5\n",
      "loss_train                                 MAE\n",
      "loss_valid                                 MAE\n",
      "lr_decay                              0.482621\n",
      "lr_decay_step_size                         100\n",
      "max_epochs                                  10\n",
      "max_steps                                 None\n",
      "mode                                    simple\n",
      "model                                deepmidas\n",
      "n_blocks                                (1, 1)\n",
      "n_freq_downsample                      (24, 1)\n",
      "n_hidden                                   256\n",
      "n_layers                                (2, 2)\n",
      "n_pool_kernel_size                      (4, 1)\n",
      "n_s_hidden                                   0\n",
      "n_time_in                                  168\n",
      "n_time_out                                  24\n",
      "n_x_hidden                                 5.0\n",
      "normalizer_x                            median\n",
      "normalizer_y                              None\n",
      "random_seed                               19.0\n",
      "seasonality                                 24\n",
      "shared_weights                           False\n",
      "stack_types               (identity, identity)\n",
      "val_idx_to_sample_freq                      24\n",
      "weight_decay                          0.002182\n",
      "dtype: object\n",
      "===============================================\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | _DeepMIDAS | 376 K \n",
      "-------------------------------------\n",
      "376 K     Trainable params\n",
      "0         Non-trainable params\n",
      "376 K     Total params\n",
      "1.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:323: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: -1it [00:00, ?it/s]\n",
      "Training:   0%|          | 0/1 [00:00<00:00, 17924.38it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<00:00, 246.68it/s]   \n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459064158/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00, 12.83it/s] \n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00, 12.51it/s, loss=4.38, v_num=74, train_loss_step=4.380]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<00:00, 17848.10it/s, loss=4.38, v_num=74, train_loss_step=4.380]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<00:00, 245.76it/s, loss=4.38, v_num=74, train_loss_step=4.380]  \n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00, 13.61it/s, loss=4.38, v_num=74, train_loss_step=4.380] \n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00, 13.11it/s, loss=5.88, v_num=74, train_loss_step=7.390, train_loss_epoch=4.380]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<00:00, 15363.75it/s, loss=5.88, v_num=74, train_loss_step=7.390, train_loss_epoch=4.380]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<00:00, 289.86it/s, loss=5.88, v_num=74, train_loss_step=7.390, train_loss_epoch=4.380]  \n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00, 13.23it/s, loss=5.88, v_num=74, train_loss_step=7.390, train_loss_epoch=4.380] \n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00, 12.63it/s, loss=5.21, v_num=74, train_loss_step=3.860, train_loss_epoch=7.390]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<00:00, 15650.39it/s, loss=5.21, v_num=74, train_loss_step=3.860, train_loss_epoch=7.390]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00, 118.10it/s, loss=5.21, v_num=74, train_loss_step=3.860, train_loss_epoch=7.390]  \n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00, 11.53it/s, loss=5.21, v_num=74, train_loss_step=3.860, train_loss_epoch=7.390] \n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00, 11.28it/s, loss=5.21, v_num=74, train_loss_step=5.230, train_loss_epoch=3.860]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00, 16710.37it/s, loss=5.21, v_num=74, train_loss_step=5.230, train_loss_epoch=3.860]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<00:00, 209.34it/s, loss=5.21, v_num=74, train_loss_step=5.230, train_loss_epoch=3.860]  \n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00, 13.56it/s, loss=5.21, v_num=74, train_loss_step=5.230, train_loss_epoch=3.860] \n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00, 13.17it/s, loss=5.15, v_num=74, train_loss_step=4.870, train_loss_epoch=5.230]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<00:00, 18157.16it/s, loss=5.15, v_num=74, train_loss_step=4.870, train_loss_epoch=5.230]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00, 252.84it/s, loss=5.15, v_num=74, train_loss_step=4.870, train_loss_epoch=5.230]  \n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00, 14.39it/s, loss=5.15, v_num=74, train_loss_step=4.870, train_loss_epoch=5.230] \n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00, 14.02it/s, loss=4.85, v_num=74, train_loss_step=3.350, train_loss_epoch=4.870]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00, 18157.16it/s, loss=4.85, v_num=74, train_loss_step=3.350, train_loss_epoch=4.870]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00, 300.02it/s, loss=4.85, v_num=74, train_loss_step=3.350, train_loss_epoch=4.870]  \n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00, 14.26it/s, loss=4.85, v_num=74, train_loss_step=3.350, train_loss_epoch=4.870] \n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00, 13.86it/s, loss=4.61, v_num=74, train_loss_step=3.210, train_loss_epoch=3.350]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00, 17848.10it/s, loss=4.61, v_num=74, train_loss_step=3.210, train_loss_epoch=3.350]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<00:00, 249.81it/s, loss=4.61, v_num=74, train_loss_step=3.210, train_loss_epoch=3.350]  \n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00, 14.38it/s, loss=4.61, v_num=74, train_loss_step=3.210, train_loss_epoch=3.350] \n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00, 13.98it/s, loss=4.51, v_num=74, train_loss_step=3.800, train_loss_epoch=3.210]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<00:00, 17476.27it/s, loss=4.51, v_num=74, train_loss_step=3.800, train_loss_epoch=3.210]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00, 273.07it/s, loss=4.51, v_num=74, train_loss_step=3.800, train_loss_epoch=3.210]  \n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00, 14.65it/s, loss=4.51, v_num=74, train_loss_step=3.800, train_loss_epoch=3.210] \n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00, 14.24it/s, loss=4.42, v_num=74, train_loss_step=3.690, train_loss_epoch=3.800]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00, 19239.93it/s, loss=4.42, v_num=74, train_loss_step=3.690, train_loss_epoch=3.800]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<00:00, 309.86it/s, loss=4.42, v_num=74, train_loss_step=3.690, train_loss_epoch=3.800]  \n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 14.34it/s, loss=4.42, v_num=74, train_loss_step=3.690, train_loss_epoch=3.800] \n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 13.95it/s, loss=4.29, v_num=74, train_loss_step=3.090, train_loss_epoch=3.690]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 13.44it/s, loss=4.29, v_num=74, train_loss_step=3.090, train_loss_epoch=3.690]\n",
      "  0%|          | 0/2 [00:03<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 1it [00:00, ?it/s]\n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "VAL y_true.shape: (7, 24)\n",
      "VAL y_hat.shape: (7, 24)\n",
      "Predicting: 1it [00:00, ?it/s]\n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "TEST y_true.shape: (7, 24)\n",
      "TEST y_hat.shape: (7, 24)\n",
      " 50%|█████     | 1/2 [00:03<00:03,  3.19s/trial, best loss: 6.4836297035217285]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.014987 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 6.483630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "\n",
      "activation                                SELU\n",
      "batch_normalization                      False\n",
      "batch_size                                 256\n",
      "complete_windows                         False\n",
      "device                                     cpu\n",
      "dropout_prob_exogenous                0.038663\n",
      "dropout_prob_theta                    0.330628\n",
      "early_stop_patience                         16\n",
      "eval_freq                                   50\n",
      "frequency                                    H\n",
      "idx_to_sample_freq                          24\n",
      "initialization                       he_normal\n",
      "l1_theta                                     0\n",
      "learning_rate                         0.000977\n",
      "loss_hypar                                 0.5\n",
      "loss_train                                 MAE\n",
      "loss_valid                                 MAE\n",
      "lr_decay                              0.354789\n",
      "lr_decay_step_size                         100\n",
      "max_epochs                                  10\n",
      "max_steps                                 None\n",
      "mode                                    simple\n",
      "model                                deepmidas\n",
      "n_blocks                                (1, 1)\n",
      "n_freq_downsample                      (24, 1)\n",
      "n_hidden                                   256\n",
      "n_layers                                (2, 2)\n",
      "n_pool_kernel_size                      (4, 1)\n",
      "n_s_hidden                                   0\n",
      "n_time_in                                  168\n",
      "n_time_out                                  24\n",
      "n_x_hidden                                 5.0\n",
      "normalizer_x                            median\n",
      "normalizer_y                              None\n",
      "random_seed                               14.0\n",
      "seasonality                                 24\n",
      "shared_weights                           False\n",
      "stack_types               (identity, identity)\n",
      "val_idx_to_sample_freq                      24\n",
      "weight_decay                           0.00253\n",
      "dtype: object\n",
      "===============================================\n",
      "\n",
      " 50%|█████     | 1/2 [00:03<00:03,  3.19s/trial, best loss: 6.4836297035217285]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | _DeepMIDAS | 376 K \n",
      "-------------------------------------\n",
      "376 K     Trainable params\n",
      "0         Non-trainable params\n",
      "376 K     Total params\n",
      "1.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:04<00:03,  3.19s/trial, best loss: 6.4836297035217285]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:323: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: -1it [00:00, ?it/s]\n",
      "Training:   0%|          | 0/1 [00:00<00:00, 17549.39it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<00:00, 253.36it/s]   \n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00, 14.83it/s] \n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00, 14.46it/s, loss=4.34, v_num=75, train_loss_step=4.340]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<00:00, 16980.99it/s, loss=4.34, v_num=75, train_loss_step=4.340]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<00:00, 245.34it/s, loss=4.34, v_num=75, train_loss_step=4.340]  \n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00, 14.74it/s, loss=4.34, v_num=75, train_loss_step=4.340] \n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00, 14.34it/s, loss=5.19, v_num=75, train_loss_step=6.030, train_loss_epoch=4.340]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<00:00, 15887.52it/s, loss=5.19, v_num=75, train_loss_step=6.030, train_loss_epoch=4.340]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<00:00, 246.13it/s, loss=5.19, v_num=75, train_loss_step=6.030, train_loss_epoch=4.340]  \n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00, 14.36it/s, loss=5.19, v_num=75, train_loss_step=6.030, train_loss_epoch=4.340] \n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00, 13.90it/s, loss=4.73, v_num=75, train_loss_step=3.800, train_loss_epoch=6.030]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<00:00, 16131.94it/s, loss=4.73, v_num=75, train_loss_step=3.800, train_loss_epoch=6.030]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00, 258.86it/s, loss=4.73, v_num=75, train_loss_step=3.800, train_loss_epoch=6.030]  \n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00, 14.42it/s, loss=4.73, v_num=75, train_loss_step=3.800, train_loss_epoch=6.030] \n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00, 13.92it/s, loss=4.64, v_num=75, train_loss_step=4.400, train_loss_epoch=3.800]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00, 10538.45it/s, loss=4.64, v_num=75, train_loss_step=4.400, train_loss_epoch=3.800]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<00:00, 249.56it/s, loss=4.64, v_num=75, train_loss_step=4.400, train_loss_epoch=3.800]  \n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00, 13.84it/s, loss=4.64, v_num=75, train_loss_step=4.400, train_loss_epoch=3.800] \n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00, 13.35it/s, loss=4.37, v_num=75, train_loss_step=3.280, train_loss_epoch=4.400]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<00:00, 16448.25it/s, loss=4.37, v_num=75, train_loss_step=3.280, train_loss_epoch=4.400]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00, 252.21it/s, loss=4.37, v_num=75, train_loss_step=3.280, train_loss_epoch=4.400]  \n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00, 14.46it/s, loss=4.37, v_num=75, train_loss_step=3.280, train_loss_epoch=4.400] \n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00, 14.04it/s, loss=4.15, v_num=75, train_loss_step=3.070, train_loss_epoch=3.280]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00, 18315.74it/s, loss=4.15, v_num=75, train_loss_step=3.070, train_loss_epoch=3.280]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00, 295.77it/s, loss=4.15, v_num=75, train_loss_step=3.070, train_loss_epoch=3.280]  \n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00, 14.91it/s, loss=4.15, v_num=75, train_loss_step=3.070, train_loss_epoch=3.280] \n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00, 14.49it/s, loss=4.05, v_num=75, train_loss_step=3.400, train_loss_epoch=3.070]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00, 17848.10it/s, loss=4.05, v_num=75, train_loss_step=3.400, train_loss_epoch=3.070]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<00:00, 258.67it/s, loss=4.05, v_num=75, train_loss_step=3.400, train_loss_epoch=3.070]  \n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00, 15.21it/s, loss=4.05, v_num=75, train_loss_step=3.400, train_loss_epoch=3.070] \n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00, 14.78it/s, loss=3.93, v_num=75, train_loss_step=3.100, train_loss_epoch=3.400]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<00:00, 20460.02it/s, loss=3.93, v_num=75, train_loss_step=3.100, train_loss_epoch=3.400]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00, 310.18it/s, loss=3.93, v_num=75, train_loss_step=3.100, train_loss_epoch=3.400]  \n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00, 15.16it/s, loss=3.93, v_num=75, train_loss_step=3.100, train_loss_epoch=3.400] \n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00, 14.73it/s, loss=3.8, v_num=75, train_loss_step=2.740, train_loss_epoch=3.100] \n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00, 18893.26it/s, loss=3.8, v_num=75, train_loss_step=2.740, train_loss_epoch=3.100]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<00:00, 272.78it/s, loss=3.8, v_num=75, train_loss_step=2.740, train_loss_epoch=3.100]  \n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 15.00it/s, loss=3.8, v_num=75, train_loss_step=2.740, train_loss_epoch=3.100] \n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 14.57it/s, loss=3.7, v_num=75, train_loss_step=2.800, train_loss_epoch=2.740]\n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 13.96it/s, loss=3.7, v_num=75, train_loss_step=2.800, train_loss_epoch=2.740]\n",
      " 50%|█████     | 1/2 [00:06<00:03,  3.19s/trial, best loss: 6.4836297035217285]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 1it [00:00, ?it/s]\n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "VAL y_true.shape: (7, 24)\n",
      "VAL y_hat.shape: (7, 24)\n",
      "Predicting: 1it [00:00, ?it/s]\n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "TEST y_true.shape: (7, 24)\n",
      "TEST y_hat.shape: (7, 24)\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.06s/trial, best loss: 5.814513683319092]\n"
     ]
    }
   ],
   "source": [
    "trials = hyperopt_tunning(space=deepmidas_space, hyperopt_max_evals=2, loss_function_val=mae,\n",
    "                          loss_functions_test={'mae': mae, 'rmse': rmse},\n",
    "                          S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "                          ds_in_val=7*24, ds_in_test=7*24, return_forecasts=True, loss_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 2,\n",
       "  'tid': 0,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 6.4836297035217285,\n",
       "   'mc': {'activation': 'SELU',\n",
       "    'batch_normalization': False,\n",
       "    'batch_size': 256,\n",
       "    'complete_windows': False,\n",
       "    'device': 'cpu',\n",
       "    'dropout_prob_exogenous': 0.38259054148933763,\n",
       "    'dropout_prob_theta': 0.29779920353083345,\n",
       "    'early_stop_patience': 16,\n",
       "    'eval_freq': 50,\n",
       "    'frequency': 'H',\n",
       "    'idx_to_sample_freq': 24,\n",
       "    'initialization': 'he_normal',\n",
       "    'l1_theta': 0,\n",
       "    'learning_rate': 0.0007869636178342339,\n",
       "    'loss_hypar': 0.5,\n",
       "    'loss_train': 'MAE',\n",
       "    'loss_valid': 'MAE',\n",
       "    'lr_decay': 0.4826214429016166,\n",
       "    'lr_decay_step_size': 100,\n",
       "    'max_epochs': 10,\n",
       "    'max_steps': None,\n",
       "    'mode': 'simple',\n",
       "    'model': 'deepmidas',\n",
       "    'n_blocks': (1, 1),\n",
       "    'n_freq_downsample': (24, 1),\n",
       "    'n_hidden': 256,\n",
       "    'n_layers': (2, 2),\n",
       "    'n_pool_kernel_size': (4, 1),\n",
       "    'n_s_hidden': 0,\n",
       "    'n_time_in': 168,\n",
       "    'n_time_out': 24,\n",
       "    'n_x_hidden': 5.0,\n",
       "    'normalizer_x': 'median',\n",
       "    'normalizer_y': None,\n",
       "    'random_seed': 19.0,\n",
       "    'seasonality': 24,\n",
       "    'shared_weights': False,\n",
       "    'stack_types': ('identity', 'identity'),\n",
       "    'val_idx_to_sample_freq': 24,\n",
       "    'weight_decay': 0.0021822157334433897,\n",
       "    'n_x': 1,\n",
       "    'n_s': 1,\n",
       "    'n_theta_hidden': [[256, 256], [256, 256]]},\n",
       "   'run_time': 3.1427860260009766,\n",
       "   'status': 'ok',\n",
       "   'test_losses': {'mae': 3.0701606, 'rmse': 4.2011194},\n",
       "   'run_forecasts': {'val_y_true': array([[44.62, 43.71, 43.34, 43.4 , 44.07, 46.47, 48.84, 51.65, 53.01,\n",
       "            53.32, 53.23, 53.65, 53.99, 54.35, 56.09, 59.04, 61.75, 62.31,\n",
       "            58.07, 53.76, 53.19, 51.55, 49.8 , 48.58],\n",
       "           [48.79, 48.08, 47.81, 47.8 , 48.34, 51.  , 55.2 , 69.53, 75.93,\n",
       "            73.66, 69.44, 67.42, 66.89, 65.02, 67.18, 74.15, 78.2 , 78.19,\n",
       "            74.15, 64.04, 53.57, 53.57, 50.91, 49.21],\n",
       "           [49.57, 48.77, 48.16, 47.86, 48.24, 50.09, 54.84, 67.43, 72.23,\n",
       "            69.97, 61.79, 58.27, 63.83, 62.6 , 63.84, 72.35, 68.05, 61.68,\n",
       "            59.  , 54.79, 52.97, 52.67, 50.74, 49.65],\n",
       "           [49.48, 48.64, 48.11, 48.45, 49.32, 52.41, 57.65, 69.61, 73.23,\n",
       "            70.83, 63.09, 61.81, 64.07, 60.76, 61.8 , 63.9 , 74.91, 71.79,\n",
       "            62.88, 56.33, 55.04, 54.43, 53.12, 51.36],\n",
       "           [53.1 , 51.56, 50.92, 51.06, 50.5 , 50.78, 51.07, 52.22, 53.07,\n",
       "            53.96, 54.72, 54.97, 53.97, 53.92, 53.97, 53.88, 53.63, 55.41,\n",
       "            52.88, 51.42, 50.3 , 49.4 , 48.08, 46.47],\n",
       "           [46.95, 46.87, 46.52, 46.35, 46.31, 46.38, 46.78, 47.27, 48.19,\n",
       "            48.67, 49.09, 50.55, 51.52, 51.45, 51.71, 52.49, 54.38, 54.76,\n",
       "            54.5 , 52.94, 52.13, 51.65, 50.67, 49.86],\n",
       "           [50.41, 49.94, 49.77, 49.05, 49.47, 52.03, 57.19, 70.21, 77.37,\n",
       "            77.82, 76.64, 77.36, 76.62, 76.62, 77.14, 77.33, 77.37, 77.34,\n",
       "            76.67, 69.5 , 56.52, 55.14, 53.56, 52.49]], dtype=float32),\n",
       "    'val_y_hat': array([[46.292877, 45.809456, 44.517246, 44.370335, 44.50484 , 43.95786 ,\n",
       "            47.437855, 48.96695 , 50.157528, 50.310585, 49.65537 , 49.460632,\n",
       "            48.75617 , 49.280853, 48.784573, 47.216614, 47.39984 , 47.79566 ,\n",
       "            47.962116, 48.53254 , 47.99333 , 47.821285, 46.651405, 46.43819 ],\n",
       "           [50.440487, 50.260197, 49.134773, 48.598385, 48.898857, 48.484795,\n",
       "            52.23737 , 54.065594, 54.86892 , 55.379642, 54.42297 , 54.08336 ,\n",
       "            53.193195, 53.766724, 53.130222, 51.34432 , 51.993595, 52.18864 ,\n",
       "            52.70723 , 53.208275, 52.42636 , 52.213566, 51.175724, 50.679455],\n",
       "           [50.974346, 50.970272, 49.79125 , 49.337364, 49.157063, 48.961983,\n",
       "            52.901802, 54.979004, 55.369064, 56.165234, 55.03769 , 54.652985,\n",
       "            53.984936, 54.512253, 53.702564, 51.958748, 52.778736, 52.96066 ,\n",
       "            53.570637, 53.954475, 53.138256, 53.17415 , 51.72167 , 50.91551 ],\n",
       "           [50.683777, 50.31587 , 49.12396 , 49.76565 , 48.963345, 48.702938,\n",
       "            52.89323 , 54.48886 , 54.860912, 55.845413, 54.88936 , 54.4816  ,\n",
       "            54.067265, 54.247734, 53.614212, 51.887356, 53.00425 , 52.376297,\n",
       "            53.3945  , 53.904327, 52.95192 , 52.867123, 51.41592 , 50.351025],\n",
       "           [51.93758 , 51.493614, 49.90081 , 50.76373 , 49.616825, 49.58987 ,\n",
       "            53.66174 , 55.469315, 55.831394, 56.480568, 55.066776, 55.23211 ,\n",
       "            55.061962, 55.262196, 54.20887 , 53.0508  , 53.76503 , 53.397038,\n",
       "            54.300217, 54.658775, 54.499187, 53.97856 , 52.336758, 51.343536],\n",
       "           [47.215237, 46.41577 , 44.76737 , 45.85603 , 44.57414 , 44.46398 ,\n",
       "            47.955822, 49.587273, 50.53635 , 50.885983, 49.653023, 50.191547,\n",
       "            49.50689 , 49.6438  , 48.965588, 47.86803 , 48.427864, 48.14459 ,\n",
       "            49.062435, 49.202477, 49.04812 , 48.576622, 47.454437, 46.68124 ],\n",
       "           [51.736168, 50.648476, 49.34126 , 49.978382, 49.409992, 49.192078,\n",
       "            52.607677, 54.02534 , 55.32432 , 55.464016, 54.750244, 54.595398,\n",
       "            54.235455, 54.616695, 53.696846, 52.69579 , 52.87418 , 52.74857 ,\n",
       "            53.373398, 54.224236, 53.96314 , 53.260937, 52.11252 , 51.259876]],\n",
       "          dtype=float32),\n",
       "    'val_mask': array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),\n",
       "    'val_meta_data': [array([['NP', Timestamp('2013-01-01 00:00:00')],\n",
       "            ['NP', Timestamp('2013-01-01 01:00:00')],\n",
       "            ['NP', Timestamp('2013-01-01 02:00:00')],\n",
       "            ...,\n",
       "            ['NP', Timestamp('2018-12-24 21:00:00')],\n",
       "            ['NP', Timestamp('2018-12-24 22:00:00')],\n",
       "            ['NP', Timestamp('2018-12-24 23:00:00')]], dtype=object)],\n",
       "    'test_y_true': array([[52.33, 51.02, 50.24, 49.42, 50.18, 52.89, 56.46, 71.25, 74.69,\n",
       "            72.06, 69.72, 67.63, 65.92, 66.08, 68.05, 69.94, 67.68, 65.96,\n",
       "            57.86, 55.12, 53.54, 52.55, 50.67, 48.69],\n",
       "           [49.55, 48.84, 48.18, 47.82, 48.65, 50.6 , 52.14, 54.25, 55.69,\n",
       "            55.27, 55.03, 55.32, 55.71, 55.68, 55.74, 56.31, 57.19, 56.71,\n",
       "            56.27, 55.2 , 54.34, 53.22, 51.67, 50.12],\n",
       "           [48.35, 48.08, 48.31, 47.36, 47.94, 50.62, 54.38, 57.84, 58.63,\n",
       "            57.53, 56.45, 56.31, 57.39, 57.21, 57.52, 57.81, 57.88, 57.2 ,\n",
       "            55.7 , 54.31, 52.25, 51.29, 50.  , 48.12],\n",
       "           [47.21, 46.42, 46.  , 46.12, 46.49, 49.97, 52.73, 55.65, 57.04,\n",
       "            57.13, 56.98, 56.04, 56.22, 55.78, 54.6 , 54.11, 54.25, 54.69,\n",
       "            54.21, 52.79, 52.05, 51.43, 50.07, 49.01],\n",
       "           [48.39, 47.72, 47.23, 46.6 , 46.94, 47.76, 48.41, 49.44, 50.29,\n",
       "            51.52, 52.56, 53.08, 52.99, 53.13, 52.93, 53.78, 55.04, 56.52,\n",
       "            57.07, 55.52, 53.05, 52.05, 51.09, 50.47],\n",
       "           [51.49, 50.83, 50.74, 50.14, 49.94, 50.46, 50.88, 51.37, 51.61,\n",
       "            52.22, 52.8 , 53.  , 53.11, 52.93, 52.93, 53.75, 55.99, 61.2 ,\n",
       "            61.2 , 57.42, 55.61, 53.99, 53.86, 52.32],\n",
       "           [51.09, 50.19, 48.98, 48.8 , 48.52, 49.8 , 50.05, 50.55, 52.33,\n",
       "            53.26, 53.14, 52.88, 53.03, 52.46, 52.44, 52.89, 53.26, 52.61,\n",
       "            51.28, 50.72, 49.86, 49.09, 49.02, 48.1 ]], dtype=float32),\n",
       "    'test_y_hat': array([[54.498062, 53.963875, 52.802265, 52.54548 , 52.15377 , 51.99901 ,\n",
       "            55.743492, 57.9606  , 59.232117, 59.631542, 58.20206 , 58.25278 ,\n",
       "            57.36585 , 58.005882, 57.43746 , 55.591633, 55.681236, 56.125553,\n",
       "            56.769222, 57.4183  , 56.50161 , 56.275078, 55.19024 , 54.540394],\n",
       "           [50.862854, 50.3661  , 49.39258 , 49.562874, 49.03361 , 48.658096,\n",
       "            53.060505, 55.17837 , 55.581375, 56.56272 , 55.078358, 54.998516,\n",
       "            54.113384, 54.728745, 53.7     , 51.929245, 52.924973, 52.88182 ,\n",
       "            54.00698 , 54.29088 , 52.869873, 52.88745 , 51.202774, 50.812386],\n",
       "           [51.96343 , 51.367363, 49.80986 , 49.819862, 50.003284, 50.413227,\n",
       "            53.962017, 55.782265, 56.514538, 57.234936, 55.979042, 55.355526,\n",
       "            55.53573 , 55.225372, 54.054558, 52.769016, 53.649128, 53.59691 ,\n",
       "            54.22144 , 55.16018 , 54.564102, 53.703056, 52.561863, 51.865383],\n",
       "           [49.691067, 49.23978 , 47.59798 , 47.6978  , 47.803078, 47.479904,\n",
       "            50.972656, 52.77952 , 53.50872 , 54.193493, 53.290943, 53.034416,\n",
       "            52.554222, 52.46737 , 51.86604 , 50.215977, 50.703365, 50.79298 ,\n",
       "            51.52339 , 51.868298, 51.70623 , 51.244915, 50.349445, 49.71837 ],\n",
       "           [50.495064, 49.38451 , 48.17412 , 49.10283 , 47.935467, 47.367485,\n",
       "            51.08173 , 52.490147, 53.32482 , 53.658295, 53.298595, 53.297104,\n",
       "            52.468   , 53.47331 , 52.279343, 51.477146, 51.4318  , 51.476368,\n",
       "            52.275925, 52.656643, 52.18878 , 51.708557, 50.33871 , 49.870968],\n",
       "           [52.010605, 50.818928, 49.771732, 49.661545, 49.520054, 49.27632 ,\n",
       "            52.725395, 53.97213 , 55.721924, 55.381157, 54.643047, 54.42325 ,\n",
       "            54.4162  , 54.5066  , 53.74236 , 52.22853 , 52.63466 , 53.269672,\n",
       "            53.530384, 53.728905, 53.27113 , 53.058437, 52.207157, 51.77833 ],\n",
       "           [54.379074, 54.12967 , 52.76119 , 52.215668, 52.651344, 52.32973 ,\n",
       "            55.953728, 57.862873, 58.567924, 59.3005  , 58.502987, 57.572243,\n",
       "            57.500202, 57.523357, 56.760666, 54.950794, 55.456852, 56.172855,\n",
       "            56.621586, 56.810905, 56.332882, 56.23965 , 55.064064, 54.95643 ]],\n",
       "          dtype=float32),\n",
       "    'test_mask': array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),\n",
       "    'test_meta_data': [array([['NP', Timestamp('2013-01-01 00:00:00')],\n",
       "            ['NP', Timestamp('2013-01-01 01:00:00')],\n",
       "            ['NP', Timestamp('2013-01-01 02:00:00')],\n",
       "            ...,\n",
       "            ['NP', Timestamp('2018-12-24 21:00:00')],\n",
       "            ['NP', Timestamp('2018-12-24 22:00:00')],\n",
       "            ['NP', Timestamp('2018-12-24 23:00:00')]], dtype=object)]}},\n",
       "  'misc': {'tid': 0,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_windows': [0],\n",
       "    'device': [0],\n",
       "    'dropout_prob_exogenous': [0],\n",
       "    'dropout_prob_theta': [0],\n",
       "    'early_stop_patience': [0],\n",
       "    'eval_freq': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [0],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'loss_valid': [0],\n",
       "    'lr_decay': [0],\n",
       "    'lr_decay_step_size': [0],\n",
       "    'max_epochs': [0],\n",
       "    'max_steps': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_freq_downsample': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_pool_kernel_size': [0],\n",
       "    'n_s_hidden': [0],\n",
       "    'n_time_in': [0],\n",
       "    'n_time_out': [0],\n",
       "    'n_x_hidden': [0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'random_seed': [0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [0],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'weight_decay': [0]},\n",
       "   'vals': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_windows': [0],\n",
       "    'device': [0],\n",
       "    'dropout_prob_exogenous': [0.38259054148933763],\n",
       "    'dropout_prob_theta': [0.29779920353083345],\n",
       "    'early_stop_patience': [0],\n",
       "    'eval_freq': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [1],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0.0007869636178342339],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'loss_valid': [0],\n",
       "    'lr_decay': [0.4826214429016166],\n",
       "    'lr_decay_step_size': [0],\n",
       "    'max_epochs': [0],\n",
       "    'max_steps': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_freq_downsample': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_pool_kernel_size': [0],\n",
       "    'n_s_hidden': [0],\n",
       "    'n_time_in': [0],\n",
       "    'n_time_out': [0],\n",
       "    'n_x_hidden': [5.0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'random_seed': [19.0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [0],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'weight_decay': [0.0021822157334433897]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 11, 10, 21, 54, 26, 404000),\n",
       "  'refresh_time': datetime.datetime(2021, 11, 10, 21, 54, 29, 569000)},\n",
       " {'state': 2,\n",
       "  'tid': 1,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 5.814513683319092,\n",
       "   'mc': {'activation': 'SELU',\n",
       "    'batch_normalization': False,\n",
       "    'batch_size': 256,\n",
       "    'complete_windows': False,\n",
       "    'device': 'cpu',\n",
       "    'dropout_prob_exogenous': 0.038662987633889645,\n",
       "    'dropout_prob_theta': 0.3306282220179785,\n",
       "    'early_stop_patience': 16,\n",
       "    'eval_freq': 50,\n",
       "    'frequency': 'H',\n",
       "    'idx_to_sample_freq': 24,\n",
       "    'initialization': 'he_normal',\n",
       "    'l1_theta': 0,\n",
       "    'learning_rate': 0.0009772468946913485,\n",
       "    'loss_hypar': 0.5,\n",
       "    'loss_train': 'MAE',\n",
       "    'loss_valid': 'MAE',\n",
       "    'lr_decay': 0.35478934936311995,\n",
       "    'lr_decay_step_size': 100,\n",
       "    'max_epochs': 10,\n",
       "    'max_steps': None,\n",
       "    'mode': 'simple',\n",
       "    'model': 'deepmidas',\n",
       "    'n_blocks': (1, 1),\n",
       "    'n_freq_downsample': (24, 1),\n",
       "    'n_hidden': 256,\n",
       "    'n_layers': (2, 2),\n",
       "    'n_pool_kernel_size': (4, 1),\n",
       "    'n_s_hidden': 0,\n",
       "    'n_time_in': 168,\n",
       "    'n_time_out': 24,\n",
       "    'n_x_hidden': 5.0,\n",
       "    'normalizer_x': 'median',\n",
       "    'normalizer_y': None,\n",
       "    'random_seed': 14.0,\n",
       "    'seasonality': 24,\n",
       "    'shared_weights': False,\n",
       "    'stack_types': ('identity', 'identity'),\n",
       "    'val_idx_to_sample_freq': 24,\n",
       "    'weight_decay': 0.0025304703924177697,\n",
       "    'n_x': 1,\n",
       "    'n_s': 1,\n",
       "    'n_theta_hidden': [[256, 256], [256, 256]]},\n",
       "   'run_time': 2.897136926651001,\n",
       "   'status': 'ok',\n",
       "   'test_losses': {'mae': 3.173148, 'rmse': 4.189379},\n",
       "   'run_forecasts': {'val_y_true': array([[44.62, 43.71, 43.34, 43.4 , 44.07, 46.47, 48.84, 51.65, 53.01,\n",
       "            53.32, 53.23, 53.65, 53.99, 54.35, 56.09, 59.04, 61.75, 62.31,\n",
       "            58.07, 53.76, 53.19, 51.55, 49.8 , 48.58],\n",
       "           [48.79, 48.08, 47.81, 47.8 , 48.34, 51.  , 55.2 , 69.53, 75.93,\n",
       "            73.66, 69.44, 67.42, 66.89, 65.02, 67.18, 74.15, 78.2 , 78.19,\n",
       "            74.15, 64.04, 53.57, 53.57, 50.91, 49.21],\n",
       "           [49.57, 48.77, 48.16, 47.86, 48.24, 50.09, 54.84, 67.43, 72.23,\n",
       "            69.97, 61.79, 58.27, 63.83, 62.6 , 63.84, 72.35, 68.05, 61.68,\n",
       "            59.  , 54.79, 52.97, 52.67, 50.74, 49.65],\n",
       "           [49.48, 48.64, 48.11, 48.45, 49.32, 52.41, 57.65, 69.61, 73.23,\n",
       "            70.83, 63.09, 61.81, 64.07, 60.76, 61.8 , 63.9 , 74.91, 71.79,\n",
       "            62.88, 56.33, 55.04, 54.43, 53.12, 51.36],\n",
       "           [53.1 , 51.56, 50.92, 51.06, 50.5 , 50.78, 51.07, 52.22, 53.07,\n",
       "            53.96, 54.72, 54.97, 53.97, 53.92, 53.97, 53.88, 53.63, 55.41,\n",
       "            52.88, 51.42, 50.3 , 49.4 , 48.08, 46.47],\n",
       "           [46.95, 46.87, 46.52, 46.35, 46.31, 46.38, 46.78, 47.27, 48.19,\n",
       "            48.67, 49.09, 50.55, 51.52, 51.45, 51.71, 52.49, 54.38, 54.76,\n",
       "            54.5 , 52.94, 52.13, 51.65, 50.67, 49.86],\n",
       "           [50.41, 49.94, 49.77, 49.05, 49.47, 52.03, 57.19, 70.21, 77.37,\n",
       "            77.82, 76.64, 77.36, 76.62, 76.62, 77.14, 77.33, 77.37, 77.34,\n",
       "            76.67, 69.5 , 56.52, 55.14, 53.56, 52.49]], dtype=float32),\n",
       "    'val_y_hat': array([[45.74769 , 44.760956, 44.796494, 44.397514, 45.23768 , 47.046993,\n",
       "            47.83739 , 50.29268 , 52.17981 , 50.844467, 51.067627, 52.030533,\n",
       "            50.77355 , 50.45314 , 50.085148, 49.041206, 49.043247, 49.99334 ,\n",
       "            49.677807, 50.52954 , 49.99665 , 49.18119 , 48.409782, 47.04438 ],\n",
       "           [49.899834, 49.26437 , 49.14279 , 48.89493 , 49.678703, 51.36877 ,\n",
       "            52.065098, 54.570004, 56.34672 , 55.1538  , 55.71962 , 56.35843 ,\n",
       "            55.08295 , 54.885838, 54.348522, 53.30954 , 53.98083 , 54.820663,\n",
       "            54.135162, 54.835876, 54.547203, 53.429928, 52.965748, 50.94576 ],\n",
       "           [50.70067 , 49.92371 , 50.190205, 49.58713 , 50.736313, 52.48966 ,\n",
       "            52.87592 , 55.384995, 57.037556, 55.72293 , 57.030155, 57.554317,\n",
       "            56.04649 , 56.097183, 54.770557, 54.075798, 55.117657, 55.802895,\n",
       "            54.91976 , 55.454586, 55.41101 , 54.016323, 53.792656, 51.357338],\n",
       "           [50.459915, 49.265476, 50.22779 , 49.232834, 50.14183 , 51.824017,\n",
       "            52.921135, 54.76953 , 56.344784, 55.84156 , 56.035397, 56.95729 ,\n",
       "            55.94393 , 55.897068, 54.179165, 53.796925, 53.840725, 55.542286,\n",
       "            55.022938, 55.12386 , 55.045876, 53.549614, 53.559914, 51.301563],\n",
       "           [51.646317, 50.517616, 51.461178, 50.24439 , 51.21954 , 53.211967,\n",
       "            54.12253 , 56.09706 , 57.580807, 56.923725, 57.49653 , 58.36562 ,\n",
       "            56.93299 , 57.08332 , 55.336224, 54.997158, 55.113583, 56.692135,\n",
       "            56.26584 , 56.29948 , 56.44805 , 54.52821 , 54.562572, 53.28968 ],\n",
       "           [47.08339 , 45.632484, 46.88327 , 45.640522, 46.314354, 48.253452,\n",
       "            49.50792 , 51.54476 , 53.49311 , 52.71562 , 52.520996, 53.81446 ,\n",
       "            52.955296, 52.353065, 50.966427, 50.86141 , 49.67104 , 52.37226 ,\n",
       "            51.725277, 52.25246 , 51.649185, 50.55403 , 49.817226, 49.017536],\n",
       "           [51.011753, 49.533607, 50.209347, 49.565258, 50.025246, 52.252743,\n",
       "            53.55553 , 55.649746, 58.015163, 56.38468 , 56.681625, 57.93226 ,\n",
       "            56.458298, 56.113953, 55.072937, 54.64654 , 53.929634, 55.546585,\n",
       "            55.29352 , 56.028786, 55.787086, 54.394142, 53.741703, 53.210423]],\n",
       "          dtype=float32),\n",
       "    'val_mask': array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),\n",
       "    'val_meta_data': [array([['NP', Timestamp('2013-01-01 00:00:00')],\n",
       "            ['NP', Timestamp('2013-01-01 01:00:00')],\n",
       "            ['NP', Timestamp('2013-01-01 02:00:00')],\n",
       "            ...,\n",
       "            ['NP', Timestamp('2018-12-24 21:00:00')],\n",
       "            ['NP', Timestamp('2018-12-24 22:00:00')],\n",
       "            ['NP', Timestamp('2018-12-24 23:00:00')]], dtype=object)],\n",
       "    'test_y_true': array([[52.33, 51.02, 50.24, 49.42, 50.18, 52.89, 56.46, 71.25, 74.69,\n",
       "            72.06, 69.72, 67.63, 65.92, 66.08, 68.05, 69.94, 67.68, 65.96,\n",
       "            57.86, 55.12, 53.54, 52.55, 50.67, 48.69],\n",
       "           [49.55, 48.84, 48.18, 47.82, 48.65, 50.6 , 52.14, 54.25, 55.69,\n",
       "            55.27, 55.03, 55.32, 55.71, 55.68, 55.74, 56.31, 57.19, 56.71,\n",
       "            56.27, 55.2 , 54.34, 53.22, 51.67, 50.12],\n",
       "           [48.35, 48.08, 48.31, 47.36, 47.94, 50.62, 54.38, 57.84, 58.63,\n",
       "            57.53, 56.45, 56.31, 57.39, 57.21, 57.52, 57.81, 57.88, 57.2 ,\n",
       "            55.7 , 54.31, 52.25, 51.29, 50.  , 48.12],\n",
       "           [47.21, 46.42, 46.  , 46.12, 46.49, 49.97, 52.73, 55.65, 57.04,\n",
       "            57.13, 56.98, 56.04, 56.22, 55.78, 54.6 , 54.11, 54.25, 54.69,\n",
       "            54.21, 52.79, 52.05, 51.43, 50.07, 49.01],\n",
       "           [48.39, 47.72, 47.23, 46.6 , 46.94, 47.76, 48.41, 49.44, 50.29,\n",
       "            51.52, 52.56, 53.08, 52.99, 53.13, 52.93, 53.78, 55.04, 56.52,\n",
       "            57.07, 55.52, 53.05, 52.05, 51.09, 50.47],\n",
       "           [51.49, 50.83, 50.74, 50.14, 49.94, 50.46, 50.88, 51.37, 51.61,\n",
       "            52.22, 52.8 , 53.  , 53.11, 52.93, 52.93, 53.75, 55.99, 61.2 ,\n",
       "            61.2 , 57.42, 55.61, 53.99, 53.86, 52.32],\n",
       "           [51.09, 50.19, 48.98, 48.8 , 48.52, 49.8 , 50.05, 50.55, 52.33,\n",
       "            53.26, 53.14, 52.88, 53.03, 52.46, 52.44, 52.89, 53.26, 52.61,\n",
       "            51.28, 50.72, 49.86, 49.09, 49.02, 48.1 ]], dtype=float32),\n",
       "    'test_y_hat': array([[53.99761 , 53.02463 , 53.51192 , 52.956966, 54.32635 , 56.24719 ,\n",
       "            56.018456, 59.42499 , 61.56162 , 59.342964, 60.712185, 61.617817,\n",
       "            60.059174, 59.745686, 59.202442, 58.584583, 58.26781 , 59.35592 ,\n",
       "            58.81104 , 59.56843 , 58.897778, 58.060154, 57.07128 , 55.242332],\n",
       "           [50.96232 , 49.524246, 50.500183, 50.113056, 50.642483, 52.203354,\n",
       "            52.938206, 56.107143, 58.23969 , 56.847443, 57.611645, 57.936752,\n",
       "            57.355354, 56.325043, 55.455635, 54.521515, 55.194885, 56.723072,\n",
       "            55.668983, 56.77734 , 55.914192, 55.0504  , 54.78996 , 51.807396],\n",
       "           [51.631435, 50.42228 , 51.030346, 50.039455, 50.95452 , 53.15246 ,\n",
       "            53.76111 , 57.055195, 59.256474, 57.196053, 58.70356 , 59.002388,\n",
       "            57.604042, 57.40393 , 56.22746 , 55.20829 , 56.300236, 57.36135 ,\n",
       "            56.597218, 57.06897 , 57.4838  , 55.369358, 55.260044, 53.477722],\n",
       "           [48.891033, 47.61571 , 48.686813, 47.449276, 48.573143, 50.367897,\n",
       "            50.810207, 53.79646 , 56.276848, 54.367363, 54.94339 , 55.76675 ,\n",
       "            54.914047, 54.130802, 53.06208 , 53.298363, 51.990814, 54.28035 ,\n",
       "            53.5298  , 54.225155, 54.02257 , 52.883522, 51.77271 , 51.009296],\n",
       "           [49.90067 , 48.45827 , 49.16942 , 48.649017, 49.152058, 51.091034,\n",
       "            52.17648 , 53.824196, 55.581413, 54.792843, 54.718605, 55.78093 ,\n",
       "            54.8675  , 54.10603 , 53.28904 , 53.06069 , 52.090042, 54.344387,\n",
       "            54.13526 , 54.30397 , 53.594254, 52.99896 , 52.246624, 51.542576],\n",
       "           [51.475822, 50.43576 , 50.21133 , 49.89577 , 50.854206, 52.168777,\n",
       "            53.2521  , 55.919384, 57.99317 , 56.20019 , 56.78317 , 57.386   ,\n",
       "            56.473145, 55.678753, 55.82368 , 54.16653 , 55.032578, 56.114403,\n",
       "            55.948055, 56.253597, 55.60811 , 54.724197, 54.463177, 52.69032 ],\n",
       "           [53.96929 , 53.066574, 53.16184 , 52.674007, 53.55371 , 55.097466,\n",
       "            55.596516, 58.94163 , 61.610516, 59.085884, 60.16968 , 60.587677,\n",
       "            59.565495, 58.99388 , 58.487476, 57.687775, 58.105194, 59.08411 ,\n",
       "            58.395306, 59.3259  , 59.009483, 57.926838, 57.22373 , 55.27079 ]],\n",
       "          dtype=float32),\n",
       "    'test_mask': array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),\n",
       "    'test_meta_data': [array([['NP', Timestamp('2013-01-01 00:00:00')],\n",
       "            ['NP', Timestamp('2013-01-01 01:00:00')],\n",
       "            ['NP', Timestamp('2013-01-01 02:00:00')],\n",
       "            ...,\n",
       "            ['NP', Timestamp('2018-12-24 21:00:00')],\n",
       "            ['NP', Timestamp('2018-12-24 22:00:00')],\n",
       "            ['NP', Timestamp('2018-12-24 23:00:00')]], dtype=object)]}},\n",
       "  'misc': {'tid': 1,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'activation': [1],\n",
       "    'batch_normalization': [1],\n",
       "    'batch_size': [1],\n",
       "    'complete_windows': [1],\n",
       "    'device': [1],\n",
       "    'dropout_prob_exogenous': [1],\n",
       "    'dropout_prob_theta': [1],\n",
       "    'early_stop_patience': [1],\n",
       "    'eval_freq': [1],\n",
       "    'frequency': [1],\n",
       "    'idx_to_sample_freq': [1],\n",
       "    'initialization': [1],\n",
       "    'l1_theta': [1],\n",
       "    'learning_rate': [1],\n",
       "    'loss': [1],\n",
       "    'loss_hypar': [1],\n",
       "    'loss_valid': [1],\n",
       "    'lr_decay': [1],\n",
       "    'lr_decay_step_size': [1],\n",
       "    'max_epochs': [1],\n",
       "    'max_steps': [1],\n",
       "    'n_blocks': [1],\n",
       "    'n_freq_downsample': [1],\n",
       "    'n_hidden': [1],\n",
       "    'n_layers': [1],\n",
       "    'n_pool_kernel_size': [1],\n",
       "    'n_s_hidden': [1],\n",
       "    'n_time_in': [1],\n",
       "    'n_time_out': [1],\n",
       "    'n_x_hidden': [1],\n",
       "    'normalizer_x': [1],\n",
       "    'normalizer_y': [1],\n",
       "    'random_seed': [1],\n",
       "    'seasonality': [1],\n",
       "    'shared_weights': [1],\n",
       "    'stack_types': [1],\n",
       "    'val_idx_to_sample_freq': [1],\n",
       "    'weight_decay': [1]},\n",
       "   'vals': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_windows': [0],\n",
       "    'device': [0],\n",
       "    'dropout_prob_exogenous': [0.038662987633889645],\n",
       "    'dropout_prob_theta': [0.3306282220179785],\n",
       "    'early_stop_patience': [0],\n",
       "    'eval_freq': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [1],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0.0009772468946913485],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'loss_valid': [0],\n",
       "    'lr_decay': [0.35478934936311995],\n",
       "    'lr_decay_step_size': [0],\n",
       "    'max_epochs': [0],\n",
       "    'max_steps': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_freq_downsample': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_pool_kernel_size': [0],\n",
       "    'n_s_hidden': [0],\n",
       "    'n_time_in': [0],\n",
       "    'n_time_out': [0],\n",
       "    'n_x_hidden': [5.0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'random_seed': [14.0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [0],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'weight_decay': [0.0025304703924177697]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 11, 10, 21, 54, 29, 592000),\n",
       "  'refresh_time': datetime.datetime(2021, 11, 10, 21, 54, 32, 505000)}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 3.0701606, 'rmse': 4.2011194}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.trials[0]['result']['test_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('nixtla': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
