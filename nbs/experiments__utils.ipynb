{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils\n",
    "> Set of functions to easily perform experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENV_VARS = dict(OMP_NUM_THREADS='2',\n",
    "                OPENBLAS_NUM_THREADS='2',\n",
    "                MKL_NUM_THREADS='3',\n",
    "                VECLIB_MAXIMUM_THREADS='2',\n",
    "                NUMEXPR_NUM_THREADS='3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import pickle\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ.update(ENV_VARS)\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch as t\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "from nixtlats.data.scalers import Scaler\n",
    "from nixtlats.data.tsdataset import TimeSeriesDataset, WindowsDataset\n",
    "from nixtlats.data.tsloader import TimeSeriesLoader\n",
    "from nixtlats.models.esrnn.esrnn import ESRNN\n",
    "from nixtlats.models.esrnn.mqesrnn import MQESRNN\n",
    "from nixtlats.models.nbeats.nbeats import NBEATS\n",
    "from nixtlats.models.deepmidas.deepmidas import DeepMIDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mask_dfs(Y_df, ds_in_val, ds_in_test):\n",
    "    # train mask\n",
    "    train_mask_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    train_mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    train_mask_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_mask_df['sample_mask'] = 1\n",
    "    train_mask_df['available_mask'] = 1\n",
    "    \n",
    "    idx_out = train_mask_df.groupby('unique_id').tail(ds_in_val+ds_in_test).index\n",
    "    train_mask_df.loc[idx_out, 'sample_mask'] = 0\n",
    "    \n",
    "    # test mask\n",
    "    test_mask_df = train_mask_df.copy()\n",
    "    test_mask_df['sample_mask'] = 0\n",
    "    idx_test = test_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    test_mask_df.loc[idx_test, 'sample_mask'] = 1\n",
    "    \n",
    "    # validation mask\n",
    "    val_mask_df = train_mask_df.copy()\n",
    "    val_mask_df['sample_mask'] = 1\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - train_mask_df['sample_mask']\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - test_mask_df['sample_mask']\n",
    "\n",
    "    assert len(train_mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(train_mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_random_mask_dfs(Y_df, ds_in_test, \n",
    "                        n_val_windows, n_ds_val_window,\n",
    "                        n_uids, freq):\n",
    "    \"\"\"\n",
    "    Generates train, test and random validation mask.\n",
    "    Train mask begins by avoiding ds_in_test\n",
    "    \n",
    "    Validation mask: 1) samples n_uids unique ids\n",
    "                     2) creates windows of size n_ds_val_window\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    n_uids: int\n",
    "        Number of unique ids in validation.\n",
    "    n_val_windows: int\n",
    "        Number of windows for validation.\n",
    "    n_ds_val_window: int\n",
    "        Number of ds in each validation window.\n",
    "    periods: int  \n",
    "        ds_in_test multiplier.\n",
    "    freq: str\n",
    "        string that determines datestamp frequency, used in\n",
    "        random windows creation.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    #----------------------- Train mask -----------------------#\n",
    "    # Initialize masks\n",
    "    train_mask_df, val_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                            ds_in_val=0,\n",
    "                                                            ds_in_test=ds_in_test)\n",
    "    \n",
    "    assert val_mask_df['sample_mask'].sum()==0, 'Muerte'\n",
    "    \n",
    "    #----------------- Random Validation mask -----------------#\n",
    "    # Overwrite validation with random windows\n",
    "    uids = train_mask_df['unique_id'].unique()\n",
    "    val_uids = np.random.choice(uids, n_uids, replace=False)\n",
    "    \n",
    "    # Validation avoids test\n",
    "    idx_test = train_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    available_ds = train_mask_df.loc[~train_mask_df.index.isin(idx_test)]['ds'].unique()\n",
    "    val_init_ds = np.random.choice(available_ds, n_val_windows, replace=False)\n",
    "    \n",
    "    # Creates windows \n",
    "    val_ds = [pd.date_range(init, periods=n_ds_val_window, freq=freq) for init in val_init_ds]\n",
    "    val_ds = np.concatenate(val_ds)\n",
    "\n",
    "    # Cleans random windows from train mask\n",
    "    val_idx = train_mask_df.query('unique_id in @val_uids & ds in @val_ds').index\n",
    "    train_mask_df.loc[val_idx, 'sample_mask'] = 0\n",
    "    val_mask_df.loc[val_idx, 'sample_mask'] = 1\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def scale_data(Y_df, X_df, mask_df, normalizer_y, normalizer_x):\n",
    "    mask = mask_df['available_mask'].values * mask_df['sample_mask'].values\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "\n",
    "    if normalizer_x is not None:\n",
    "        X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        for col in X_cols:\n",
    "            scaler_x = Scaler(normalizer=normalizer_x)\n",
    "            X_df[col] = scaler_x.scale(x=X_df[col].values, mask=mask)\n",
    "\n",
    "    return Y_df, X_df, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_datasets(mc, S_df, Y_df, X_df, f_cols,\n",
    "                    ds_in_test, ds_in_val):\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    train_mask_df, valid_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                              ds_in_val=ds_in_val,\n",
    "                                                              ds_in_test=ds_in_test)\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "    Y_df, X_df, scaler_y = scale_data(Y_df=Y_df, X_df=X_df, mask_df=train_mask_df,\n",
    "                                      normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "\n",
    "    #----------------------------------------- Declare Dataset and Loaders ----------------------------------#\n",
    "    \n",
    "    if mc['mode'] == 'simple':\n",
    "        train_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       complete_windows=mc['complete_windows'],\n",
    "                                       verbose=True)\n",
    "        \n",
    "        valid_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                       complete_windows=True,\n",
    "                                       verbose=True)\n",
    "        \n",
    "        test_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                      mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                      input_size=int(mc['n_time_in']),\n",
    "                                      output_size=int(mc['n_time_out']),\n",
    "                                      sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                      complete_windows=True,\n",
    "                                      verbose=True)\n",
    "    \n",
    "    if mc['mode'] == 'full':\n",
    "        train_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          verbose=True)\n",
    "        \n",
    "        valid_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                          mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                          input_size=int(mc['n_time_in']),\n",
    "                                          output_size=int(mc['n_time_out']),\n",
    "                                          verbose=True)\n",
    "        \n",
    "        test_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                         mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                         input_size=int(mc['n_time_in']),\n",
    "                                         output_size=int(mc['n_time_out']),\n",
    "                                         verbose=True)        \n",
    "    \n",
    "    if ds_in_test == 0:\n",
    "        test_dataset = None\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_loaders(mc, train_dataset, val_dataset, test_dataset):\n",
    "    train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                    batch_size=int(mc['batch_size']),\n",
    "                                    eq_batch_size=mc.get('eq_batch_size') or True,\n",
    "                                    shuffle=True)\n",
    "    if val_dataset is not None:\n",
    "        val_loader = TimeSeriesLoader(dataset=val_dataset,\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False)\n",
    "    else:\n",
    "        val_loader = None\n",
    "\n",
    "    if test_dataset is not None:\n",
    "        test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                                       batch_size=1,\n",
    "                                       shuffle=False)\n",
    "    else:\n",
    "        test_loader = None\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nbeats(mc):\n",
    "    mc['n_theta_hidden'] = len(mc['stack_types']) * [ [int(mc['n_hidden']), int(mc['n_hidden'])] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "\n",
    "    model = NBEATS(n_time_in=int(mc['n_time_in']),\n",
    "                   n_time_out=int(mc['n_time_out']),\n",
    "                   n_x=mc['n_x'],\n",
    "                   n_s=mc['n_s'],\n",
    "                   n_s_hidden=int(mc['n_s_hidden']),\n",
    "                   n_x_hidden=int(mc['n_x_hidden']),\n",
    "                   shared_weights=mc['shared_weights'],\n",
    "                   initialization=mc['initialization'],\n",
    "                   activation=mc['activation'],\n",
    "                   stack_types=mc['stack_types'],\n",
    "                   n_blocks=mc['n_blocks'],\n",
    "                   n_layers=mc['n_layers'],\n",
    "                   n_theta_hidden=mc['n_theta_hidden'],\n",
    "                   n_harmonics=int(mc['n_harmonics']),\n",
    "                   n_polynomials=int(mc['n_polynomials']),\n",
    "                   batch_normalization = mc['batch_normalization'],\n",
    "                   dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                   learning_rate=float(mc['learning_rate']),\n",
    "                   lr_decay=float(mc['lr_decay']),\n",
    "                   lr_decay_step_size=lr_decay_step_size,\n",
    "                   weight_decay=mc['weight_decay'],\n",
    "                   loss_train=mc['loss_train'],\n",
    "                   loss_hypar=float(mc['loss_hypar']),\n",
    "                   loss_valid=mc['loss_valid'],\n",
    "                   frequency=mc['frequency'],\n",
    "                   seasonality=int(mc['seasonality']),\n",
    "                   random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_esrnn(mc):    \n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "    model = ESRNN(# Architecture parameters\n",
    "                  n_series=mc['n_series'],\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  sample_freq=int(mc['sample_freq']),\n",
    "                  input_size=int(mc['n_time_in']),\n",
    "                  output_size=int(mc['n_time_out']),\n",
    "                  es_component=mc['es_component'],\n",
    "                  cell_type=mc['cell_type'],\n",
    "                  state_hsize=int(mc['state_hsize']),\n",
    "                  dilations=mc['dilations'],\n",
    "                  add_nl_layer=mc['add_nl_layer'],\n",
    "                  # Optimization parameters                \n",
    "                  learning_rate=mc['learning_rate'],\n",
    "                  lr_scheduler_step_size=lr_decay_step_size,\n",
    "                  lr_decay=mc['lr_decay'],\n",
    "                  per_series_lr_multip=mc['per_series_lr_multip'],\n",
    "                  gradient_eps=mc['gradient_eps'],\n",
    "                  gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                  rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                  noise_std=mc['noise_std'],\n",
    "                  level_variability_penalty=mc['level_variability_penalty'],\n",
    "                  testing_percentile=mc['testing_percentile'],\n",
    "                  training_percentile=mc['training_percentile'],\n",
    "                  loss=mc['loss_train'],\n",
    "                  val_loss=mc['loss_valid'],\n",
    "                  seasonality=mc['seasonality']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_mqesrnn(mc): \n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])   \n",
    "    model = MQESRNN(# Architecture parameters\n",
    "                    n_series=mc['n_series'],\n",
    "                    n_x=mc['n_x'],\n",
    "                    n_s=mc['n_s'],\n",
    "                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                    input_size=int(mc['n_time_in']),\n",
    "                    output_size=int(mc['n_time_out']),\n",
    "                    es_component=mc['es_component'],\n",
    "                    cell_type=mc['cell_type'],\n",
    "                    state_hsize=int(mc['state_hsize']),\n",
    "                    dilations=mc['dilations'],\n",
    "                    add_nl_layer=mc['add_nl_layer'],\n",
    "                    # Optimization parameters                 \n",
    "                    learning_rate=mc['learning_rate'],\n",
    "                    lr_scheduler_step_size=lr_decay_step_size,\n",
    "                    lr_decay=mc['lr_decay'],\n",
    "                    gradient_eps=mc['gradient_eps'],\n",
    "                    gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                    rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                    noise_std=mc['noise_std'],\n",
    "                    testing_percentiles=list(mc['testing_percentiles']),\n",
    "                    training_percentiles=list(mc['training_percentiles']),\n",
    "                    loss=mc['loss_train'],\n",
    "                    val_loss=mc['loss_valid']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_deepmidas(mc):\n",
    "    mc['n_theta_hidden'] = len(mc['stack_types']) * [ [int(mc['n_hidden']), int(mc['n_hidden'])] ]\n",
    "\n",
    "    if mc['max_epochs'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_epochs'] / mc['n_lr_decays'])\n",
    "    elif mc['max_steps'] is not None:\n",
    "        lr_decay_step_size = int(mc['max_steps'] / mc['n_lr_decays'])\n",
    "\n",
    "    model = DeepMIDAS(n_time_in=int(mc['n_time_in']),\n",
    "                      n_time_out=int(mc['n_time_out']),\n",
    "                      n_x=mc['n_x'],\n",
    "                      n_s=mc['n_s'],\n",
    "                      n_s_hidden=int(mc['n_s_hidden']),\n",
    "                      n_x_hidden=int(mc['n_x_hidden']),\n",
    "                      shared_weights = mc['shared_weights'],\n",
    "                      initialization=mc['initialization'],\n",
    "                      activation=mc['activation'],\n",
    "                      stack_types=mc['stack_types'],\n",
    "                      n_blocks=mc['n_blocks'],\n",
    "                      n_layers=mc['n_layers'],\n",
    "                      n_theta_hidden=mc['n_theta_hidden'],\n",
    "                      n_pool_kernel_size=mc['n_pool_kernel_size'],\n",
    "                      n_freq_downsample=mc['n_freq_downsample'],\n",
    "                      batch_normalization = mc['batch_normalization'],\n",
    "                      dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                      learning_rate=float(mc['learning_rate']),\n",
    "                      lr_decay=float(mc['lr_decay']),\n",
    "                      lr_decay_step_size=lr_decay_step_size,\n",
    "                      weight_decay=mc['weight_decay'],\n",
    "                      loss_train=mc['loss_train'],\n",
    "                      loss_hypar=float(mc['loss_hypar']),\n",
    "                      loss_valid=mc['loss_valid'],\n",
    "                      frequency=mc['frequency'],\n",
    "                      seasonality=int(mc['seasonality']),\n",
    "                      random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_model(mc):\n",
    "    MODEL_DICT = {'nbeats': instantiate_nbeats,\n",
    "                  'esrnn': instantiate_esrnn,\n",
    "                  'mqesrnn': instantiate_mqesrnn,\n",
    "                  'deepmidas': instantiate_deepmidas}\n",
    "    return MODEL_DICT[mc['model']](mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def predict(mc, model, trainer, loader, scaler_y):\n",
    "   outputs = trainer.predict(model, loader)\n",
    "   y_true, y_hat, mask = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "   meta_data = loader.dataset.meta_data\n",
    "\n",
    "   # Scale to original scale\n",
    "   if mc['normalizer_y'] is not None:\n",
    "        y_true_shape = y_true.shape\n",
    "        y_true = scaler_y.inv_scale(x=y_true.flatten())\n",
    "        y_true = np.reshape(y_true, y_true_shape)\n",
    "\n",
    "        y_hat = scaler_y.inv_scale(x=y_hat.flatten())\n",
    "        y_hat = np.reshape(y_hat, y_true_shape)\n",
    "\n",
    "   return y_true, y_hat, mask, meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_fit_predict(mc, S_df, Y_df, X_df, f_cols, ds_in_val, ds_in_test):\n",
    "    \n",
    "    # Protect inplace modifications\n",
    "    Y_df = Y_df.copy()\n",
    "    if X_df is not None:\n",
    "        X_df = X_df.copy()\n",
    "    if S_df is not None:\n",
    "        S_df = S_df.copy()        \n",
    "\n",
    "    #----------------------------------------------- Datasets -----------------------------------------------#\n",
    "    train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc,\n",
    "                                                                         S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                                                         f_cols=f_cols,\n",
    "                                                                         ds_in_val=ds_in_val,\n",
    "                                                                         ds_in_test=ds_in_test)\n",
    "    mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()\n",
    "\n",
    "    #------------------------------------------- Instantiate & fit -------------------------------------------#\n",
    "    train_loader, val_loader, test_loader = instantiate_loaders(mc=mc,\n",
    "                                                                train_dataset=train_dataset,\n",
    "                                                                val_dataset=val_dataset,\n",
    "                                                                test_dataset=test_dataset)\n",
    "    model = instantiate_model(mc=mc)\n",
    "    callbacks = []\n",
    "    if mc['early_stop_patience']:\n",
    "        early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, \n",
    "                                                    patience=mc['early_stop_patience'],\n",
    "                                                    verbose=True, \n",
    "                                                    mode='min') \n",
    "        callbacks=[early_stopping]\n",
    "\n",
    "    gpus = -1 if t.cuda.is_available() else 0\n",
    "    trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                         max_steps=mc['max_steps'],\n",
    "                         check_val_every_n_epoch=mc['eval_freq'],\n",
    "                         progress_bar_refresh_rate=1,\n",
    "                         gpus=gpus,\n",
    "                         callbacks=callbacks,\n",
    "                         logger=False)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    #------------------------------------------------ Predict ------------------------------------------------#\n",
    "    results = {}\n",
    "\n",
    "    if ds_in_val > 0:\n",
    "        y_true, y_hat, mask, meta_data = predict(mc, model, trainer, val_loader, scaler_y)\n",
    "        val_values = (('val_y_true', y_true), ('val_y_hat', y_hat), ('val_mask', mask), ('val_meta_data', meta_data))\n",
    "        results.update(val_values)\n",
    "\n",
    "        print(f\"VAL y_true.shape: {y_true.shape}\")\n",
    "        print(f\"VAL y_hat.shape: {y_hat.shape}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Predict test if available\n",
    "    if ds_in_test > 0:\n",
    "        y_true, y_hat, mask, meta_data = predict(mc, model, trainer, test_loader, scaler_y)\n",
    "        test_values = (('test_y_true', y_true), ('test_y_hat', y_hat), ('test_mask', mask), ('test_meta_data', meta_data))\n",
    "        results.update(test_values)\n",
    "\n",
    "        print(f\"TEST y_true.shape: {y_true.shape}\")\n",
    "        print(f\"TEST y_hat.shape: {y_hat.shape}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_model(mc, loss_function_val, loss_functions_test, \n",
    "                   S_df, Y_df, X_df, f_cols,\n",
    "                   ds_in_val, ds_in_test,\n",
    "                   return_forecasts,\n",
    "                   save_progress,\n",
    "                   trials,\n",
    "                   results_file,\n",
    "                   loss_kwargs):\n",
    "\n",
    "    if (save_progress) and (len(trials) % 5 == 0):\n",
    "        with open(results_file, \"wb\") as f:\n",
    "            pickle.dump(trials, f)\n",
    "    \n",
    "    print(47*'=' + '\\n')\n",
    "    print(pd.Series(mc))\n",
    "    print(47*'=' + '\\n')\n",
    "    \n",
    "    # Some asserts due to work in progress\n",
    "    n_series = Y_df['unique_id'].nunique()\n",
    "    if n_series > 1:\n",
    "        assert mc['normalizer_y'] is None, 'Data scaling not implemented with multiple time series'\n",
    "        assert mc['normalizer_x'] is None, 'Data scaling not implemented with multiple time series'\n",
    "\n",
    "    assert ds_in_test % mc['val_idx_to_sample_freq']==0, 'outsample size should be multiple of val_idx_to_sample_freq'\n",
    "\n",
    "    # Make predictions\n",
    "    start = time.time()\n",
    "    results = model_fit_predict(mc=mc,\n",
    "                                S_df=S_df, \n",
    "                                Y_df=Y_df,\n",
    "                                X_df=X_df,\n",
    "                                f_cols=f_cols,\n",
    "                                ds_in_val=ds_in_val,\n",
    "                                ds_in_test=ds_in_test)\n",
    "    run_time = time.time() - start\n",
    "\n",
    "    # Evaluate predictions\n",
    "    val_loss = loss_function_val(y=results['val_y_true'], y_hat=results['val_y_hat'], weights=results['val_mask'], **loss_kwargs)\n",
    "\n",
    "    results_output = {'loss': val_loss,\n",
    "                      'mc': mc,\n",
    "                      'run_time': run_time,\n",
    "                      'status': STATUS_OK}\n",
    "\n",
    "    # Evaluation in test (if provided)\n",
    "    if ds_in_test > 0:\n",
    "        test_loss_dict = {}\n",
    "        for loss_name, loss_function in loss_functions_test.items():\n",
    "            test_loss_dict[loss_name] = loss_function(y=results['test_y_true'], y_hat=results['test_y_hat'], weights=results['test_mask'])\n",
    "        results_output['test_losses'] = test_loss_dict\n",
    "\n",
    "    if return_forecasts:\n",
    "        forecasts_test = {}\n",
    "        test_values = (('test_y_true', results['test_y_true']), ('test_y_hat', results['test_y_hat']),\n",
    "                        ('test_mask', results['test_mask']), ('test_meta_data', results['test_meta_data']))\n",
    "        forecasts_test.update(test_values)\n",
    "        results_output['forecasts_test'] = forecasts_test\n",
    "\n",
    "    return results_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hyperopt_tunning(space, hyperopt_max_evals, loss_function_val, loss_functions_test,\n",
    "                     S_df, Y_df, X_df, f_cols,\n",
    "                     ds_in_val, ds_in_test,\n",
    "                     return_forecasts,\n",
    "                     save_progress,\n",
    "                     results_file,\n",
    "                     loss_kwargs=None):\n",
    "    assert ds_in_val > 0, 'Validation set is needed for tunning!'\n",
    "\n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(evaluate_model, loss_function_val=loss_function_val, loss_functions_test=loss_functions_test,\n",
    "                             S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=f_cols,\n",
    "                             ds_in_val=ds_in_val, ds_in_test=ds_in_test,\n",
    "                             return_forecasts=return_forecasts, save_progress=save_progress, trials=trials,\n",
    "                             results_file=results_file,\n",
    "                             loss_kwargs=loss_kwargs or {})\n",
    "\n",
    "    fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=hyperopt_max_evals, trials=trials, verbose=True)\n",
    "\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from nixtlats.losses.numpy import mae, mape, smape, rmse, pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if t.cuda.is_available(): device = 'cuda'  \n",
    "\n",
    "deepmidas_space= {# Architecture parameters\n",
    "               'model':'deepmidas',\n",
    "               'mode': 'simple',\n",
    "               'n_time_in': hp.choice('n_time_in', [7*24]),\n",
    "               'n_time_out': hp.choice('n_time_out', [24]),\n",
    "               'n_x_hidden': hp.quniform('n_x_hidden', 1, 10, 1),\n",
    "               'n_s_hidden': hp.choice('n_s_hidden', [0]),\n",
    "               'shared_weights': hp.choice('shared_weights', [False]),\n",
    "               'activation': hp.choice('activation', ['SELU']),\n",
    "               'initialization':  hp.choice('initialization', ['glorot_normal','he_normal']),\n",
    "               'stack_types': hp.choice('stack_types', [2*['identity']]),\n",
    "               'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "               'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "               'n_hidden': hp.choice('n_hidden', [ 256 ]),\n",
    "               'n_pool_kernel_size': hp.choice('n_pool_kernel_size', [ [ 4, 1 ] ]),\n",
    "               'n_freq_downsample': hp.choice('n_freq_downsample', [ [ 24, 1 ] ]),\n",
    "               # Regularization and optimization parameters\n",
    "               'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "               'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 0.5),\n",
    "               'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "               'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.001)),\n",
    "               'lr_decay': hp.uniform('lr_decay', 0.3, 0.5),\n",
    "               'n_lr_decays': hp.choice('n_lr_decays', [3]), \n",
    "               'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "               'max_epochs': hp.choice('max_epochs', [10]), #'n_iterations': hp.choice('n_iterations', [10])\n",
    "               'max_steps': hp.choice('max_steps', [None]),\n",
    "               'early_stop_patience': hp.choice('early_stop_patience', [16]),\n",
    "               'eval_freq': hp.choice('eval_freq', [50]),\n",
    "               'loss_train': hp.choice('loss', ['MAE']),\n",
    "               'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "               'loss_valid': hp.choice('loss_valid', ['MAE']), #[args.val_loss]),\n",
    "               'l1_theta': hp.choice('l1_theta', [0]),\n",
    "               # Data parameters\n",
    "               'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "               'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "               'complete_windows': hp.choice('complete_windows', [False]),\n",
    "               'frequency': hp.choice('frequency', ['H']),\n",
    "               'seasonality': hp.choice('seasonality', [24]),      \n",
    "               'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "               'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [24]),\n",
    "               'batch_size': hp.choice('batch_size', [256]),\n",
    "               'random_seed': hp.quniform('random_seed', 10, 20, 1),\n",
    "               'device': hp.choice('device', [device])}\n",
    "\n",
    "mc = {'model':'deepmidas',\n",
    "      # Architecture parameters\n",
    "      'n_time_in': 7*24,\n",
    "      'n_time_out': 24,\n",
    "      'n_x_hidden': 3,\n",
    "      'n_s_hidden': 0,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'SELU',\n",
    "      'initialization': 'he_normal',\n",
    "      'stack_types': ['identity', 'identity'],\n",
    "      'n_blocks': [1, 1],\n",
    "      'n_layers': [2, 2],\n",
    "      'n_pool_kernel_size': [4, 1],\n",
    "      'n_freq_downsample': [24, 1],\n",
    "      'n_hidden': 364,\n",
    "      # Regularization and optimization parameters\n",
    "      'max_epochs': 10, #'n_iterations': 100,\n",
    "      'max_steps': None,      \n",
    "      'early_stop_patience': 8,\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.2,\n",
    "      'learning_rate': 0.0005, #0.002,\n",
    "      'lr_decay': 0.64,\n",
    "      'n_lr_decays': 3,\n",
    "      'weight_decay': 0.00015,\n",
    "      'eval_freq': 50,\n",
    "      'loss_train': 'PINBALL',\n",
    "      'loss_hypar': 0.5, #0.49,\n",
    "      'loss_valid': 'MAE',\n",
    "      'l1_theta': 0,\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'window_sampling_limit': 100_000,\n",
    "      'complete_windows': False,\n",
    "      'frequency':'H',\n",
    "      'seasonality': 24,\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'val_idx_to_sample_freq': 24,\n",
    "      'batch_size': 256,\n",
    "      'n_series_per_batch': 1,\n",
    "      'random_seed': 10,\n",
    "      'device': 'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nixtlats.data.datasets.epf import EPF, EPFInfo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = ['NP']\n",
    "\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "\n",
    "X_df = X_df[['unique_id', 'ds', 'week_day']]\n",
    "Y_min = Y_df.y.min()\n",
    "#Y_df.y = Y_df.y - Y_min + 20\n",
    "\n",
    "plt.plot(Y_df.y.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: think about use of big windows during train for extremely long series\n",
    "# backpropagation trough time is slow\n",
    "# result = evaluate_model(loss_function=mae, mc=mc, \n",
    "#                         S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "#                         ds_in_test=0, ds_in_val=728*24,\n",
    "#                         n_uids=None, n_val_windows=None, freq=None,\n",
    "#                         is_val_random=False, loss_kwargs={})\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(result['y_hat'].flatten())\n",
    "# plt.plot(Y_df['y'][-728*24:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.010585 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "\n",
      "activation                                SELU\n",
      "batch_normalization                      False\n",
      "batch_size                                 256\n",
      "complete_windows                         False\n",
      "device                                     cpu\n",
      "dropout_prob_exogenous                0.193444\n",
      "dropout_prob_theta                    0.122824\n",
      "early_stop_patience                         16\n",
      "eval_freq                                   50\n",
      "frequency                                    H\n",
      "idx_to_sample_freq                          24\n",
      "initialization                       he_normal\n",
      "l1_theta                                     0\n",
      "learning_rate                         0.000572\n",
      "loss_hypar                                 0.5\n",
      "loss_train                                 MAE\n",
      "loss_valid                                 MAE\n",
      "lr_decay                              0.455533\n",
      "max_epochs                                  10\n",
      "max_steps                                 None\n",
      "mode                                    simple\n",
      "model                                deepmidas\n",
      "n_blocks                                (1, 1)\n",
      "n_freq_downsample                      (24, 1)\n",
      "n_hidden                                   256\n",
      "n_layers                                (2, 2)\n",
      "n_lr_decays                                  3\n",
      "n_pool_kernel_size                      (4, 1)\n",
      "n_s_hidden                                   0\n",
      "n_time_in                                  168\n",
      "n_time_out                                  24\n",
      "n_x_hidden                                 4.0\n",
      "normalizer_x                            median\n",
      "normalizer_y                              None\n",
      "random_seed                               17.0\n",
      "seasonality                                 24\n",
      "shared_weights                           False\n",
      "stack_types               (identity, identity)\n",
      "val_idx_to_sample_freq                      24\n",
      "weight_decay                          0.000128\n",
      "dtype: object\n",
      "===============================================\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | _DeepMIDAS | 376 K \n",
      "-------------------------------------\n",
      "376 K     Trainable params\n",
      "0         Non-trainable params\n",
      "376 K     Total params\n",
      "1.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:323: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: -1it [00:00, ?it/s]\n",
      "Training:   0%|          | 0/1 [00:00<00:00, 26214.40it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<00:00, 326.38it/s]   \n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00, 49.58it/s, loss=4.18, v_num=0, train_loss_step=4.180]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<00:00, 22310.13it/s, loss=4.18, v_num=0, train_loss_step=4.180]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<00:00, 430.67it/s, loss=4.18, v_num=0, train_loss_step=4.180]  \n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00, 58.35it/s, loss=4.89, v_num=0, train_loss_step=5.600, train_loss_epoch=4.180]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<00:00, 20867.18it/s, loss=4.89, v_num=0, train_loss_step=5.600, train_loss_epoch=4.180]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<00:00, 306.29it/s, loss=4.89, v_num=0, train_loss_step=5.600, train_loss_epoch=4.180]  \n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00, 52.37it/s, loss=4.27, v_num=0, train_loss_step=3.030, train_loss_epoch=5.600] \n",
      "Epoch 2:   0%|          | 0/1 [00:00<00:00, 18893.26it/s, loss=4.27, v_num=0, train_loss_step=3.030, train_loss_epoch=5.600]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00, 340.03it/s, loss=4.27, v_num=0, train_loss_step=3.030, train_loss_epoch=5.600]  \n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459064158/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00, 59.70it/s, loss=4.24, v_num=0, train_loss_step=4.170, train_loss_epoch=3.030] \n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00, 21732.15it/s, loss=4.24, v_num=0, train_loss_step=4.170, train_loss_epoch=3.030]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<00:00, 373.39it/s, loss=4.24, v_num=0, train_loss_step=4.170, train_loss_epoch=3.030]  \n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00, 60.26it/s, loss=4.16, v_num=0, train_loss_step=3.830, train_loss_epoch=4.170] \n",
      "Epoch 4:   0%|          | 0/1 [00:00<00:00, 24818.37it/s, loss=4.16, v_num=0, train_loss_step=3.830, train_loss_epoch=4.170]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00, 305.97it/s, loss=4.16, v_num=0, train_loss_step=3.830, train_loss_epoch=4.170]  \n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00, 66.29it/s, loss=4.05, v_num=0, train_loss_step=3.510, train_loss_epoch=3.830] \n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00, 21732.15it/s, loss=4.05, v_num=0, train_loss_step=3.510, train_loss_epoch=3.830]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00, 358.43it/s, loss=4.05, v_num=0, train_loss_step=3.510, train_loss_epoch=3.830]  \n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00, 63.15it/s, loss=3.91, v_num=0, train_loss_step=3.060, train_loss_epoch=3.510] \n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00, 19508.39it/s, loss=3.91, v_num=0, train_loss_step=3.060, train_loss_epoch=3.510]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<00:00, 208.55it/s, loss=3.91, v_num=0, train_loss_step=3.060, train_loss_epoch=3.510]  \n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00, 48.84it/s, loss=3.77, v_num=0, train_loss_step=2.810, train_loss_epoch=3.060] \n",
      "Epoch 7:   0%|          | 0/1 [00:00<00:00, 9799.78it/s, loss=3.77, v_num=0, train_loss_step=2.810, train_loss_epoch=3.060]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00, 169.32it/s, loss=3.77, v_num=0, train_loss_step=2.810, train_loss_epoch=3.060] \n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00, 45.04it/s, loss=3.64, v_num=0, train_loss_step=2.560, train_loss_epoch=2.810] \n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00, 19972.88it/s, loss=3.64, v_num=0, train_loss_step=2.560, train_loss_epoch=2.810]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<00:00, 229.89it/s, loss=3.64, v_num=0, train_loss_step=2.560, train_loss_epoch=2.810]  \n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 54.43it/s, loss=3.53, v_num=0, train_loss_step=2.590, train_loss_epoch=2.560] \n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 48.30it/s, loss=3.53, v_num=0, train_loss_step=2.590, train_loss_epoch=2.560]\n",
      "  0%|          | 0/2 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 1it [00:00, ?it/s]\n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "VAL y_true.shape: (7, 24)\n",
      "VAL y_hat.shape: (7, 24)\n",
      "Predicting: 1it [00:00, ?it/s]\n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "TEST y_true.shape: (7, 24)\n",
      "TEST y_hat.shape: (7, 24)\n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.50s/trial, best loss: 5.847511291503906]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.010630 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 5.847511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "\n",
      "activation                                SELU\n",
      "batch_normalization                      False\n",
      "batch_size                                 256\n",
      "complete_windows                         False\n",
      "device                                     cpu\n",
      "dropout_prob_exogenous                0.213042\n",
      "dropout_prob_theta                    0.277656\n",
      "early_stop_patience                         16\n",
      "eval_freq                                   50\n",
      "frequency                                    H\n",
      "idx_to_sample_freq                          24\n",
      "initialization                   glorot_normal\n",
      "l1_theta                                     0\n",
      "learning_rate                         0.000905\n",
      "loss_hypar                                 0.5\n",
      "loss_train                                 MAE\n",
      "loss_valid                                 MAE\n",
      "lr_decay                              0.465463\n",
      "max_epochs                                  10\n",
      "max_steps                                 None\n",
      "mode                                    simple\n",
      "model                                deepmidas\n",
      "n_blocks                                (1, 1)\n",
      "n_freq_downsample                      (24, 1)\n",
      "n_hidden                                   256\n",
      "n_layers                                (2, 2)\n",
      "n_lr_decays                                  3\n",
      "n_pool_kernel_size                      (4, 1)\n",
      "n_s_hidden                                   0\n",
      "n_time_in                                  168\n",
      "n_time_out                                  24\n",
      "n_x_hidden                                 5.0\n",
      "normalizer_x                            median\n",
      "normalizer_y                              None\n",
      "random_seed                               18.0\n",
      "seasonality                                 24\n",
      "shared_weights                           False\n",
      "stack_types               (identity, identity)\n",
      "val_idx_to_sample_freq                      24\n",
      "weight_decay                          0.000384\n",
      "dtype: object\n",
      "===============================================\n",
      "\n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.50s/trial, best loss: 5.847511291503906]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2018-12-11 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2018-12-10 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=99.36, \t52080 time stamps \n",
      "Outsample percentage=0.64, \t336 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "          1           2018-12-11 2018-12-17 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-17 23:00:00\n",
      "          1           2018-12-18 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.32, \t168 time stamps \n",
      "Outsample percentage=99.68, \t52248 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | _DeepMIDAS | 376 K \n",
      "-------------------------------------\n",
      "376 K     Trainable params\n",
      "0         Non-trainable params\n",
      "376 K     Total params\n",
      "1.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]\n",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:02<00:01,  1.50s/trial, best loss: 5.847511291503906]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n",
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:323: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: -1it [00:00, ?it/s]\n",
      "Training:   0%|          | 0/1 [00:00<00:00, 25115.59it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<00:00, 336.49it/s]   \n",
      "Epoch 0: 100%|##########| 1/1 [00:00<00:00, 58.43it/s, loss=4.84, v_num=1, train_loss_step=4.840]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<00:00, 20867.18it/s, loss=4.84, v_num=1, train_loss_step=4.840]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<00:00, 347.10it/s, loss=4.84, v_num=1, train_loss_step=4.840]  \n",
      "Epoch 1: 100%|##########| 1/1 [00:00<00:00, 63.12it/s, loss=6.43, v_num=1, train_loss_step=8.020, train_loss_epoch=4.840]\n",
      "Epoch 1:   0%|          | 0/1 [00:00<00:00, 22671.91it/s, loss=6.43, v_num=1, train_loss_step=8.020, train_loss_epoch=4.840]\n",
      "Epoch 2:   0%|          | 0/1 [00:00<00:00, 362.14it/s, loss=6.43, v_num=1, train_loss_step=8.020, train_loss_epoch=4.840]  \n",
      "Epoch 2: 100%|##########| 1/1 [00:00<00:00, 66.28it/s, loss=5.71, v_num=1, train_loss_step=4.260, train_loss_epoch=8.020] \n",
      "Epoch 2:   0%|          | 0/1 [00:00<00:00, 20763.88it/s, loss=5.71, v_num=1, train_loss_step=4.260, train_loss_epoch=8.020]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00, 356.39it/s, loss=5.71, v_num=1, train_loss_step=4.260, train_loss_epoch=8.020]  \n",
      "Epoch 3: 100%|##########| 1/1 [00:00<00:00, 57.07it/s, loss=5.48, v_num=1, train_loss_step=4.800, train_loss_epoch=4.260] \n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00, 20360.70it/s, loss=5.48, v_num=1, train_loss_step=4.800, train_loss_epoch=4.260]\n",
      "Epoch 4:   0%|          | 0/1 [00:00<00:00, 343.77it/s, loss=5.48, v_num=1, train_loss_step=4.800, train_loss_epoch=4.260]  \n",
      "Epoch 4: 100%|##########| 1/1 [00:00<00:00, 56.22it/s, loss=5.25, v_num=1, train_loss_step=4.330, train_loss_epoch=4.800] \n",
      "Epoch 4:   0%|          | 0/1 [00:00<00:00, 11491.24it/s, loss=5.25, v_num=1, train_loss_step=4.330, train_loss_epoch=4.800]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00, 219.87it/s, loss=5.25, v_num=1, train_loss_step=4.330, train_loss_epoch=4.800]  \n",
      "Epoch 5: 100%|##########| 1/1 [00:00<00:00, 55.00it/s, loss=5.03, v_num=1, train_loss_step=3.920, train_loss_epoch=4.330] \n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00, 19599.55it/s, loss=5.03, v_num=1, train_loss_step=3.920, train_loss_epoch=4.330]\n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00, 310.85it/s, loss=5.03, v_num=1, train_loss_step=3.920, train_loss_epoch=4.330]  \n",
      "Epoch 6: 100%|##########| 1/1 [00:00<00:00, 64.37it/s, loss=4.75, v_num=1, train_loss_step=3.120, train_loss_epoch=3.920] \n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00, 20360.70it/s, loss=4.75, v_num=1, train_loss_step=3.120, train_loss_epoch=3.920]\n",
      "Epoch 7:   0%|          | 0/1 [00:00<00:00, 369.67it/s, loss=4.75, v_num=1, train_loss_step=3.120, train_loss_epoch=3.920]  \n",
      "Epoch 7: 100%|##########| 1/1 [00:00<00:00, 53.01it/s, loss=4.54, v_num=1, train_loss_step=3.060, train_loss_epoch=3.120] \n",
      "Epoch 7:   0%|          | 0/1 [00:00<00:00, 23831.27it/s, loss=4.54, v_num=1, train_loss_step=3.060, train_loss_epoch=3.120]\n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00, 294.75it/s, loss=4.54, v_num=1, train_loss_step=3.060, train_loss_epoch=3.120]  \n",
      "Epoch 8: 100%|##########| 1/1 [00:00<00:00, 65.20it/s, loss=4.38, v_num=1, train_loss_step=3.090, train_loss_epoch=3.060] \n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00, 18157.16it/s, loss=4.38, v_num=1, train_loss_step=3.090, train_loss_epoch=3.060]\n",
      "Epoch 9:   0%|          | 0/1 [00:00<00:00, 266.73it/s, loss=4.38, v_num=1, train_loss_step=3.090, train_loss_epoch=3.060]  \n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 53.64it/s, loss=4.27, v_num=1, train_loss_step=3.230, train_loss_epoch=3.090] \n",
      "Epoch 9: 100%|##########| 1/1 [00:00<00:00, 47.26it/s, loss=4.27, v_num=1, train_loss_step=3.230, train_loss_epoch=3.090]\n",
      " 50%|█████     | 1/2 [00:02<00:01,  1.50s/trial, best loss: 5.847511291503906]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 1it [00:00, ?it/s]\n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "VAL y_true.shape: (7, 24)\n",
      "VAL y_hat.shape: (7, 24)\n",
      "Predicting: 1it [00:00, ?it/s]\n",
      "Predicting: 100%|##########| 1/1 [00:00<?, ?it/s]\n",
      "TEST y_true.shape: (7, 24)\n",
      "TEST y_hat.shape: (7, 24)\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.47s/trial, best loss: 5.847511291503906]\n"
     ]
    }
   ],
   "source": [
    "trials = hyperopt_tunning(space=deepmidas_space, hyperopt_max_evals=2, loss_function_val=mae,\n",
    "                          loss_functions_test={'mae': mae, 'rmse': rmse},\n",
    "                          S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "                          ds_in_val=7*24, ds_in_test=7*24, return_forecasts=True, save_progress=False, results_file=None, loss_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 2,\n",
       "  'tid': 0,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 5.847511291503906,\n",
       "   'mc': {'activation': 'SELU',\n",
       "    'batch_normalization': False,\n",
       "    'batch_size': 256,\n",
       "    'complete_windows': False,\n",
       "    'device': 'cpu',\n",
       "    'dropout_prob_exogenous': 0.19344375455683255,\n",
       "    'dropout_prob_theta': 0.1228237937775486,\n",
       "    'early_stop_patience': 16,\n",
       "    'eval_freq': 50,\n",
       "    'frequency': 'H',\n",
       "    'idx_to_sample_freq': 24,\n",
       "    'initialization': 'he_normal',\n",
       "    'l1_theta': 0,\n",
       "    'learning_rate': 0.0005717811854514028,\n",
       "    'loss_hypar': 0.5,\n",
       "    'loss_train': 'MAE',\n",
       "    'loss_valid': 'MAE',\n",
       "    'lr_decay': 0.45553346370380393,\n",
       "    'max_epochs': 10,\n",
       "    'max_steps': None,\n",
       "    'mode': 'simple',\n",
       "    'model': 'deepmidas',\n",
       "    'n_blocks': (1, 1),\n",
       "    'n_freq_downsample': (24, 1),\n",
       "    'n_hidden': 256,\n",
       "    'n_layers': (2, 2),\n",
       "    'n_lr_decays': 3,\n",
       "    'n_pool_kernel_size': (4, 1),\n",
       "    'n_s_hidden': 0,\n",
       "    'n_time_in': 168,\n",
       "    'n_time_out': 24,\n",
       "    'n_x_hidden': 4.0,\n",
       "    'normalizer_x': 'median',\n",
       "    'normalizer_y': None,\n",
       "    'random_seed': 17.0,\n",
       "    'seasonality': 24,\n",
       "    'shared_weights': False,\n",
       "    'stack_types': ('identity', 'identity'),\n",
       "    'val_idx_to_sample_freq': 24,\n",
       "    'weight_decay': 0.0001276206348508508,\n",
       "    'n_x': 1,\n",
       "    'n_s': 1,\n",
       "    'n_theta_hidden': [[256, 256], [256, 256]]},\n",
       "   'run_time': 1.4711720943450928,\n",
       "   'status': 'ok',\n",
       "   'test_losses': {'mae': 2.8727837, 'rmse': 4.0669765},\n",
       "   'forecasts_test': {'test_y_true': array([[52.33, 51.02, 50.24, 49.42, 50.18, 52.89, 56.46, 71.25, 74.69,\n",
       "            72.06, 69.72, 67.63, 65.92, 66.08, 68.05, 69.94, 67.68, 65.96,\n",
       "            57.86, 55.12, 53.54, 52.55, 50.67, 48.69],\n",
       "           [49.55, 48.84, 48.18, 47.82, 48.65, 50.6 , 52.14, 54.25, 55.69,\n",
       "            55.27, 55.03, 55.32, 55.71, 55.68, 55.74, 56.31, 57.19, 56.71,\n",
       "            56.27, 55.2 , 54.34, 53.22, 51.67, 50.12],\n",
       "           [48.35, 48.08, 48.31, 47.36, 47.94, 50.62, 54.38, 57.84, 58.63,\n",
       "            57.53, 56.45, 56.31, 57.39, 57.21, 57.52, 57.81, 57.88, 57.2 ,\n",
       "            55.7 , 54.31, 52.25, 51.29, 50.  , 48.12],\n",
       "           [47.21, 46.42, 46.  , 46.12, 46.49, 49.97, 52.73, 55.65, 57.04,\n",
       "            57.13, 56.98, 56.04, 56.22, 55.78, 54.6 , 54.11, 54.25, 54.69,\n",
       "            54.21, 52.79, 52.05, 51.43, 50.07, 49.01],\n",
       "           [48.39, 47.72, 47.23, 46.6 , 46.94, 47.76, 48.41, 49.44, 50.29,\n",
       "            51.52, 52.56, 53.08, 52.99, 53.13, 52.93, 53.78, 55.04, 56.52,\n",
       "            57.07, 55.52, 53.05, 52.05, 51.09, 50.47],\n",
       "           [51.49, 50.83, 50.74, 50.14, 49.94, 50.46, 50.88, 51.37, 51.61,\n",
       "            52.22, 52.8 , 53.  , 53.11, 52.93, 52.93, 53.75, 55.99, 61.2 ,\n",
       "            61.2 , 57.42, 55.61, 53.99, 53.86, 52.32],\n",
       "           [51.09, 50.19, 48.98, 48.8 , 48.52, 49.8 , 50.05, 50.55, 52.33,\n",
       "            53.26, 53.14, 52.88, 53.03, 52.46, 52.44, 52.89, 53.26, 52.61,\n",
       "            51.28, 50.72, 49.86, 49.09, 49.02, 48.1 ]], dtype=float32),\n",
       "    'test_y_hat': array([[54.58335 , 53.331837, 52.41033 , 50.52648 , 50.991657, 53.618195,\n",
       "            57.47383 , 60.160793, 61.224087, 59.173756, 59.674747, 59.745525,\n",
       "            59.81904 , 58.13237 , 56.290325, 57.930187, 59.143524, 56.69668 ,\n",
       "            59.035065, 59.29616 , 57.043205, 57.880062, 55.157604, 53.140507],\n",
       "           [51.28105 , 49.694267, 49.45726 , 48.532925, 48.383064, 50.13819 ,\n",
       "            53.630894, 57.480312, 57.82875 , 56.334938, 56.14134 , 55.507145,\n",
       "            56.523403, 54.00462 , 53.358055, 55.089375, 55.7385  , 54.32377 ,\n",
       "            57.04119 , 56.201435, 54.963646, 55.19079 , 52.20211 , 50.219723],\n",
       "           [52.42753 , 50.5889  , 49.663788, 47.87977 , 49.477917, 50.716976,\n",
       "            54.671143, 58.068016, 58.68021 , 57.237247, 57.28131 , 56.376328,\n",
       "            56.958225, 55.45016 , 54.124313, 55.73529 , 56.99061 , 55.06836 ,\n",
       "            57.76631 , 57.542847, 55.93144 , 56.298687, 53.97447 , 52.389816],\n",
       "           [48.87973 , 47.572617, 46.122536, 45.410667, 46.359516, 47.344963,\n",
       "            51.513718, 54.650394, 55.03282 , 53.44555 , 53.96023 , 52.944588,\n",
       "            53.115562, 52.386826, 50.701523, 52.47048 , 53.321423, 52.44243 ,\n",
       "            52.93713 , 53.235767, 52.814476, 51.542873, 49.975143, 48.5639  ],\n",
       "           [48.96855 , 48.26785 , 47.757137, 47.368034, 47.374043, 49.48477 ,\n",
       "            52.066765, 55.855064, 56.36388 , 54.076084, 54.22337 , 54.154503,\n",
       "            54.953438, 53.439857, 52.41001 , 53.685238, 54.110596, 52.87704 ,\n",
       "            54.36713 , 54.249523, 52.998436, 52.329826, 51.174755, 49.30695 ],\n",
       "           [51.325474, 50.195503, 49.885975, 48.067398, 49.33471 , 51.103622,\n",
       "            53.75799 , 57.713604, 58.2386  , 56.940117, 56.413536, 56.780663,\n",
       "            56.87771 , 54.68297 , 53.982075, 55.367233, 56.59761 , 54.707283,\n",
       "            57.13394 , 57.019127, 55.6057  , 55.467644, 53.65875 , 52.03377 ],\n",
       "           [54.153324, 52.736164, 52.129944, 50.955723, 51.612152, 52.2771  ,\n",
       "            56.735104, 59.814457, 60.961716, 58.819687, 59.5916  , 58.329   ,\n",
       "            58.837845, 56.98048 , 55.675568, 57.681973, 57.96444 , 57.325184,\n",
       "            59.393124, 58.89714 , 57.667706, 57.758507, 55.83545 , 53.630276]],\n",
       "          dtype=float32),\n",
       "    'test_mask': array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),\n",
       "    'test_meta_data': [array([['NP', Timestamp('2013-01-01 00:00:00')],\n",
       "            ['NP', Timestamp('2013-01-01 01:00:00')],\n",
       "            ['NP', Timestamp('2013-01-01 02:00:00')],\n",
       "            ...,\n",
       "            ['NP', Timestamp('2018-12-24 21:00:00')],\n",
       "            ['NP', Timestamp('2018-12-24 22:00:00')],\n",
       "            ['NP', Timestamp('2018-12-24 23:00:00')]], dtype=object)]}},\n",
       "  'misc': {'tid': 0,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_windows': [0],\n",
       "    'device': [0],\n",
       "    'dropout_prob_exogenous': [0],\n",
       "    'dropout_prob_theta': [0],\n",
       "    'early_stop_patience': [0],\n",
       "    'eval_freq': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [0],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'loss_valid': [0],\n",
       "    'lr_decay': [0],\n",
       "    'max_epochs': [0],\n",
       "    'max_steps': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_freq_downsample': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_lr_decays': [0],\n",
       "    'n_pool_kernel_size': [0],\n",
       "    'n_s_hidden': [0],\n",
       "    'n_time_in': [0],\n",
       "    'n_time_out': [0],\n",
       "    'n_x_hidden': [0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'random_seed': [0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [0],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'weight_decay': [0]},\n",
       "   'vals': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_windows': [0],\n",
       "    'device': [0],\n",
       "    'dropout_prob_exogenous': [0.19344375455683255],\n",
       "    'dropout_prob_theta': [0.1228237937775486],\n",
       "    'early_stop_patience': [0],\n",
       "    'eval_freq': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [1],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0.0005717811854514028],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'loss_valid': [0],\n",
       "    'lr_decay': [0.45553346370380393],\n",
       "    'max_epochs': [0],\n",
       "    'max_steps': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_freq_downsample': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_lr_decays': [0],\n",
       "    'n_pool_kernel_size': [0],\n",
       "    'n_s_hidden': [0],\n",
       "    'n_time_in': [0],\n",
       "    'n_time_out': [0],\n",
       "    'n_x_hidden': [4.0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'random_seed': [17.0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [0],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'weight_decay': [0.0001276206348508508]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 12, 22, 19, 4, 28, 166000),\n",
       "  'refresh_time': datetime.datetime(2021, 12, 22, 19, 4, 29, 648000)},\n",
       " {'state': 2,\n",
       "  'tid': 1,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 5.947025299072266,\n",
       "   'mc': {'activation': 'SELU',\n",
       "    'batch_normalization': False,\n",
       "    'batch_size': 256,\n",
       "    'complete_windows': False,\n",
       "    'device': 'cpu',\n",
       "    'dropout_prob_exogenous': 0.21304176465667407,\n",
       "    'dropout_prob_theta': 0.2776564423971945,\n",
       "    'early_stop_patience': 16,\n",
       "    'eval_freq': 50,\n",
       "    'frequency': 'H',\n",
       "    'idx_to_sample_freq': 24,\n",
       "    'initialization': 'glorot_normal',\n",
       "    'l1_theta': 0,\n",
       "    'learning_rate': 0.0009050819929552008,\n",
       "    'loss_hypar': 0.5,\n",
       "    'loss_train': 'MAE',\n",
       "    'loss_valid': 'MAE',\n",
       "    'lr_decay': 0.4654631099519051,\n",
       "    'max_epochs': 10,\n",
       "    'max_steps': None,\n",
       "    'mode': 'simple',\n",
       "    'model': 'deepmidas',\n",
       "    'n_blocks': (1, 1),\n",
       "    'n_freq_downsample': (24, 1),\n",
       "    'n_hidden': 256,\n",
       "    'n_layers': (2, 2),\n",
       "    'n_lr_decays': 3,\n",
       "    'n_pool_kernel_size': (4, 1),\n",
       "    'n_s_hidden': 0,\n",
       "    'n_time_in': 168,\n",
       "    'n_time_out': 24,\n",
       "    'n_x_hidden': 5.0,\n",
       "    'normalizer_x': 'median',\n",
       "    'normalizer_y': None,\n",
       "    'random_seed': 18.0,\n",
       "    'seasonality': 24,\n",
       "    'shared_weights': False,\n",
       "    'stack_types': ('identity', 'identity'),\n",
       "    'val_idx_to_sample_freq': 24,\n",
       "    'weight_decay': 0.0003840753699219232,\n",
       "    'n_x': 1,\n",
       "    'n_s': 1,\n",
       "    'n_theta_hidden': [[256, 256], [256, 256]]},\n",
       "   'run_time': 1.4051473140716553,\n",
       "   'status': 'ok',\n",
       "   'test_losses': {'mae': 3.0067255, 'rmse': 4.1930113},\n",
       "   'forecasts_test': {'test_y_true': array([[52.33, 51.02, 50.24, 49.42, 50.18, 52.89, 56.46, 71.25, 74.69,\n",
       "            72.06, 69.72, 67.63, 65.92, 66.08, 68.05, 69.94, 67.68, 65.96,\n",
       "            57.86, 55.12, 53.54, 52.55, 50.67, 48.69],\n",
       "           [49.55, 48.84, 48.18, 47.82, 48.65, 50.6 , 52.14, 54.25, 55.69,\n",
       "            55.27, 55.03, 55.32, 55.71, 55.68, 55.74, 56.31, 57.19, 56.71,\n",
       "            56.27, 55.2 , 54.34, 53.22, 51.67, 50.12],\n",
       "           [48.35, 48.08, 48.31, 47.36, 47.94, 50.62, 54.38, 57.84, 58.63,\n",
       "            57.53, 56.45, 56.31, 57.39, 57.21, 57.52, 57.81, 57.88, 57.2 ,\n",
       "            55.7 , 54.31, 52.25, 51.29, 50.  , 48.12],\n",
       "           [47.21, 46.42, 46.  , 46.12, 46.49, 49.97, 52.73, 55.65, 57.04,\n",
       "            57.13, 56.98, 56.04, 56.22, 55.78, 54.6 , 54.11, 54.25, 54.69,\n",
       "            54.21, 52.79, 52.05, 51.43, 50.07, 49.01],\n",
       "           [48.39, 47.72, 47.23, 46.6 , 46.94, 47.76, 48.41, 49.44, 50.29,\n",
       "            51.52, 52.56, 53.08, 52.99, 53.13, 52.93, 53.78, 55.04, 56.52,\n",
       "            57.07, 55.52, 53.05, 52.05, 51.09, 50.47],\n",
       "           [51.49, 50.83, 50.74, 50.14, 49.94, 50.46, 50.88, 51.37, 51.61,\n",
       "            52.22, 52.8 , 53.  , 53.11, 52.93, 52.93, 53.75, 55.99, 61.2 ,\n",
       "            61.2 , 57.42, 55.61, 53.99, 53.86, 52.32],\n",
       "           [51.09, 50.19, 48.98, 48.8 , 48.52, 49.8 , 50.05, 50.55, 52.33,\n",
       "            53.26, 53.14, 52.88, 53.03, 52.46, 52.44, 52.89, 53.26, 52.61,\n",
       "            51.28, 50.72, 49.86, 49.09, 49.02, 48.1 ]], dtype=float32),\n",
       "    'test_y_hat': array([[53.563362, 53.50281 , 52.559696, 51.15349 , 53.92445 , 55.13789 ,\n",
       "            57.27786 , 59.63642 , 57.621063, 56.735645, 59.003677, 60.899063,\n",
       "            58.498375, 60.238487, 57.18109 , 58.210552, 59.272137, 57.385204,\n",
       "            56.634182, 57.73477 , 58.916035, 57.179485, 56.69267 , 54.66183 ],\n",
       "           [50.1111  , 48.9338  , 48.891335, 47.343086, 49.65281 , 51.458363,\n",
       "            53.39969 , 56.97281 , 55.30368 , 53.943306, 56.27744 , 56.808308,\n",
       "            55.54164 , 57.265213, 53.014908, 54.72462 , 55.48417 , 52.93752 ,\n",
       "            53.554348, 54.308853, 55.748257, 53.760246, 52.713135, 50.465794],\n",
       "           [51.787697, 50.6342  , 49.586876, 49.575356, 51.605755, 52.473763,\n",
       "            55.00312 , 57.936634, 56.19935 , 54.794197, 56.69571 , 57.978275,\n",
       "            56.433235, 58.05834 , 55.481853, 56.349903, 56.96913 , 55.140793,\n",
       "            54.711357, 55.517483, 56.770306, 54.898598, 54.104652, 52.805138],\n",
       "           [49.15814 , 49.110756, 46.32924 , 47.374035, 48.230034, 49.368156,\n",
       "            52.899723, 54.167725, 53.537502, 52.529182, 54.24752 , 55.526184,\n",
       "            53.48379 , 55.86478 , 52.92308 , 53.33116 , 53.88451 , 53.204243,\n",
       "            52.758804, 53.3402  , 53.9703  , 52.246544, 50.822094, 50.27565 ],\n",
       "           [48.817142, 49.850876, 47.474434, 48.41213 , 48.850204, 49.716404,\n",
       "            53.83    , 54.889828, 54.471466, 53.61315 , 54.234383, 54.878227,\n",
       "            53.261555, 55.821693, 52.45423 , 53.21579 , 54.01657 , 52.957542,\n",
       "            53.25521 , 54.690834, 54.18101 , 52.94851 , 51.643623, 50.610245],\n",
       "           [50.204617, 51.29056 , 49.506084, 49.87899 , 51.092453, 51.707443,\n",
       "            54.207794, 56.819515, 55.613968, 54.095154, 55.52661 , 56.766697,\n",
       "            55.05188 , 56.581017, 54.24356 , 55.46851 , 56.934067, 54.106285,\n",
       "            54.60294 , 54.99549 , 55.91269 , 55.00688 , 53.95988 , 52.50413 ],\n",
       "           [53.837452, 53.23396 , 51.69062 , 51.897907, 53.12548 , 54.431965,\n",
       "            56.596275, 60.001183, 58.248386, 56.658737, 58.839222, 60.0324  ,\n",
       "            58.384087, 59.704193, 57.9283  , 58.337074, 59.25921 , 56.831116,\n",
       "            57.001324, 57.465706, 58.579994, 56.863586, 55.904984, 54.511265]],\n",
       "          dtype=float32),\n",
       "    'test_mask': array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "            1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),\n",
       "    'test_meta_data': [array([['NP', Timestamp('2013-01-01 00:00:00')],\n",
       "            ['NP', Timestamp('2013-01-01 01:00:00')],\n",
       "            ['NP', Timestamp('2013-01-01 02:00:00')],\n",
       "            ...,\n",
       "            ['NP', Timestamp('2018-12-24 21:00:00')],\n",
       "            ['NP', Timestamp('2018-12-24 22:00:00')],\n",
       "            ['NP', Timestamp('2018-12-24 23:00:00')]], dtype=object)]}},\n",
       "  'misc': {'tid': 1,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'activation': [1],\n",
       "    'batch_normalization': [1],\n",
       "    'batch_size': [1],\n",
       "    'complete_windows': [1],\n",
       "    'device': [1],\n",
       "    'dropout_prob_exogenous': [1],\n",
       "    'dropout_prob_theta': [1],\n",
       "    'early_stop_patience': [1],\n",
       "    'eval_freq': [1],\n",
       "    'frequency': [1],\n",
       "    'idx_to_sample_freq': [1],\n",
       "    'initialization': [1],\n",
       "    'l1_theta': [1],\n",
       "    'learning_rate': [1],\n",
       "    'loss': [1],\n",
       "    'loss_hypar': [1],\n",
       "    'loss_valid': [1],\n",
       "    'lr_decay': [1],\n",
       "    'max_epochs': [1],\n",
       "    'max_steps': [1],\n",
       "    'n_blocks': [1],\n",
       "    'n_freq_downsample': [1],\n",
       "    'n_hidden': [1],\n",
       "    'n_layers': [1],\n",
       "    'n_lr_decays': [1],\n",
       "    'n_pool_kernel_size': [1],\n",
       "    'n_s_hidden': [1],\n",
       "    'n_time_in': [1],\n",
       "    'n_time_out': [1],\n",
       "    'n_x_hidden': [1],\n",
       "    'normalizer_x': [1],\n",
       "    'normalizer_y': [1],\n",
       "    'random_seed': [1],\n",
       "    'seasonality': [1],\n",
       "    'shared_weights': [1],\n",
       "    'stack_types': [1],\n",
       "    'val_idx_to_sample_freq': [1],\n",
       "    'weight_decay': [1]},\n",
       "   'vals': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_windows': [0],\n",
       "    'device': [0],\n",
       "    'dropout_prob_exogenous': [0.21304176465667407],\n",
       "    'dropout_prob_theta': [0.2776564423971945],\n",
       "    'early_stop_patience': [0],\n",
       "    'eval_freq': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [0],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0.0009050819929552008],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'loss_valid': [0],\n",
       "    'lr_decay': [0.4654631099519051],\n",
       "    'max_epochs': [0],\n",
       "    'max_steps': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_freq_downsample': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_lr_decays': [0],\n",
       "    'n_pool_kernel_size': [0],\n",
       "    'n_s_hidden': [0],\n",
       "    'n_time_in': [0],\n",
       "    'n_time_out': [0],\n",
       "    'n_x_hidden': [5.0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'random_seed': [18.0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [0],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'weight_decay': [0.0003840753699219232]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 12, 22, 19, 4, 29, 666000),\n",
       "  'refresh_time': datetime.datetime(2021, 12, 22, 19, 4, 31, 87000)}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 2.8727837, 'rmse': 4.0669765}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.trials[0]['result']['test_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_y_true': array([[52.33, 51.02, 50.24, 49.42, 50.18, 52.89, 56.46, 71.25, 74.69,\n",
       "         72.06, 69.72, 67.63, 65.92, 66.08, 68.05, 69.94, 67.68, 65.96,\n",
       "         57.86, 55.12, 53.54, 52.55, 50.67, 48.69],\n",
       "        [49.55, 48.84, 48.18, 47.82, 48.65, 50.6 , 52.14, 54.25, 55.69,\n",
       "         55.27, 55.03, 55.32, 55.71, 55.68, 55.74, 56.31, 57.19, 56.71,\n",
       "         56.27, 55.2 , 54.34, 53.22, 51.67, 50.12],\n",
       "        [48.35, 48.08, 48.31, 47.36, 47.94, 50.62, 54.38, 57.84, 58.63,\n",
       "         57.53, 56.45, 56.31, 57.39, 57.21, 57.52, 57.81, 57.88, 57.2 ,\n",
       "         55.7 , 54.31, 52.25, 51.29, 50.  , 48.12],\n",
       "        [47.21, 46.42, 46.  , 46.12, 46.49, 49.97, 52.73, 55.65, 57.04,\n",
       "         57.13, 56.98, 56.04, 56.22, 55.78, 54.6 , 54.11, 54.25, 54.69,\n",
       "         54.21, 52.79, 52.05, 51.43, 50.07, 49.01],\n",
       "        [48.39, 47.72, 47.23, 46.6 , 46.94, 47.76, 48.41, 49.44, 50.29,\n",
       "         51.52, 52.56, 53.08, 52.99, 53.13, 52.93, 53.78, 55.04, 56.52,\n",
       "         57.07, 55.52, 53.05, 52.05, 51.09, 50.47],\n",
       "        [51.49, 50.83, 50.74, 50.14, 49.94, 50.46, 50.88, 51.37, 51.61,\n",
       "         52.22, 52.8 , 53.  , 53.11, 52.93, 52.93, 53.75, 55.99, 61.2 ,\n",
       "         61.2 , 57.42, 55.61, 53.99, 53.86, 52.32],\n",
       "        [51.09, 50.19, 48.98, 48.8 , 48.52, 49.8 , 50.05, 50.55, 52.33,\n",
       "         53.26, 53.14, 52.88, 53.03, 52.46, 52.44, 52.89, 53.26, 52.61,\n",
       "         51.28, 50.72, 49.86, 49.09, 49.02, 48.1 ]], dtype=float32),\n",
       " 'test_y_hat': array([[54.58335 , 53.331837, 52.41033 , 50.52648 , 50.991657, 53.618195,\n",
       "         57.47383 , 60.160793, 61.224087, 59.173756, 59.674747, 59.745525,\n",
       "         59.81904 , 58.13237 , 56.290325, 57.930187, 59.143524, 56.69668 ,\n",
       "         59.035065, 59.29616 , 57.043205, 57.880062, 55.157604, 53.140507],\n",
       "        [51.28105 , 49.694267, 49.45726 , 48.532925, 48.383064, 50.13819 ,\n",
       "         53.630894, 57.480312, 57.82875 , 56.334938, 56.14134 , 55.507145,\n",
       "         56.523403, 54.00462 , 53.358055, 55.089375, 55.7385  , 54.32377 ,\n",
       "         57.04119 , 56.201435, 54.963646, 55.19079 , 52.20211 , 50.219723],\n",
       "        [52.42753 , 50.5889  , 49.663788, 47.87977 , 49.477917, 50.716976,\n",
       "         54.671143, 58.068016, 58.68021 , 57.237247, 57.28131 , 56.376328,\n",
       "         56.958225, 55.45016 , 54.124313, 55.73529 , 56.99061 , 55.06836 ,\n",
       "         57.76631 , 57.542847, 55.93144 , 56.298687, 53.97447 , 52.389816],\n",
       "        [48.87973 , 47.572617, 46.122536, 45.410667, 46.359516, 47.344963,\n",
       "         51.513718, 54.650394, 55.03282 , 53.44555 , 53.96023 , 52.944588,\n",
       "         53.115562, 52.386826, 50.701523, 52.47048 , 53.321423, 52.44243 ,\n",
       "         52.93713 , 53.235767, 52.814476, 51.542873, 49.975143, 48.5639  ],\n",
       "        [48.96855 , 48.26785 , 47.757137, 47.368034, 47.374043, 49.48477 ,\n",
       "         52.066765, 55.855064, 56.36388 , 54.076084, 54.22337 , 54.154503,\n",
       "         54.953438, 53.439857, 52.41001 , 53.685238, 54.110596, 52.87704 ,\n",
       "         54.36713 , 54.249523, 52.998436, 52.329826, 51.174755, 49.30695 ],\n",
       "        [51.325474, 50.195503, 49.885975, 48.067398, 49.33471 , 51.103622,\n",
       "         53.75799 , 57.713604, 58.2386  , 56.940117, 56.413536, 56.780663,\n",
       "         56.87771 , 54.68297 , 53.982075, 55.367233, 56.59761 , 54.707283,\n",
       "         57.13394 , 57.019127, 55.6057  , 55.467644, 53.65875 , 52.03377 ],\n",
       "        [54.153324, 52.736164, 52.129944, 50.955723, 51.612152, 52.2771  ,\n",
       "         56.735104, 59.814457, 60.961716, 58.819687, 59.5916  , 58.329   ,\n",
       "         58.837845, 56.98048 , 55.675568, 57.681973, 57.96444 , 57.325184,\n",
       "         59.393124, 58.89714 , 57.667706, 57.758507, 55.83545 , 53.630276]],\n",
       "       dtype=float32),\n",
       " 'test_mask': array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32),\n",
       " 'test_meta_data': [array([['NP', Timestamp('2013-01-01 00:00:00')],\n",
       "         ['NP', Timestamp('2013-01-01 01:00:00')],\n",
       "         ['NP', Timestamp('2013-01-01 02:00:00')],\n",
       "         ...,\n",
       "         ['NP', Timestamp('2018-12-24 21:00:00')],\n",
       "         ['NP', Timestamp('2018-12-24 22:00:00')],\n",
       "         ['NP', Timestamp('2018-12-24 23:00:00')]], dtype=object)]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.trials[0]['result']['forecasts_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('nixtla': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
