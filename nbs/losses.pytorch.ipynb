{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp losses.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# PyTorch Losses\n",
    "\n",
    "> NeuralForecast contains a collection PyTorch Loss classes aimed to be used during the models' optimization. The most important train signal is the forecast error, which is the difference between the observed value $y_{\\tau}$ and the prediction $\\hat{y}_{\\tau}$, at time $y_{\\tau}$:$$e_{\\tau} = y_{\\tau}-\\hat{y}_{\\tau} \\qquad \\qquad \\tau \\in \\{t+1,\\dots,t+H \\}$$ The train loss summarizes the forecast errors in different train optimization objectives.<br><br>All the losses are `torch.nn.modules` which helps to automatically moved them across CPU/GPU/TPU devices with Pytorch Lightning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa68dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Union\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e07e98-b4c8-4ade-b3b6-1d27f367aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _divide_no_nan(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Auxiliary funtion to handle divide by 0\n",
    "    \"\"\"\n",
    "    div = a / b\n",
    "    div[div != div] = 0.0\n",
    "    div[div == float('inf')] = 0.0\n",
    "    return div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a94d7d",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\">1. Scale-dependent Errors </span>\n",
    "\n",
    "These metrics are on the same scale as the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc4679",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e413fae-c590-4713-aab9-37c61ed37dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MAE(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Mean Absolute Error\n",
    "\n",
    "        Calculates Mean Absolute Error between\n",
    "        `y` and `y_hat`. MAE measures the relative prediction\n",
    "        accuracy of a forecasting method by calculating the\n",
    "        deviation of the prediction and the true\n",
    "        value at a given time and averages these devations\n",
    "        over the length of the series.\n",
    "        \n",
    "        $$ \\mathrm{MAE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} |y_{\\\\tau} - \\hat{y}_{\\\\tau}| $$\n",
    "        \"\"\"\n",
    "        super(MAE, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.output_names = ['']\n",
    "        \n",
    "    def adapt_output(self, y_pred):\n",
    "        return y_pred\n",
    "\n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "        \n",
    "        **Returns:**<br>\n",
    "        `mae`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(y)\n",
    "\n",
    "        mae = torch.abs(y - y_hat) * mask\n",
    "        mae = torch.mean(mae)\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d004cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MAE, name='MAE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a20a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MAE.__call__, name='MAE', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0292c74d",
   "metadata": {},
   "source": [
    "![](imgs_losses/mae_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f31cc3d",
   "metadata": {},
   "source": [
    "## Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MSE(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"  Mean Squared Error\n",
    "\n",
    "        Calculates Mean Squared Error between\n",
    "        `y` and `y_hat`. MSE measures the relative prediction\n",
    "        accuracy of a forecasting method by calculating the \n",
    "        squared deviation of the prediction and the true\n",
    "        value at a given time, and averages these devations\n",
    "        over the length of the series.\n",
    "        \n",
    "        $$ \\mathrm{MSE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} (y_{\\\\tau} - \\hat{y}_{\\\\tau})^{2} $$\n",
    "        \"\"\"\n",
    "        super(MSE, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.output_names = ['']\n",
    "        \n",
    "    def adapt_output(self, y_pred):\n",
    "        return y_pred\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mse`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        mse = (y - y_hat)**2\n",
    "        mse = mask * mse\n",
    "        mse = torch.mean(mse)\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c65b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MSE, name='MSE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0126a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MSE.__call__, name='MSE', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23f9c1",
   "metadata": {},
   "source": [
    "![](imgs_losses/mse_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b160140b",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RMSE(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Root Mean Squared Error\n",
    "\n",
    "        Calculates Root Mean Squared Error between\n",
    "        `y` and `y_hat`. RMSE measures the relative prediction\n",
    "        accuracy of a forecasting method by calculating the squared deviation\n",
    "        of the prediction and the observed value at a given time and\n",
    "        averages these devations over the length of the series.\n",
    "        Finally the RMSE will be in the same scale\n",
    "        as the original time series so its comparison with other\n",
    "        series is possible only if they share a common scale. \n",
    "        RMSE has a direct connection to the L2 norm.\n",
    "        \n",
    "        $$ \\mathrm{RMSE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\sqrt{\\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} (y_{\\\\tau} - \\hat{y}_{\\\\tau})^{2}} $$\n",
    "        \"\"\"\n",
    "        super(RMSE, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.output_names = ['']\n",
    "        \n",
    "    def adapt_output(self, y_pred):\n",
    "        return y_pred\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `rmse`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        mse = (y - y_hat)**2\n",
    "        mse = mask * mse\n",
    "        mse = torch.mean(mse)\n",
    "        mse = torch.sqrt(mse)\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MSE, name='RMSE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d398d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RMSE.__call__, name='RMSE', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4539e38",
   "metadata": {},
   "source": [
    "![](imgs_losses/rmse_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf5488",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 2. Percentage errors </span>\n",
    "\n",
    "These metrics are unit-free, suitable for comparisons across series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab97ec",
   "metadata": {},
   "source": [
    "## Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbddc1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MAPE(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Mean Absolute Percentage Error\n",
    "\n",
    "        Calculates Mean Absolute Percentage Error  between\n",
    "        `y` and `y_hat`. MAPE measures the relative prediction\n",
    "        accuracy of a forecasting method by calculating the percentual deviation\n",
    "        of the prediction and the observed value at a given time and\n",
    "        averages these devations over the length of the series.\n",
    "        The closer to zero an observed value is, the higher penalty MAPE loss\n",
    "        assigns to the corresponding error.\n",
    "        \n",
    "        $$ \\mathrm{MAPE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{|y_{\\\\tau}|} $$\n",
    "        \"\"\"\n",
    "        super(MAPE, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.output_names = ['']\n",
    "        \n",
    "    def adapt_output(self, y_pred):\n",
    "        return y_pred\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mape`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        mask = _divide_no_nan(mask, torch.abs(y))\n",
    "        mape = torch.abs(y - y_hat) * mask\n",
    "        mape = torch.mean(mape)\n",
    "        return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MAPE, name='MAPE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MAPE, name='MAPE.__call__', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccdc69",
   "metadata": {},
   "source": [
    "![](imgs_losses/mape_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb245891",
   "metadata": {},
   "source": [
    "## SMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71497ee-4485-4a17-97d9-81e324fade3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SMAPE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" Symmetric Mean Absolute Percentage Error\n",
    "        \n",
    "        Calculates Symmetric Mean Absolute Percentage Error between\n",
    "        `y` and `y_hat`. SMAPE measures the relative prediction\n",
    "        accuracy of a forecasting method by calculating the relative deviation\n",
    "        of the prediction and the observed value scaled by the sum of the\n",
    "        absolute values for the prediction and observed value at a\n",
    "        given time, then averages these devations over the length\n",
    "        of the series. This allows the SMAPE to have bounds between\n",
    "        0% and 200% which is desireble compared to normal MAPE that\n",
    "        may be undetermined when the target is zero.\n",
    "        \n",
    "        $$ \\mathrm{sMAPE}_{2}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{|y_{\\\\tau}|+|\\hat{y}_{\\\\tau}|} $$\n",
    "        \n",
    "        **References:**<br>\n",
    "        [Makridakis S., \"Accuracy measures: theoretical and practical concerns\".](https://www.sciencedirect.com/science/article/pii/0169207093900793)\n",
    "        \"\"\"\n",
    "        super(SMAPE, self).__init__()\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.output_names = ['']\n",
    "        \n",
    "    def adapt_output(self, y_pred):\n",
    "        return y_pred\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `smape`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        delta_y = torch.abs((y - y_hat))\n",
    "        scale = torch.abs(y) + torch.abs(y_hat)\n",
    "        smape = _divide_no_nan(delta_y, scale)\n",
    "        smape = smape * mask\n",
    "        smape = 2 * torch.mean(smape)\n",
    "        return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee99fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SMAPE, name='SMAPE.__init__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SMAPE.__call__, name='SMAPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f2d6f",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 3. Scale-independent Errors </span>\n",
    "\n",
    "These metrics measure the relative improvements versus baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2dee1f",
   "metadata": {},
   "source": [
    "## Mean Absolute Scaled Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7618c9-648a-41d5-9d17-dd7027a452ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MASE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, seasonality: int):\n",
    "        \"\"\" Mean Absolute Scaled Error \n",
    "        Calculates the Mean Absolute Scaled Error between\n",
    "        `y` and `y_hat`. MASE measures the relative prediction\n",
    "        accuracy of a forecasting method by comparinng the mean absolute errors\n",
    "        of the prediction and the observed value against the mean\n",
    "        absolute errors of the seasonal naive model.\n",
    "        The MASE partially composed the Overall Weighted Average (OWA), \n",
    "        used in the M4 Competition.\n",
    "        \n",
    "        $$ \\mathrm{MASE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{season}_{\\\\tau}) = \\\\frac{1}{H} \\sum^{t+H}_{\\\\tau=t+1} \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{\\mathrm{MAE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{season}_{\\\\tau})} $$\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `seasonality`: int. Main frequency of the time series; Hourly 24,  Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.\n",
    "        \n",
    "        **References:**<br>\n",
    "        [Rob J. Hyndman, & Koehler, A. B. \"Another look at measures of forecast accuracy\".](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n",
    "        [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, \"The M4 Competition: 100,000 time series and 61 forecasting methods\".](https://www.sciencedirect.com/science/article/pii/S0169207019301128)\n",
    "        \"\"\"\n",
    "        super(MASE, self).__init__()\n",
    "        self.seasonality = seasonality\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.output_names = ['']\n",
    "        \n",
    "    def adapt_output(self, y_pred):\n",
    "        return y_pred\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor,  y_insample: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor (batch_size, output_size), Actual values.<br>\n",
    "        `y_hat`: tensor (batch_size, output_size)), Predicted values.<br>\n",
    "        `y_insample`: tensor (batch_size, input_size), Actual insample Seasonal Naive predictions.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mase`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        delta_y = torch.abs(y - y_hat)\n",
    "        scale = torch.mean(torch.abs(y_insample[:, self.seasonality:] - \\\n",
    "                                     y_insample[:, :-self.seasonality]), axis=1)\n",
    "        mase = _divide_no_nan(delta_y, scale[:, None])\n",
    "        mase = mase * mask\n",
    "        mase = torch.mean(mase)\n",
    "        return mase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MASE, name='MASE.__init__', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MASE.__call__, name='MASE', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c8fe5",
   "metadata": {},
   "source": [
    "![](imgs_losses/mase_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828438e",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkBlue\"> 4. Probabilistic Errors </span>\n",
    "\n",
    "These measure absolute deviation non-symmetrically, that produce under/over estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d8cb2",
   "metadata": {},
   "source": [
    "## QuantileLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class QuantileLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, q):\n",
    "        \"\"\" Quantile Loss\n",
    "\n",
    "        Computes the quantile loss between `y` and `y_hat`.\n",
    "        QL measures the deviation of a quantile forecast.\n",
    "        By weighting the absolute deviation in a non symmetric way, the\n",
    "        loss pays more attention to under or over estimation.\n",
    "        A common value for q is 0.5 for the deviation from the median (Pinball loss).\n",
    "\n",
    "        $$ \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q)}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\\\tau} - y_{\\\\tau} )_{+} + q\\,( y_{\\\\tau} - \\hat{y}^{(q)}_{\\\\tau} )_{+} \\Big) $$\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `q`: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level.<br>\n",
    "\n",
    "        **References:**<br>\n",
    "        [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)\n",
    "        \"\"\"\n",
    "        super(QuantileLoss, self).__init__()\n",
    "        self.q = q\n",
    "        self.outputsize_multiplier = 1\n",
    "        self.output_names = [f'_ql{q}']\n",
    "        \n",
    "    def adapt_output(self, y_pred):\n",
    "        return y_pred\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `quantile_loss`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        delta_y = y - y_hat\n",
    "        loss = torch.max(torch.mul(self.q, delta_y), torch.mul((self.q - 1), delta_y))\n",
    "        loss = loss * mask\n",
    "        quantile_loss = torch.mean(loss)\n",
    "        return quantile_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(QuantileLoss, name='QuantileLoss.__init__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1588e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(QuantileLoss.__call__, name='QuantileLoss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac874f",
   "metadata": {},
   "source": [
    "![](imgs_losses/q_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dbb002",
   "metadata": {},
   "source": [
    "## MQLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def level_to_outputs(level):\n",
    "    qs = sum([[50-l/2, 50+l/2] for l in level], [])\n",
    "    output_names = sum([[f'-lo-{l}', f'-hi-{l}'] for l in level], [])\n",
    "\n",
    "    sort_idx = np.argsort(qs)\n",
    "    quantiles = np.array(qs)[sort_idx]\n",
    "\n",
    "    # Add default median\n",
    "    quantiles = np.concatenate([np.array([50]), quantiles])\n",
    "    quantiles = torch.Tensor(quantiles) / 100\n",
    "    output_names = list(np.array(output_names)[sort_idx])\n",
    "    output_names.insert(0, '-median')\n",
    "    \n",
    "    return quantiles, output_names\n",
    "\n",
    "def quantiles_to_outputs(quantiles):\n",
    "    output_names = []\n",
    "    for q in quantiles:\n",
    "        if q<.50:\n",
    "            output_names.append(f'-lo-{np.round(100-200*q,2)}')\n",
    "        elif q>.50:\n",
    "            output_names.append(f'-hi-{np.round(100-200*(1-q),2)}')\n",
    "        else:\n",
    "            output_names.append('-median')\n",
    "    return quantiles, output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MQLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, level=[80, 90], quantiles=None):\n",
    "        \"\"\"  Multi-Quantile loss\n",
    "    \n",
    "        Calculates the Multi-Quantile loss (MQL) between `y` and `y_hat`.\n",
    "        MQL calculates the average multi-quantile Loss for\n",
    "        a given set of quantiles, based on the absolute \n",
    "        difference between predicted quantiles and observed values.\n",
    "        \n",
    "        $$ \\mathrm{MQL}(\\\\mathbf{y}_{\\\\tau},[\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \\\\frac{1}{n} \\\\sum_{q_{i}} \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau}) $$\n",
    "        \n",
    "        The limit behavior of MQL allows to measure the accuracy \n",
    "        of a full predictive distribution $\\mathbf{\\hat{F}}_{\\\\tau}$ with \n",
    "        the continuous ranked probability score (CRPS). This can be achieved \n",
    "        through a numerical integration technique, that discretizes the quantiles \n",
    "        and treats the CRPS integral with a left Riemann approximation, averaging over \n",
    "        uniformly distanced quantiles.    \n",
    "        \n",
    "        $$ \\mathrm{CRPS}(y_{\\\\tau}, \\mathbf{\\hat{F}}_{\\\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\\\tau}, \\hat{y}^{(q)}_{\\\\tau}) dq $$\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
    "        `quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
    "\n",
    "        **References:**<br>\n",
    "        [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)<br>\n",
    "        [James E. Matheson and Robert L. Winkler, \"Scoring Rules for Continuous Probability Distributions\".](https://www.jstor.org/stable/2629907)\n",
    "        \"\"\"\n",
    "        super(MQLoss, self).__init__()\n",
    "        # Transform level to MQLoss parameters\n",
    "        if level:\n",
    "            qs, self.output_names = level_to_outputs(level)\n",
    "            quantiles = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "            quantiles = torch.Tensor(quantiles)\n",
    "\n",
    "        self.quantiles = torch.nn.Parameter(quantiles, requires_grad=False)\n",
    "        self.outputsize_multiplier = len(self.output_names)\n",
    "\n",
    "    def adapt_output(self, y_pred):\n",
    "        # Model output is adapted to have quantiles in the last dim\n",
    "        all_but_last_dims = y_pred.size()[:-1]\n",
    "        last_dim = y_pred.size()[-1]\n",
    "        y_pred = y_pred.view(*all_but_last_dims, \n",
    "                             last_dim//self.outputsize_multiplier,\n",
    "                             self.outputsize_multiplier)\n",
    "        return y_pred\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mqloss`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        n_q = len(self.quantiles)\n",
    "        \n",
    "        error  = y_hat - y.unsqueeze(-1)\n",
    "        sq     = torch.maximum(-error, torch.zeros_like(error))\n",
    "        s1_q   = torch.maximum(error, torch.zeros_like(error))\n",
    "        mqloss = (self.quantiles * sq + (1 - self.quantiles) * s1_q)\n",
    "            \n",
    "        # Match y/weights dimensions and compute weighted average\n",
    "        mask = mask / torch.sum(mask)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        mqloss = (1/n_q) * mqloss * mask\n",
    "        return torch.sum(mqloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MQLoss, name='MQLoss.__init__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac2237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MQLoss.__call__, name='MQLoss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b66b0e",
   "metadata": {},
   "source": [
    "![](imgs_losses/mq_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65d575",
   "metadata": {},
   "source": [
    "## wMQ Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d34c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class wMQLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, level=[80, 90], quantiles=None):\n",
    "        \"\"\" Weighted Multi-Quantile loss\n",
    "        \n",
    "        Calculates the Weighted Multi-Quantile loss (WMQL) between `y` and `y_hat`.\n",
    "        WMQL calculates the weighted average multi-quantile Loss for\n",
    "        a given set of quantiles, based on the absolute \n",
    "        difference between predicted quantiles and observed values.  \n",
    "            \n",
    "        $$ \\mathrm{wMQL}(\\\\mathbf{y}_{\\\\tau},[\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \\\\frac{1}{n} \\\\sum_{q_{i}} \\\\frac{\\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau})}{\\\\sum^{t+H}_{\\\\tau=t+1} |y_{\\\\tau}|} $$\n",
    "        \n",
    "        **Parameters:**<br>\n",
    "        `level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n",
    "        `quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n",
    "\n",
    "        **References:**<br>\n",
    "        [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)<br>\n",
    "        [James E. Matheson and Robert L. Winkler, \"Scoring Rules for Continuous Probability Distributions\".](https://www.jstor.org/stable/2629907)\n",
    "        \"\"\"\n",
    "        super(wMQLoss, self).__init__()\n",
    "        # Transform level to MQLoss parameters\n",
    "        if level:\n",
    "            qs, self.output_names = level_to_outputs(level)\n",
    "            quantiles = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "            quantiles = torch.Tensor(quantiles)\n",
    "\n",
    "        self.quantiles = torch.nn.Parameter(quantiles, requires_grad=False)\n",
    "        self.outputsize_multiplier = len(self.output_names)\n",
    "\n",
    "    def adapt_output(self, y_pred):\n",
    "        # Model output is adapted to have quantiles in the last dim\n",
    "        all_but_last_dims = y_pred.size()[:-1]\n",
    "        last_dim = y_pred.size()[-1]\n",
    "        y_pred = y_pred.view(*all_but_last_dims, \n",
    "                             last_dim//self.outputsize_multiplier,\n",
    "                             self.outputsize_multiplier)\n",
    "        return y_pred\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor, y_hat: torch.Tensor, \n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \"\"\"\n",
    "        **Parameters:**<br>\n",
    "        `y`: tensor, Actual values.<br>\n",
    "        `y_hat`: tensor, Predicted values.<br>\n",
    "        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `mqloss`: tensor (single value).\n",
    "        \"\"\"\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y_hat)\n",
    "\n",
    "        error = y_hat - y.unsqueeze(-1)\n",
    "        \n",
    "        sq = torch.maximum(-error, torch.zeros_like(error))\n",
    "        s1_q = torch.maximum(error, torch.zeros_like(error))\n",
    "        loss = (self.quantiles * sq + (1 - self.quantiles) * s1_q)\n",
    "        \n",
    "        wmqloss = _divide_no_nan(torch.sum(loss * mask, axis=-2), \n",
    "                                 torch.sum(torch.abs(y.unsqueeze(-1)) * mask, axis=-2))\n",
    "        return torch.mean(wmqloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d916af",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(wMQLoss, name='wMQLoss.__init__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bdcbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(wMQLoss.__call__, name='wMQLoss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da37f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# check = MQLoss(level=[80, 90])\n",
    "check = MQLoss(quantiles=[0.0500, 0.1000, 0.50, 0.9000, 0.9500])\n",
    "print(check.output_names)\n",
    "print(check.quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f459b8",
   "metadata": {},
   "source": [
    "## Poisson Mixture Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PMM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, level=[80, 90], quantiles=None):\n",
    "        \"\"\" Poisson Mixture Mesh\n",
    "\n",
    "        This Poisson Mixture statistical model assumes independence across groups of \n",
    "        data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
    "\n",
    "        $$ P\\\\left(\\mathbf{y}_{[b][t+1:t+H]}\\\\right) = \n",
    "        \\prod_{ [g_{i}] \\in \\mathcal{G}} P \\\\left(\\mathbf{y}_{[g_{i}][\\\\tau]} \\\\right) =\n",
    "        \\prod_{\\\\beta\\in[g_{i}]} \n",
    "        \\\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\\\beta,\\\\tau) \\in [g_i][t+1:t+H]} \\mathrm{Poisson}(y_{\\\\beta,\\\\tau}, \\hat{\\\\lambda}_{\\\\beta,\\\\tau,k}) \\\\right)$$\n",
    "\n",
    "        **References:**<br>\n",
    "        [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
    "        Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
    "        Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)\n",
    "        \"\"\"\n",
    "        super(PMM, self).__init__()\n",
    "        # Transform level to MQLoss parameters\n",
    "        if level:\n",
    "            qs, self.output_names = level_to_outputs(level)\n",
    "            quantiles = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "            quantiles = torch.Tensor(quantiles)\n",
    "\n",
    "        self.quantiles = torch.nn.Parameter(quantiles, requires_grad=False)\n",
    "        self.outputsize_multiplier = len(self.output_names)\n",
    "\n",
    "    def sample(self, weights, lambdas, num_samples=500):        \n",
    "        B, H, K = lambdas.size()\n",
    "        Q = len(self.quantiles)\n",
    "\n",
    "        # Sample K ~ Mult(weights)\n",
    "        # shared across B, H\n",
    "        weights = torch.repeat_interleave(input=weights, repeats=H, dim=2)\n",
    "\n",
    "        # Avoid loop, vectorize\n",
    "        weights = weights.reshape(-1, K)\n",
    "        lambdas = lambdas.flatten()        \n",
    "\n",
    "        # Vectorization trick to recover row_idx\n",
    "        sample_idxs = torch.multinomial(input=weights, \n",
    "                                        num_samples=num_samples,\n",
    "                                        replacement=True)\n",
    "        aux_col_idx = torch.unsqueeze(torch.arange(B*H),-1) * K\n",
    "\n",
    "        # To device\n",
    "        sample_idxs = sample_idxs.to(lambdas.device)\n",
    "        aux_col_idx = aux_col_idx.to(lambdas.device)\n",
    "\n",
    "        sample_idxs = sample_idxs + aux_col_idx\n",
    "        sample_idxs = sample_idxs.flatten()\n",
    "\n",
    "        sample_lambdas = lambdas[sample_idxs]\n",
    "\n",
    "        # Sample y ~ Poisson(lambda) independently\n",
    "        samples = torch.poisson(sample_lambdas).to(lambdas.device)\n",
    "        samples = samples.view(B*H, num_samples)\n",
    "\n",
    "        # Compute quantiles\n",
    "        quantiles_device = self.quantiles.to(lambdas.device)\n",
    "        quants = torch.quantile(input=samples, q=quantiles_device, dim=1)\n",
    "        quants = quants.permute((1,0)) # Q, B*H\n",
    "\n",
    "        # Final reshapes\n",
    "        samples = samples.view(B, H, num_samples)\n",
    "        quants  = quants.view(B, H, Q)\n",
    "\n",
    "        return samples, quants        \n",
    "    \n",
    "    def neglog_likelihood(self,\n",
    "                          y: torch.Tensor,\n",
    "                          weights: torch.Tensor,\n",
    "                          lambdas: torch.Tensor,\n",
    "                          mask: Union[torch.Tensor, None] = None):\n",
    "\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(y)\n",
    "\n",
    "        B, H, K = lambdas.size()\n",
    "        eps  = 1e-10\n",
    "        \n",
    "        log = y * torch.log(lambdas + eps) - lambdas\\\n",
    "              - ( (y) * torch.log(y + eps) - y )   # Stirling's Factorial\n",
    "\n",
    "        #log  = torch.sum(log, dim=0, keepdim=True) # Joint within batch/group\n",
    "        #log  = torch.sum(log, dim=1, keepdim=True) # Joint within horizon\n",
    "        \n",
    "        # Numerical stability mixture and loglik\n",
    "        log_max = torch.amax(log, dim=2, keepdim=True) # [1,1,K] (collapsed joints)\n",
    "        lik     = weights * torch.exp(log-log_max)     # Take max\n",
    "        loglik  = torch.log(torch.sum(lik, dim=2, keepdim=True)) + log_max # Return max\n",
    "        loglik  = loglik * mask #replace with mask\n",
    "        \n",
    "        loss = -torch.mean(loglik)\n",
    "        return loss\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor,\n",
    "                 weights: torch.Tensor,\n",
    "                 lambdas: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "        \n",
    "        return self.neglog_likelihood(y=y, weights=weights, lambdas=lambdas, mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create single mixture and broadcast to N,H,K\n",
    "weights = torch.ones((1,3))[None, :, :]\n",
    "lambdas = torch.Tensor([[5,10,15], [10,20,30]])[None, :, :]\n",
    "\n",
    "# # Create repetitions for the batch dimension N.\n",
    "N=2\n",
    "weights = torch.repeat_interleave(input=weights, repeats=N, dim=0)\n",
    "lambdas = torch.repeat_interleave(input=lambdas, repeats=N, dim=0)\n",
    "\n",
    "print('weights.shape (N,H,K) \\t', weights.shape)\n",
    "print('lambdas.shape (N,H,K) \\t', lambdas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PMM(quantiles=[0.1, 0.40, 0.5, 0.60, 0.9])\n",
    "\n",
    "samples, quants = model.sample(weights=weights,\n",
    "                               lambdas=lambdas, num_samples=500)\n",
    "\n",
    "print('samples.shape (N,H,num_samples) ', samples.shape)\n",
    "print('quants.shape  (N,H,Q) \\t\\t', quants.shape)\n",
    "\n",
    "# Plot synthethic data\n",
    "x_plot = range(quants.shape[1]) # H length\n",
    "y_plot_hat = quants[0,:,:]  # Filter N,G,T -> H,Q\n",
    "samples_hat = samples[0,:,:]  # Filter N,G,T -> H,num_samples\n",
    "\n",
    "# Kernel density plot for single forecast horizon \\tau = t+1\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "\n",
    "ax.hist(samples_hat[0,:], alpha=0.5, label=r'Horizon $\\tau+1$')\n",
    "ax.hist(samples_hat[1,:], alpha=0.5, label=r'Horizon $\\tau+2$')\n",
    "ax.set(xlabel='Y values', ylabel='Probability')\n",
    "plt.title('Single horizon Distributions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot simulated trajectory\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "plt.plot(x_plot, y_plot_hat[:,2], color='black', label='median [q50]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,1], y2=y_plot_hat[:,3],\n",
    "                 facecolor='blue', alpha=0.4, label='[p25-p75]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,0], y2=y_plot_hat[:,4],\n",
    "                 facecolor='blue', alpha=0.2, label='[p1-p99]')\n",
    "ax.set(xlabel='Horizon', ylabel='Y values')\n",
    "plt.title('PMM Probabilistic Predictions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e0dd4",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GMM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, level=[80, 90], quantiles=None):\n",
    "        \"\"\" Gaussian Mixture Mesh\n",
    "\n",
    "        This Gaussian Mixture statistical model assumes independence across groups of \n",
    "        data $\\mathcal{G}=\\{[g_{i}]\\}$, and estimates relationships within the group.\n",
    "\n",
    "        $$ P\\\\left(\\mathbf{y}_{[b][t+1:t+H]}\\\\right) = \n",
    "        \\prod_{ [g_{i}] \\in \\mathcal{G}} P\\left(\\mathbf{y}_{[g_{i}][\\\\tau]}\\\\right)=\n",
    "        \\prod_{\\\\beta\\in[g_{i}]}\n",
    "        \\\\left(\\sum_{k=1}^{K} w_k \\prod_{(\\\\beta,\\\\tau) \\in [g_i][t+1:t+H]} \n",
    "        \\mathrm{Gaussian}(y_{\\\\beta,\\\\tau}, \\hat{\\mu}_{\\\\beta,\\\\tau,k}, \\sigma_{\\\\beta,\\\\tau,k})\\\\right)$$\n",
    "\n",
    "        **References:**<br>\n",
    "        [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker. \n",
    "        Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. Submitted to the International \n",
    "        Journal Forecasting, Working paper available at arxiv.](https://arxiv.org/pdf/2110.13179.pdf)\n",
    "        \"\"\"\n",
    "        super(GMM, self).__init__()\n",
    "        # Transform level to MQLoss parameters\n",
    "        if level:\n",
    "            qs, self.output_names = level_to_outputs(level)\n",
    "            quantiles = torch.Tensor(qs)\n",
    "\n",
    "        # Transform quantiles to homogeneus output names\n",
    "        if quantiles is not None:\n",
    "            _, self.output_names = quantiles_to_outputs(quantiles)\n",
    "            quantiles = torch.Tensor(quantiles)\n",
    "\n",
    "        self.quantiles = torch.nn.Parameter(quantiles, requires_grad=False)\n",
    "        self.outputsize_multiplier = len(self.output_names)\n",
    "\n",
    "    def sample(self, weights, means, stds, num_samples=500):\n",
    "        B, H, K = means.size()\n",
    "        Q = len(self.quantiles)\n",
    "        assert means.shape == stds.shape\n",
    "\n",
    "        # Sample K ~ Mult(weights)\n",
    "        # shared across B, H\n",
    "        # weights = torch.repeat_interleave(input=weights, repeats=H, dim=2)\n",
    "\n",
    "        # Avoid loop, vectorize\n",
    "        weights = weights.reshape(-1, K)\n",
    "        means = means.flatten()\n",
    "        stds = stds.flatten()\n",
    "\n",
    "        # Vectorization trick to recover row_idx\n",
    "        sample_idxs = torch.multinomial(input=weights, \n",
    "                                        num_samples=num_samples,\n",
    "                                        replacement=True)\n",
    "        aux_col_idx = torch.unsqueeze(torch.arange(B*H),-1) * K\n",
    "\n",
    "        # To device\n",
    "        sample_idxs = sample_idxs.to(means.device)\n",
    "        aux_col_idx = aux_col_idx.to(means.device)\n",
    "\n",
    "        sample_idxs = sample_idxs + aux_col_idx\n",
    "        sample_idxs = sample_idxs.flatten()\n",
    "\n",
    "        sample_means = means[sample_idxs]\n",
    "        sample_stds  = stds[sample_idxs]\n",
    "\n",
    "        # Sample y ~ Normal(mu, std) independently\n",
    "        samples = torch.normal(sample_means, sample_stds).to(means.device)\n",
    "        samples = samples.view(B*H, num_samples)\n",
    "\n",
    "        # Compute quantiles\n",
    "        quantiles_device = self.quantiles.to(means.device)\n",
    "        quants = torch.quantile(input=samples, q=quantiles_device, dim=1)\n",
    "        quants = quants.permute((1,0)) # Q, B*H\n",
    "\n",
    "        # Final reshapes\n",
    "        samples = samples.view(B, H, num_samples)\n",
    "        quants  = quants.view(B, H, Q)\n",
    "\n",
    "        return samples, quants        \n",
    "    \n",
    "    def neglog_likelihood(self,\n",
    "                          y: torch.Tensor,\n",
    "                          weights: torch.Tensor,\n",
    "                          means: torch.Tensor,\n",
    "                          stds: torch.Tensor,\n",
    "                          mask: Union[torch.Tensor, None] = None):\n",
    "\n",
    "        if mask is None: \n",
    "            mask = torch.ones_like(means)\n",
    "\n",
    "        B, H, K = means.size()\n",
    "        # eps  = 1e-10\n",
    "        \n",
    "        log  = -0.5 * ((1/stds)*(y - means))**2\\\n",
    "                - torch.log(((2*math.pi)**(0.5)) * stds)\n",
    "\n",
    "        #log  = torch.sum(log, dim=0, keepdim=True) # Joint within batch/group\n",
    "        #log  = torch.sum(log, dim=1, keepdim=True) # Joint within horizon\n",
    "\n",
    "        # Numerical stability mixture and loglik\n",
    "        log_max = torch.amax(log, dim=2, keepdim=True) # [1,1,K] (collapsed joints)\n",
    "        lik     = weights * torch.exp(log-log_max)     # Take max\n",
    "        loglik  = torch.log(torch.sum(lik, dim=2, keepdim=True)) + log_max # Return max\n",
    "        \n",
    "        loglik  = loglik * mask #replace with mask\n",
    "\n",
    "        loss = -torch.mean(loglik)\n",
    "        return loss\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor,\n",
    "                 weights: torch.Tensor,\n",
    "                 means: torch.Tensor,\n",
    "                 stds: torch.Tensor,\n",
    "                 mask: Union[torch.Tensor, None] = None):\n",
    "\n",
    "        return self.neglog_likelihood(y=y, weights=weights,\n",
    "                                 means=means, stds=stds, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5493e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create single mixture and broadcast to N,H,K\n",
    "means   = torch.Tensor([[5,10,15], [10,20,30]])[None, :, :]\n",
    "\n",
    "# # Create repetitions for the batch dimension N.\n",
    "N=2\n",
    "means = torch.repeat_interleave(input=means, repeats=N, dim=0)\n",
    "weights = torch.ones_like(means)\n",
    "stds  = torch.ones_like(means)\n",
    "\n",
    "print('weights.shape (N,H,K) \\t', weights.shape)\n",
    "print('means.shape (N,H,K) \\t', means.shape)\n",
    "print('stds.shape (N,H,K) \\t', stds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d2382",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GMM(quantiles=[0.1, 0.40, 0.5, 0.60, 0.9])\n",
    "\n",
    "samples, quants = model.sample(weights=weights,\n",
    "                               means=means, stds=stds,\n",
    "                               num_samples=2000)\n",
    "\n",
    "print('samples.shape (N,H,num_samples) ', samples.shape)\n",
    "print('quants.shape  (N,H,Q) \\t\\t', quants.shape)\n",
    "\n",
    "# Plot synthethic data\n",
    "x_plot = range(quants.shape[1]) # H length\n",
    "y_plot_hat = quants[0,:,:]  # Filter N,G,T -> H,Q\n",
    "samples_hat = samples[0,:,:]  # Filter N,G,T -> H,num_samples\n",
    "\n",
    "# Kernel density plot for single forecast horizon \\tau = t+1\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "\n",
    "ax.hist(samples_hat[0,:], alpha=0.5, bins=50,\n",
    "        label=r'Horizon $\\tau+1$')\n",
    "ax.hist(samples_hat[1,:], alpha=0.5, bins=50,\n",
    "        label=r'Horizon $\\tau+2$')\n",
    "ax.set(xlabel='Y values', ylabel='Probability')\n",
    "plt.title('Single horizon Distributions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot simulated trajectory\n",
    "fig, ax = plt.subplots(figsize=(3.7, 2.9))\n",
    "plt.plot(x_plot, y_plot_hat[:,2], color='black', label='median [q50]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,1], y2=y_plot_hat[:,3],\n",
    "                 facecolor='blue', alpha=0.4, label='[p25-p75]')\n",
    "plt.fill_between(x_plot,\n",
    "                 y1=y_plot_hat[:,0], y2=y_plot_hat[:,4],\n",
    "                 facecolor='blue', alpha=0.2, label='[p1-p99]')\n",
    "ax.set(xlabel='Horizon', ylabel='Y values')\n",
    "plt.title('PMM Probabilistic Predictions')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f724a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "neuralforecast"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
