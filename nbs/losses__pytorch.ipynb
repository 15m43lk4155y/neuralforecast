{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Losses\n",
    "\n",
    "> The most important train signal is the forecast error, which is the difference between the observed value $y_{\\\\tau}$ and the prediction $\\\\hat{y}_{\\\\tau}$, at time $\\\\tau$: $$e_{\\\\tau} = y_{\\\\tau}-\\\\hat{y}_{\\\\tau} \\\\qquad \\\\qquad \\\\tau \\\\in \\\\{t+1,\\\\dots,t+H \\\\}.$$The train loss summarizes the forecast errors in different train objectives: <br><br> 1. Scale-dependent errors - These metrics are on the same scale as the data. <ul><li>MAELoss</li><li>MSELoss</li><li>RMSELoss</li></ul>   <br> 2. Percentage errors - These metrics are unit-free, suitable for comparisons across series. <ul><li>MAPELoss</li><li>sMAPELoss</li></ul>. <br> 3. Scale-independent errors - These metrics measure the relative improvements versus baselines, the available metric is <ul><li>MASELoss</li><li>RMAELoss</li></ul>   <br>4. Probabilistic errors - These measure absolute deviation non-symmetrically, that produce under/over estimation. <ul><li>QuantileLoss</li><li>MQLoss</li><li>wMQLoss</li></ul>. <br> 5. Other errors - Aditionally, two loss functions related to the M4 competition winner, ESRNN. <ul><li>LevelVariabilityLoss</li><li>SmylLoss (QL+LevelLoss)</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch as t\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from IPython.display import Image\n",
    "WIDTH = 600\n",
    "HEIGHT = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _divide_no_nan(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Auxiliary funtion to handle divide by 0\n",
    "    \"\"\"\n",
    "    div = a / b\n",
    "    div[div != div] = 0.0\n",
    "    div[div == float('inf')] = 0.0\n",
    "    return div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkOrange\">1. Scale-dependent Errors </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def MAELoss(y: t.Tensor, y_hat: t.Tensor, mask: t.Tensor =None) -> t.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Calculates Mean Absolute Error (MAE) between\n",
    "    y and y_hat. MAE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the\n",
    "    deviation of the prediction and the true\n",
    "    value at a given time and averages these devations\n",
    "    over the length of the series.\n",
    "    \n",
    "    $$ \\mathrm{MAE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \n",
    "        \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \n",
    "        |y_{\\\\tau} - \\hat{y}_{\\\\tau}| $$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: tensor (batch_size, output_size).\n",
    "            Aactual values in torch tensor.\n",
    "        y_hat: tensor (batch_size, output_size).\n",
    "            Predicted values in torch tensor.\n",
    "        mask: tensor (batch_size, output_size).\n",
    "            Specifies date stamps per serie\n",
    "            to consider in loss.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mae: tensor (single value).\n",
    "            Mean absolute error.\n",
    "    \"\"\"\n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    mae = t.abs(y - y_hat) * mask\n",
    "    mae = t.mean(mae)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "mae_loss_image = Image(filename='loss_imgs/mae_loss.png', width=WIDTH, height=HEIGHT)\n",
    "mae_loss_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def MSELoss(y: t.Tensor, y_hat: t.Tensor, mask: t.Tensor =None) -> t.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Calculates Mean Squared Error (MSE) between\n",
    "    y and y_hat. MSE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the \n",
    "    squared deviation of the prediction and the true\n",
    "    value at a given time, and averages these devations\n",
    "    over the length of the series.\n",
    "    \n",
    "    $$ \\mathrm{MSE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \n",
    "        \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} (y_{\\\\tau} - \\hat{y}_{\\\\tau})^{2} $$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: tensor (batch_size, output_size).\n",
    "            Actual values in torch tensor.\n",
    "        y_hat: tensor (batch_size, output_size).\n",
    "            Predicted values in torch tensor.\n",
    "        mask: tensor (batch_size, output_size).\n",
    "            Specifies date stamps per serie\n",
    "            to consider in loss.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mse: tensor (single value).\n",
    "            Mean Squared Error.\n",
    "    \"\"\"\n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    mse = (y - y_hat)**2\n",
    "    mse = mask * mse\n",
    "    mse = t.mean(mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "mse_image = Image(filename='loss_imgs/mse_loss.png', width=WIDTH, height=HEIGHT)\n",
    "mse_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def RMSELoss(y: t.Tensor, y_hat: t.Tensor, mask: t.Tensor =None) -> t.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Calculates Root Mean Squared Error (RMSE) between\n",
    "    y and y_hat. RMSE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the squared deviation\n",
    "    of the prediction and the observed value at a given time and\n",
    "    averages these devations over the length of the series.\n",
    "    Finally the RMSE will be in the same scale\n",
    "    as the original time series so its comparison with other\n",
    "    series is possible only if they share a common scale. \n",
    "    RMSE has a direct connection to the L2 norm.\n",
    "    \n",
    "    $$ \\mathrm{RMSE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \n",
    "        \\\\sqrt{\\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} (y_{\\\\tau} - \\hat{y}_{\\\\tau})^{2}} $$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: tensor (batch_size, output_size).\n",
    "            Actual values in torch tensor.\n",
    "        y_hat: tensor (batch_size, output_size).\n",
    "            Predicted values in torch tensor.\n",
    "        mask: tensor (batch_size, output_size).\n",
    "            Specifies date stamps per serie\n",
    "            to consider in loss.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        rmse: tensor (single value).\n",
    "            Root Mean Squared Error.\n",
    "    \"\"\"\n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    rmse = (y - y_hat)**2\n",
    "    rmse = mask * rmse\n",
    "    rmse = t.sqrt(t.mean(rmse))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "rmse_loss_image = Image(filename='loss_imgs/rmse_loss.png', width=WIDTH, height=HEIGHT)\n",
    "rmse_loss_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkOrange\">2. Percentage Errors </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MAPELoss(y: t.Tensor, y_hat: t.Tensor, mask: t.Tensor =None) -> t.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Calculates Mean Absolute Percentage Error (MAPE) between\n",
    "    y and y_hat. MAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the percentual deviation\n",
    "    of the prediction and the observed value at a given time and\n",
    "    averages these devations over the length of the series.\n",
    "    The closer to zero an observed value is, the higher penalty MAPE loss\n",
    "    assigns to the corresponding error.\n",
    "    \n",
    "    $$ \\mathrm{MAPE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \n",
    "        \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1}\n",
    "        \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{|y_{\\\\tau}|} $$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: tensor (batch_size, output_size).\n",
    "            Actual values in torch tensor.\n",
    "        y_hat: tensor (batch_size, output_size).\n",
    "            Predicted values in torch tensor.\n",
    "        mask: tensor (batch_size, output_size).\n",
    "            Specifies date stamps per serie\n",
    "            to consider in loss.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mape: tensor (single value).\n",
    "            Mean absolute percentage error.\n",
    "    \"\"\"\n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    mask = _divide_no_nan(mask, t.abs(y))\n",
    "    mape = t.abs(y - y_hat) * mask\n",
    "    mape = t.mean(mape)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "mape_image = Image(filename='loss_imgs/mape_loss.png', width=WIDTH, height=HEIGHT)\n",
    "mape_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def SMAPELoss(y: t.Tensor, y_hat: t.Tensor, mask: t.Tensor =None) -> t.Tensor:\n",
    "    \"\"\"\n",
    "    \n",
    "    Calculates Symmetric Mean Absolute Percentage Error (SMAPE) between\n",
    "    y and y_hat. SMAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the relative deviation\n",
    "    of the prediction and the observed value scaled by the sum of the\n",
    "    absolute values for the prediction and observed value at a\n",
    "    given time, then averages these devations over the length\n",
    "    of the series. This allows the SMAPE to have bounds between\n",
    "    0% and 200% which is desireble compared to normal MAPE that\n",
    "    may be undetermined when the target is zero.\n",
    "    \n",
    "    $$ \\mathrm{sMAPE}_{2}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \n",
    "       \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \n",
    "       \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{|y_{\\\\tau}|+|\\hat{y}_{\\\\tau}|} $$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: tensor (batch_size, output_size).\n",
    "            Actual values in torch tensor.\n",
    "        y_hat: tensor (batch_size, output_size).\n",
    "            Predicted values in torch tensor.\n",
    "        mask: tensor (batch_size, output_size).\n",
    "            Specifies date stamps per serie\n",
    "            to consider in loss.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        smape: tensor (single value).\n",
    "            Symmetric mean absolute percentage error.\n",
    "    \"\"\"\n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    delta_y = t.abs((y - y_hat))\n",
    "    scale = t.abs(y) + t.abs(y_hat)\n",
    "    smape = _divide_no_nan(delta_y, scale)\n",
    "    smape = smape * mask\n",
    "    smape = 200 * t.mean(smape)\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkOrange\">3. Scale-independent Errors </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Scaled Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def MASELoss(y: t.Tensor, y_hat: t.Tensor, y_insample: t.Tensor, \n",
    "                seasonality: int, mask: t.Tensor =None) -> t.Tensor:\n",
    "    \"\"\" \n",
    "\n",
    "    Calculates the Mean Absolute Scaled Error (MASE) between\n",
    "    y and y_hat. MASE measures the relative prediction\n",
    "    accuracy of a forecasting method by comparinng the mean absolute errors\n",
    "    of the prediction and the observed value against the mean\n",
    "    absolute errors of the seasonal naive model.\n",
    "    The MASE partially composed the Overall Weighted Average (OWA), \n",
    "    used in the M4 Competition.\n",
    "    \n",
    "    $$ \\mathrm{MASE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{season}_{\\\\tau}) = \n",
    "        \\\\frac{1}{H} \\sum^{t+H}_{\\\\tau=t+1} \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{\\mathrm{MAE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{season}_{\\\\tau})} $$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: tensor (batch_size, output_size).\n",
    "            Actual values in torch tensor.\n",
    "        y_hat: tensor (batch_size, output_size).\n",
    "            Predicted values in torch tensor.\n",
    "        y_insample: tensor (batch_size, input_size). \n",
    "            Actual insample Seasonal Naive predictions.\n",
    "        seasonality: int.\n",
    "            Main frequency of the time series;\n",
    "            Hourly 24,  Daily 7, Weekly 52,\n",
    "            Monthly 12, Quarterly 4, Yearly 1.\n",
    "        mask: tensor (batch_size, output_size).\n",
    "            Specifies date stamps per serie\n",
    "            to consider in loss.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        mase: tensor (single value).\n",
    "            Mean absolute scaled error.\n",
    "            \n",
    "        References\n",
    "        ----------\n",
    "        [1] https://robjhyndman.com/papers/mase.pdf\n",
    "    \"\"\"\n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    delta_y = t.abs(y - y_hat)\n",
    "    scale = t.mean(t.abs(y_insample[:, seasonality:] - \\\n",
    "                            y_insample[:, :-seasonality]), axis=1)\n",
    "    mase = _divide_no_nan(delta_y, scale[:, None])\n",
    "    mase = mase * mask\n",
    "    mase = t.mean(mase)\n",
    "    return mase \n",
    "    #remove this later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "mase_loss_image = Image(filename='loss_imgs/mase_loss.png', width=WIDTH, height=HEIGHT)\n",
    "mase_loss_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def RMAELoss(y: t.Tensor, y_hat1: t.Tensor, y_hat2: t.Tensor, mask: t.Tensor =None) -> t.tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Calculates Relative Mean Absolute Error (RMAE) between\n",
    "    two sets of forecasts (from two different forecasting methods).\n",
    "    A number smaller than one implies that the forecast in the\n",
    "    numerator is better than the forecast in the denominator.\n",
    "\n",
    "    $$ \\mathrm{RMAE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{base}_{\\\\tau}) =\n",
    "        \\\\frac{1}{H} \\sum^{t+H}_{\\\\tau=t+1} \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{\\mathrm{MAE}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{base}_{\\\\tau})} $$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: tensor (batch_size, output_size). \n",
    "            Actual values in torch tensor.\n",
    "        y_hat1: tensor (batch_size, output_size). \n",
    "            Predicted values in torch tensor. \n",
    "        y_hat2: tensor (batch_size, output_size). \n",
    "            Predicted values in torch tensor. \n",
    "        mask: tensor (batch_size, output_size).\n",
    "            Specifies date stamps per series \n",
    "            to consider in loss. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        rmae: tensor (single value).\n",
    "            Relative Mean Absolute Error. \n",
    "    \"\"\"\n",
    "    \n",
    "    if mask is None: mask = t.ones_like(y_hat1)\n",
    "    \n",
    "    numerator = MAELoss(y=y, y_hat=y_hat1)\n",
    "    denominator = MAELoss(y=y, y_hat=y_hat2)\n",
    "    rmae_loss = _divide_no_nan(numerator, denominator)\n",
    "\n",
    "    return rmae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "rmae_loss_image = Image(filename='loss_imgs/rmae_loss.png', width=WIDTH, height=HEIGHT)\n",
    "rmae_loss_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkOrange\">4. Probabilistic Errors </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def QuantileLoss(y: t.Tensor, y_hat: t.Tensor, mask: t.Tensor =None, \n",
    "                    q: float =0.5) -> t.Tensor:\n",
    "    \"\"\"\n",
    "    \n",
    "    Computes the quantile loss (QL) between y and y_hat. \n",
    "    QL measures the deviation of a quantile forecast.\n",
    "    By weighting the absolute deviation in a non symmetric way, the\n",
    "    loss pays more attention to under or over estimation.\n",
    "    A common value for q is 0.5 for the deviation from the median (Pinball loss).\n",
    "\n",
    "    $$ \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q)}_{\\\\tau}) = \n",
    "        \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \n",
    "        \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\\\tau} - y_{\\\\tau} )_{+} \n",
    "        + q\\,( y_{\\\\tau} - \\hat{y}^{(q)}_{\\\\tau} )_{+} \\Big) $$\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        y: tensor (batch_size, output_size).\n",
    "            Actual values in torch tensor.\n",
    "        y_hat: tensor (batch_size, output_size).\n",
    "            Predicted values in torch tensor.\n",
    "        mask: tensor (batch_size, output_size).\n",
    "            Specifies date stamps per serie\n",
    "            to consider in loss.\n",
    "        q: float, between 0 and 1. \n",
    "            The slope of the quantile loss, in the context of \n",
    "            quantile regression, the q determines the conditional \n",
    "            quantile level.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        quantile_loss: tensor (single value).\n",
    "            Average quantile loss.\n",
    "    \"\"\"\n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    delta_y = t.sub(y, y_hat)\n",
    "    loss = t.max(t.mul(q, delta_y), t.mul((q - 1), delta_y))\n",
    "    loss = loss * mask\n",
    "    quantile_loss = t.mean(loss)\n",
    "    return quantile_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "qloss_image = Image(filename='loss_imgs/q_loss.png', width=WIDTH, height=HEIGHT)\n",
    "qloss_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MQLoss(y: t.Tensor, y_hat: t.Tensor, quantiles: t.Tensor, \n",
    "            mask: t.Tensor =None) -> t.Tensor: \n",
    "    \"\"\"\n",
    "    \n",
    "    Calculates the Multi-Quantile loss (MQL) between y and y_hat. \n",
    "    MQL calculates the average multi-quantile Loss for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted quantiles and observed values.\n",
    "    \n",
    "    $$ \\mathrm{MQL}(\\\\mathbf{y}_{\\\\tau},\n",
    "                    [\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \n",
    "       \\\\frac{1}{n} \\\\sum_{q_{i}} \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau}) $$\n",
    "    \n",
    "    The limit behavior of MQL allows to measure the accuracy \n",
    "    of a full predictive distribution $\\mathbf{\\hat{F}}_{\\\\tau}$ with \n",
    "    the continuous ranked probability score (CRPS). This can be achieved \n",
    "    through a numerical integration technique, that discretizes the quantiles \n",
    "    and treats the CRPS integral with a left Riemann approximation, averaging over \n",
    "    uniformly distanced quantiles.    \n",
    "    \n",
    "    $$ \\mathrm{CRPS}(y_{\\\\tau}, \\mathbf{\\hat{F}}_{\\\\tau}) = \n",
    "        \\int^{1}_{0} \\mathrm{QL}(y_{\\\\tau}, \\hat{y}^{(q)}_{\\\\tau}) dq $$        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y: tensor (batch_size, output_size).\n",
    "            Actual values in torch tensor.\n",
    "        y_hat: tensor (batch_size, output_size).\n",
    "            Predicted values in torch tensor.\n",
    "        mask: tensor (batch_size, output_size).\n",
    "            Specifies date stamps per serie to consider in loss.\n",
    "        quantiles: tensor(n_quantiles). \n",
    "            Quantiles to estimate from the distribution of y.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mqloss: tensor(n_quantiles).\n",
    "            Average multi-quantile loss.\n",
    "            \n",
    "        References\n",
    "        ----------\n",
    "        [1] https://www.jstor.org/stable/2629907\n",
    "    \"\"\"    \n",
    "    assert len(quantiles) > 1, f'your quantiles are of len: {len(quantiles)}'\n",
    "    \n",
    "    if mask is None: mask = t.ones_like(y)\n",
    "\n",
    "    n_q = len(quantiles)\n",
    "    \n",
    "    error  = y_hat - y.unsqueeze(-1)\n",
    "    sq     = t.maximum(-error, t.zeros_like(error))\n",
    "    s1_q   = t.maximum(error, t.zeros_like(error))\n",
    "    mqloss = (quantiles * sq + (1 - quantiles) * s1_q)\n",
    "        \n",
    "    # Match y/weights dimensions and compute weighted average\n",
    "    mask = mask / t.sum(mask)\n",
    "    mask = mask.unsqueeze(-1)\n",
    "    mqloss = (1/n_q) * mqloss * mask\n",
    "\n",
    "    return t.sum(mqloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "mqloss_image = Image(filename='loss_imgs/mq_loss.png', width=WIDTH, height=HEIGHT)\n",
    "mqloss_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Multi-Quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def wMQLoss(y: t.Tensor, y_hat: t.Tensor, quantiles: t.Tensor, \n",
    "            mask: t.Tensor =None) -> t.Tensor: \n",
    "    \"\"\"        \n",
    "        \n",
    "    Calculates the Weighted Multi-Quantile loss (WMQL) between y and y_hat. \n",
    "    WMQL calculates the weighted average multi-quantile Loss for\n",
    "    a given set of quantiles, based on the absolute \n",
    "    difference between predicted quantiles and observed values.  \n",
    "        \n",
    "    $$ \\mathrm{WMQL}(\\\\mathbf{y}_{\\\\tau},\n",
    "                    [\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \n",
    "       \\\\frac{1}{n} \\\\sum_{q_{i}} \n",
    "           \\\\frac{\\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau})}\n",
    "            {\\\\sum^{t+H}_{\\\\tau=t+1} |y_{\\\\tau}|} $$\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y: tensor (batch_size, output_size).\n",
    "            Actual values in torch tensor.\n",
    "        y_hat: tensor (batch_size, output_size).\n",
    "            Predicted values in torch tensor.\n",
    "        mask: tensor (batch_size, output_size).\n",
    "            Specifies date stamps per serie to consider in loss.\n",
    "        quantiles: tensor(n_quantiles). \n",
    "            Quantiles to estimate from the distribution of y.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        wmqloss: tensor(n_quantiles).\n",
    "            Weighted average multi-quantile loss.\n",
    "    \"\"\"    \n",
    "    assert len(quantiles) > 1, f'your quantiles are of len: {len(quantiles)}'\n",
    "    \n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "    \n",
    "    n_q = len(quantiles)\n",
    "    \n",
    "    error = y_hat - y.unsqueeze(-1)\n",
    "    \n",
    "    sq = t.maximum(-error, t.zeros_like(error))\n",
    "    s1_q = t.maximum(error, t.zeros_like(error))\n",
    "    loss = (quantiles * sq + (1 - quantiles) * s1_q)\n",
    "    \n",
    "    wmqloss = _divide_no_nan(t.sum(loss * mask, axis=-2), \n",
    "                          t.sum(t.abs(y.unsqueeze(-1)) * mask, axis=-2))\n",
    "    \n",
    "    return t.mean(wmqloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DarkOrange\">5. Other Errors </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES-RNN PyTorch Loss\n",
    "\n",
    "The M4 competition winner, the exponential smoothing recurrent neural network\n",
    "combines the Holt-Winter method for the seasonal and levels \n",
    "with a dilated Recurrent neural network is defined by:\n",
    "\n",
    "* $\\text{Level:} \\quad l_{\\tau} = \\text{median}(y_{\\tau})$\n",
    "* $\\text{Residual:} \\quad z_{\\tau} = y_{\\tau} - l_{\\tau}$\n",
    "* $\\text{NN Forecast:} \\quad \\hat{z}_{\\tau} = \\text{DRNN}(z_{\\tau}, x_{\\tau}, s_{\\tau})$\n",
    "* $\\text{Level Forecast:} \\quad \\hat{l}_{\\tau} = \\text{Naive}(l_{\\tau})$\n",
    "* $\\text{Forecast:} \\quad \\hat{y}_{\\tau+H} = \\hat{l}_{\\tau+H} + \\hat{z}_{\\tau+h}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def LevelVariabilityLoss(levels: t.Tensor, level_variability_penalty: float) -> t.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the variability penalty for the level of the ES-RNN.\n",
    "    The levels of the ES-RNN are based on the Holt-Winters model.\n",
    "    \n",
    "    $$  Penalty = \\lambda * (\\hat{l}_{τ+1}-\\hat{l}_{τ})^{2} $$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        levels: tensor with shape (batch, n_time).\n",
    "            Levels obtained from exponential smoothing component of ESRNN.\n",
    "        level_variability_penalty: float.\n",
    "            This parameter controls the strength of the penalization \n",
    "            to the wigglines of the level vector, induces smoothness\n",
    "            in the output.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        level_var_loss: tensor (single value).\n",
    "            Wiggliness loss for the level vector.\n",
    "    \"\"\"\n",
    "    assert levels.shape[1] > 2\n",
    "    level_prev = t.log(levels[:, :-1])\n",
    "    level_next = t.log(levels[:, 1:])\n",
    "    log_diff_of_levels = t.sub(level_prev, level_next)\n",
    "\n",
    "    log_diff_prev = log_diff_of_levels[:, :-1]\n",
    "    log_diff_next = log_diff_of_levels[:, 1:]\n",
    "    diff = t.sub(log_diff_prev, log_diff_next)\n",
    "    level_var_loss = diff**2\n",
    "    level_var_loss = level_var_loss.mean() * level_variability_penalty\n",
    "    \n",
    "    return level_var_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SmylLoss(y: t.Tensor, y_hat: t.Tensor, levels: t.Tensor, \n",
    "                mask: t.Tensor, tau: float, level_variability_penalty: float =0.0) -> t.Tensor:\n",
    "    \"\"\"\n",
    "    \n",
    "    Computes the Smyl Loss that combines level \n",
    "    variability regularization with with Quantile loss.\n",
    "    \n",
    "        Parameters\n",
    "        ----------    \n",
    "        y: tensor (batch_size, output_size).\n",
    "            Actual values in torch tensor.\n",
    "        y_hat: tensor (batch_size, output_size).\n",
    "            Predicted values in torch tensor.\n",
    "        levels: tensor with shape (batch, n_time).\n",
    "            Levels obtained from exponential smoothing component of ESRNN.\n",
    "        mask: tensor (batch_size, output_size).\n",
    "            Specifies date stamps per serie to consider in loss.\n",
    "        tau: float, between 0 and 1. \n",
    "            The slope of the quantile loss, in the context of \n",
    "            quantile regression, the q determines the conditional \n",
    "            quantile level.\n",
    "        level_variability_penalty: float.\n",
    "            This parameter controls the strength of the penalization \n",
    "            to the wigglines of the level vector, induces smoothness\n",
    "            in the output.\n",
    "                           \n",
    "        Returns\n",
    "        ----------\n",
    "        smyl_loss: tensor (single value).\n",
    "            Smyl loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    if mask is None: mask = t.ones_like(y_hat)\n",
    "        \n",
    "    smyl_loss = QuantileLoss(y, y_hat, mask, tau)\n",
    "    \n",
    "    if level_variability_penalty > 0:\n",
    "        log_diff_of_levels = LevelVariabilityLoss(levels, level_variability_penalty) \n",
    "        smyl_loss += log_diff_of_levels\n",
    "    \n",
    "    return smyl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Checks for PyTorch train losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import time\n",
    "from scipy.stats import hmean\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "class Model(nn.Module):  \n",
    "\n",
    "    def __init__(self, horizon, n_quantiles):\n",
    "        super(Model, self).__init__()\n",
    "        self.horizon = horizon\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.linear_layer = nn.Linear(in_features=n_obs, \n",
    "                                      out_features=horizon * n_quantiles, \n",
    "                                      bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.linear_layer(x)\n",
    "        y_hat = y_hat.view(-1, self.horizon, self.n_quantiles)\n",
    "        return y_hat\n",
    "    \n",
    "class Data(Dataset):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, Y, X):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.len = Y.shape[0]\n",
    "\n",
    "    # Getter\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]\n",
    "    \n",
    "    # Get Length\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Hyperparameters and sample data parameters\n",
    "t.cuda.manual_seed(7)\n",
    "\n",
    "# Sample data\n",
    "n_ts = 1000\n",
    "n_obs = horizon = 10\n",
    "mean = 0.0 # to generate random numbers from N(mean, std)\n",
    "std = 7.0 # to generate random numbers from N(mean, std)\n",
    "start = 0.05 # First quantile\n",
    "end = 0.95 # Last quantiles\n",
    "steps = 4 # Number of quantiles\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 500\n",
    "lr = 0.08\n",
    "epochs = 100\n",
    "\n",
    "# Sample data\n",
    "quantiles = t.Tensor([0.0500, 0.3500, 0.6500, 0.9500])\n",
    "print(f'quantiles:\\n{quantiles}')\n",
    "Y = t.normal(mean=mean, std=std, size=(n_ts, n_obs))\n",
    "X = t.ones(size=(n_ts, n_obs))\n",
    "\n",
    "Y_test = t.normal(mean=mean, std=std, size=(n_ts, horizon))\n",
    "X_test = t.ones(size=(n_ts, horizon))\n",
    "print(f'Y.shape: {Y.shape}, X.shape: {X.shape}')\n",
    "print(f'Y_test.shape: {Y_test.shape}, X_test.shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Model training \n",
    "model = Model(horizon=horizon, n_quantiles=len(quantiles))\n",
    "dataset = Data(X=X, Y=Y)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train_model(model, epochs, print_progress=False):\n",
    "\n",
    "    start = time.time()\n",
    "    i = 0 \n",
    "    training_trajectory = {'epoch': [],\n",
    "                           'train_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, y in dataloader:\n",
    "            \n",
    "            i += 1\n",
    "            y_hat = model(x)\n",
    "            #training_loss = wMQLoss(y=y, y_hat=y_hat, quantiles=quantiles)\n",
    "            training_loss = MQLoss(y=y, y_hat=y_hat, quantiles=quantiles)\n",
    "            if i % (epoch + 1) == 0: \n",
    "                training_trajectory['epoch'].append(i)\n",
    "                training_trajectory['train_loss'].append(training_loss.detach().numpy())\n",
    "            optimizer.zero_grad()\n",
    "            training_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            display_string = 'Step: {}, Time: {:03.3f}, Insample {}: {:.5f}'.format(i, \n",
    "                                                                                    time.time()-start, \n",
    "                                                                                    \"MQLoss\", \n",
    "                                                                                    training_loss.cpu().data.numpy())\n",
    "            if print_progress: print(display_string)\n",
    "\n",
    "    return model, training_trajectory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
