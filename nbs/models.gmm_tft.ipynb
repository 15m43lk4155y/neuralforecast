{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.gmm_tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM TFT\n",
    "\n",
    "> Combination of TFT with GMM loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from neuralforecast.models.tft import TFT\n",
    "from neuralforecast.losses.pytorch import MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GMM_TFT(TFT):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 h,\n",
    "                 input_size,\n",
    "                 K=100,\n",
    "                 tgt_size=1,\n",
    "                 hidden_size=128,\n",
    "                 s_cont_cols=None,\n",
    "                 s_cat_cols=None,\n",
    "                 o_cont_cols=None,\n",
    "                 o_cat_cols=None,\n",
    "                 k_cont_cols=None,\n",
    "                 k_cat_cols=None,\n",
    "                 s_cat_inp_lens=None,\n",
    "                 s_cont_inp_size=0,\n",
    "                 k_cat_inp_lens=None,\n",
    "                 k_cont_inp_size=1,\n",
    "                 o_cat_inp_lens=None,\n",
    "                 o_cont_inp_size=0,\n",
    "                 n_head=4,\n",
    "                 attn_dropout=0.0,\n",
    "                 dropout=0.1,\n",
    "                 loss=MAE(),\n",
    "                 learning_rate=1e-3,\n",
    "                 batch_size=32,\n",
    "                 windows_batch_size=1024,\n",
    "                 step_size=1,\n",
    "                 scaler_type='robust',\n",
    "                 num_workers_loader=0,\n",
    "                 drop_last_loader=False,\n",
    "                 random_seed=1,\n",
    "                 **trainer_kwargs):\n",
    "        # Inherit TFT class and extend it with GMM\n",
    "        super(GMM_TFT, self).__init__(h=h,\n",
    "                                      input_size=input_size,\n",
    "                                      tgt_size=tgt_size,\n",
    "                                      hidden_size=hidden_size,\n",
    "                                      s_cont_cols=s_cont_cols,\n",
    "                                      s_cat_cols=s_cat_cols,\n",
    "                                      o_cont_cols=o_cont_cols,\n",
    "                                      o_cat_cols=o_cat_cols,\n",
    "                                      k_cont_cols=k_cont_cols,\n",
    "                                      k_cat_cols=k_cat_cols,\n",
    "                                      s_cat_inp_lens=s_cat_inp_lens,\n",
    "                                      s_cont_inp_size=s_cont_inp_size,\n",
    "                                      k_cat_inp_lens=k_cat_inp_lens,\n",
    "                                      k_cont_inp_size=k_cont_inp_size,\n",
    "                                      o_cat_inp_lens=o_cat_inp_lens,\n",
    "                                      o_cont_inp_size=o_cont_inp_size,\n",
    "                                      n_head=n_head,\n",
    "                                      attn_dropout=attn_dropout,\n",
    "                                      dropout=dropout,\n",
    "                                      loss=loss,\n",
    "                                      learning_rate=learning_rate,\n",
    "                                      batch_size=batch_size,\n",
    "                                      windows_batch_size=windows_batch_size,\n",
    "                                      step_size=step_size,\n",
    "                                      scaler_type=scaler_type,\n",
    "                                      num_workers_loader=num_workers_loader,\n",
    "                                      drop_last_loader=drop_last_loader,\n",
    "                                      random_seed=random_seed,\n",
    "                                      **trainer_kwargs)\n",
    "        \n",
    "        # Define Mixture specialized parameters\n",
    "        self.K = K\n",
    "\n",
    "        # Adapter with Loss dependent dimensions\n",
    "        self.output_adapter = nn.Linear(in_features=hidden_size, out_features=K)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Deviates from orignal `BaseWindows.training_step` to \n",
    "        # allow the model to receive future exogenous available\n",
    "        # at the time of the prediction.\n",
    "        \n",
    "        # Create windows [Ws, L+H, C]\n",
    "        windows = self._create_windows(batch, step='train')\n",
    "\n",
    "        # Normalize windows\n",
    "        if self.scaler is not None:\n",
    "            windows = self._normalization(windows=windows)\n",
    "\n",
    "        # outsample\n",
    "        y_idx = batch['temporal_cols'].get_loc('y')\n",
    "        mask_idx = batch['temporal_cols'].get_loc('available_mask')\n",
    "        outsample_y = windows['temporal'][:, -self.h:, y_idx]\n",
    "        outsample_mask = windows['temporal'][:, -self.h:, mask_idx]\n",
    "\n",
    "        # [Ws, H, K]\n",
    "        means_hat = self(x=windows)\n",
    "        stds = (1/self.K) * torch.ones_like(means_hat).to(means_hat.device)\n",
    "        weights = (1/self.K) * torch.ones_like(means_hat).to(means_hat.device)\n",
    "\n",
    "        loss = self.loss(y=outsample_y[:,:,None], means=means_hat,\n",
    "                         stds=stds, weights=weights,\n",
    "                         mask=outsample_mask[:,:,None])\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # Deviates from orignal `BaseWindows.training_step` to \n",
    "        # allow the model to receive future exogenous available\n",
    "        # at the time of the prediction.        \n",
    "        \n",
    "        # Create windows [Ws, L+H, C]\n",
    "        windows = self._create_windows(batch, step='predict')\n",
    "\n",
    "        # Normalize windows\n",
    "        if self.scaler is not None:\n",
    "            windows = self._normalization(windows=windows)\n",
    "\n",
    "        # [Ws, H, K]\n",
    "        means_hat = self(x=windows)\n",
    "        stds = (1/self.K) * torch.ones_like(means_hat).to(means_hat.device)\n",
    "        weights = (1/self.K) * torch.ones_like(means_hat).to(means_hat.device)\n",
    "\n",
    "        _, quants = self.loss.sample(weights=weights,\n",
    "                                     means=means_hat, stds=stds,\n",
    "                                     num_samples=2000)\n",
    "\n",
    "        # Inv Normalize\n",
    "        if self.scaler is not None:\n",
    "            quants = self._inv_normalization(y_hat=quants,\n",
    "                                             temporal_cols=batch['temporal_cols'])\n",
    "\n",
    "        return quants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(GMM_TFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(GMM_TFT.fit, name='TFT.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(GMM_TFT.predict, name='TFT.predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast.utils import AirPassengers, AirPassengersPanel\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n",
    "\n",
    "from neuralforecast.losses.pytorch import MQLoss, GMM\n",
    "\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]] # 12 test\n",
    "\n",
    "dataset, *_ = TimeSeriesDataset.from_df(df = AirPassengersPanel)\n",
    "model = GMM_TFT(h=12, input_size=48,\n",
    "                hidden_size=100,\n",
    "                k_cont_cols=['trend'], #['trend', 'y_[lag12]'],\n",
    "                k_cont_inp_size=2,\n",
    "                max_epochs=1,\n",
    "                scaler_type='robust',\n",
    "                loss=GMM(level=[80, 90]),\n",
    "                windows_batch_size=None,\n",
    "                enable_progress_bar=True)\n",
    "\n",
    "model.fit(dataset=dataset, test_size=12)\n",
    "\n",
    "# Parse quantile predictions\n",
    "y_hat = model.predict(dataset=dataset)\n",
    "Y_hat_df = pd.DataFrame.from_records(data=y_hat,\n",
    "                columns=['TFT'+q for q in model.loss.output_names],\n",
    "                index=Y_test_df.index)\n",
    "\n",
    "# Plot quantile predictions\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline2']\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline2'].drop('unique_id', axis=1)\n",
    "\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "# plt.plot(plot_df['ds'], plot_df['TFT'], c='blue', label='median')\n",
    "plt.plot(plot_df['ds'], plot_df['TFT-median'], c='blue', label='median')\n",
    "plt.fill_between(x=plot_df['ds'], \n",
    "                 y1=plot_df['TFT-lo-90'], y2=plot_df['TFT-hi-90'],\n",
    "                 alpha=0.4, label='level 90')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "neuralforecast"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
