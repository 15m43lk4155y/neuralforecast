{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cho et. al proposed the Gated Recurrent Unit (`GRU`) to improve on LSTM and Elman cells. The predictions at each time are given by a MLP decoder. This architecture follows closely the original Multi Layer Elman `RNN` with the main difference being its use of the GRU cells.\n",
    "\n",
    "**References**<br>\n",
    "-[Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio (2014). \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\".](https:arxivorg/abs/1412.3555)<br>\n",
    "-[Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio (2014). \"On the Properties of Neural Machine Translation: Encoder-Decoder Approaches\".](https://arxiv.org/abs/1409.1259)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1. Gated Recurrent Unit Cell.](imgs_models/gru.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_recurrent import BaseRecurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GRU(BaseRecurrent):\n",
    "    \"\"\" GRU\n",
    "\n",
    "    Multi Layer Recurrent Network with Gated Units (GRU), and simple multi-step \n",
    "    MLP decoder. The network has `tanh` or `relu` non-linearities, it is trained \n",
    "    using ADAM stochastic gradient descent. The network accepts static, historic \n",
    "    and future exogenous data, flattens the inputs.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `h`: int, forecast horizon.<br>\n",
    "    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n",
    "    `n_layers`: int=2, number of layers for the GRU.<br>\n",
    "    `state_hsize`: int=200, units for the GRU's hidden state size.<br>\n",
    "    `activation`: str=`tanh`, type of GRU activation from `tanh` or `relu`.<br>\n",
    "    `bias`: bool=True, whether or not to use biases b_ih, b_hh within GRU units.<br>\n",
    "    `dropout`: float=0., dropout regularization applied to GRU outputs.<br>\n",
    "    `stat_exog_list`: str list, static exogenous columns.<br>\n",
    "    `hist_exog_list`: str list, historic exogenous columns.<br>\n",
    "    `futr_exog_list`: str list, future exogenous columns.<br>\n",
    "    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n",
    "    `learning_rate`: float=1e-3, initial optimization learning rate (0,1).<br>\n",
    "    `batch_size`: int=32, number of differentseries in each batch.<br>\n",
    "    `step_size`: int=1, step size between each window of temporal data.<br>\n",
    "    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n",
    "    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n",
    "    `num_workers_loader`: int=os.cpu_count(), workers to be used by `TimeSeriesDataLoader`.<br>\n",
    "    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n",
    "    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 h: int,\n",
    "                 input_size: int,\n",
    "                 n_layers: int = 2,\n",
    "                 state_hsize: int = 200, \n",
    "                 activation: str = 'tanh',\n",
    "                 bias: bool = True,\n",
    "                 dropout: float = 0.,\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 loss=MAE(),\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 batch_size=32,\n",
    "                 step_size: int = 1,\n",
    "                 scaler_type: str='robust',\n",
    "                 num_workers_loader=0,\n",
    "                 drop_last_loader=False,\n",
    "                 random_seed=1,\n",
    "                 **trainer_kwargs):\n",
    "        super(GRU, self).__init__(\n",
    "            h = h,\n",
    "            input_size = input_size,\n",
    "            loss=loss,\n",
    "            learning_rate = learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            scaler_type=scaler_type,\n",
    "            futr_exog_list=futr_exog_list,\n",
    "            hist_exog_list=hist_exog_list,\n",
    "            stat_exog_list=stat_exog_list,\n",
    "            num_workers_loader=num_workers_loader,\n",
    "            drop_last_loader=drop_last_loader,\n",
    "            random_seed=random_seed,\n",
    "            **trainer_kwargs\n",
    "        )\n",
    "\n",
    "        # Architecture\n",
    "        self.state_hsize = state_hsize\n",
    "        self.step_size = step_size\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.futr_exog_size = len(self.futr_exog_list)\n",
    "        self.hist_exog_size = len(self.hist_exog_list)\n",
    "        self.stat_exog_size = len(self.stat_exog_list)\n",
    "\n",
    "        input_rnn = input_size + self.hist_exog_size*input_size + \\\n",
    "                    self.futr_exog_size*(input_size + h) + self.stat_exog_size\n",
    "\n",
    "        # Instantiate model\n",
    "        self.gru = nn.GRU(input_size=input_rnn,\n",
    "                            hidden_size=self.state_hsize,\n",
    "                            num_layers=self.n_layers,\n",
    "                            bias=self.bias,\n",
    "                            dropout=self.dropout,\n",
    "                            batch_first=True)\n",
    "        self.adapterW  = nn.Linear(self.state_hsize, self.h)\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        \n",
    "        # Parse windows_batch\n",
    "        insample_y    = windows_batch['insample_y']\n",
    "        futr_exog     = windows_batch['futr_exog']\n",
    "        hist_exog     = windows_batch['hist_exog']\n",
    "        stat_exog     = windows_batch['stat_exog']\n",
    "\n",
    "        # Flatten inputs [B, W, C, L+H] -> [B, W, C*(L+H)]\n",
    "        # Contatenate [ Y_t, | X_{t-L},..., X_{t} | F_{t-L},..., F_{t+H} | S ]\n",
    "        batch_size, windows_size = insample_y.shape[:2]\n",
    "        if self.hist_exog_size > 0:\n",
    "            hist_exog = hist_exog.permute(0,2,1,3) # [B, C, W, L] -> [B, W, C, L]\n",
    "            insample_y = torch.cat(( insample_y, hist_exog.reshape(batch_size, windows_size, -1) ), dim=2)\n",
    "\n",
    "        if self.futr_exog_size > 0:\n",
    "            futr_exog = futr_exog.permute(0,2,1,3) # [B, C, W, L] -> [B, W, C, L]\n",
    "            insample_y = torch.cat(( insample_y, futr_exog.reshape(batch_size, windows_size, -1) ), dim=2)\n",
    "\n",
    "        if self.stat_exog_size > 0:\n",
    "            stat_exog = stat_exog.unsqueeze(1).repeat(1, windows_size, 1) # [B, C] -> [B, W, C]\n",
    "            insample_y = torch.cat(( insample_y, stat_exog ), dim=2)\n",
    "\n",
    "        # GRU forward\n",
    "        insample_y, _ = self.gru(insample_y)\n",
    "        insample_y = self.adapterW(insample_y)\n",
    "        \n",
    "        return insample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(GRU.fit, name='GRU.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(GRU.predict, name='GRU.predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MQLoss\n",
    "from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n",
    "\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "fcst = NeuralForecast(\n",
    "    models=[GRU(h=12,\n",
    "                input_size=12,\n",
    "                state_hsize=100,\n",
    "                max_epochs=200,\n",
    "                scaler_type='robust',\n",
    "                futr_exog_list=['trend'],\n",
    "                hist_exog_list=['y_[lag12]'],\n",
    "                stat_exog_list=['airline1']),\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "fcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\n",
    "forecasts = fcst.predict(futr_df=Y_test_df)\n",
    "\n",
    "# Plot\n",
    "plot_df = Y_test_df.copy()\n",
    "plot_df['GRU'] = forecasts.reset_index(drop=False)['GRU']\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['GRU'], c='blue', label='median')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralforecast",
   "language": "python",
   "name": "neuralforecast"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
