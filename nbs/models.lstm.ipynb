{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "> The Long Short-Term Memory Recurrent Neural Network (`LSTM`), uses a multilayer `LSTM` encoder and an `MLP` decoder. It builds upon the LSTM-cell that improves the exploding and vanishing gradients of classic `RNN`'s. This network has been extensively used in sequential prediction tasks like language modeling, phonetic labeling, and forecasting.<br><br>**References**<br>-[Jeffrey L. Elman (1990). \"Finding Structure in Time\".](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1)<br>-[Haşim Sak, Andrew Senior, Françoise Beaufays (2014). \"Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition.\"](https://arxiv.org/abs/1402.1128)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1. Long Short-Term Memory Cell.](imgs_models/lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_recurrent import BaseRecurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LSTM(BaseRecurrent):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 h: int,\n",
    "                 state_hsize: int = 200, \n",
    "                 step_size: int = 1,\n",
    "                 n_layers: int = 2,\n",
    "                 bias: bool = True,\n",
    "                 dropout: float = 0.,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 normalize: bool = True,\n",
    "                 loss=MAE(),\n",
    "                 batch_size=32, \n",
    "                 num_workers_loader=0,\n",
    "                 drop_last_loader=False,\n",
    "                 random_seed=1,\n",
    "                 **trainer_kwargs):\n",
    "        super(LSTM, self).__init__(\n",
    "            loss=loss,\n",
    "            batch_size=batch_size,\n",
    "            num_workers_loader=num_workers_loader,\n",
    "            drop_last_loader=drop_last_loader,\n",
    "            random_seed=random_seed,\n",
    "            **trainer_kwargs\n",
    "        )\n",
    "\n",
    "        # Architecture\n",
    "        self.input_size = input_size\n",
    "        self.h = h\n",
    "        self.state_hsize = state_hsize\n",
    "        self.step_size = step_size\n",
    "        self.n_layers = n_layers\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Optimization\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.normalize = normalize\n",
    "        self.random_seed = random_seed\n",
    "        self.padder = nn.ConstantPad1d(padding=(0, self.h), value=0)\n",
    "\n",
    "        # Instantiate model\n",
    "        self.model = nn.LSTM(input_size=self.input_size,\n",
    "                             hidden_size=self.state_hsize,\n",
    "                             num_layers=self.n_layers,\n",
    "                             bias=self.bias,\n",
    "                             dropout=self.dropout,\n",
    "                             batch_first=True)\n",
    "        self.adapterW  = nn.Linear(self.state_hsize, self.h)\n",
    "\n",
    "    def forward(self, insample_y, insample_mask):\n",
    "\n",
    "        # LSTM forward\n",
    "        insample_y, _ = self.model(insample_y)\n",
    "        insample_y = self.adapterW(insample_y)\n",
    "        \n",
    "        return insample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from neuralforecast.utils import AirPassengersDF as Y_df\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n",
    "\n",
    "# Add second series\n",
    "Y_df_2 = Y_df.tail(100).copy()\n",
    "Y_df_2['unique_id'] = 2.0\n",
    "Y_df_2['y'] = 0.5*Y_df_2['y']\n",
    "Y_df = Y_df.append(Y_df_2).reset_index(drop=True)\n",
    "\n",
    "# Train/Test split\n",
    "Y_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\n",
    "Y_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test\n",
    "\n",
    "dataset, *_ = TimeSeriesDataset.from_df(df = Y_train_df)\n",
    "model = LSTM(24, 12, learning_rate=1e-3, max_epochs=100)\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)\n",
    "\n",
    "Y_test_df['LSTM'] = y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "pd.concat([Y_train_df[Y_train_df['unique_id']==1.0], Y_test_df[Y_test_df['unique_id']==1.0]]).drop('unique_id', axis=1).set_index('ds').plot()\n",
    "pd.concat([Y_train_df[Y_train_df['unique_id']==2.0], Y_test_df[Y_test_df['unique_id']==2.0]]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
