{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "The Long Short-Term Memory Recurrent Neural Network (`LSTM`), uses a multilayer `LSTM` encoder and an `MLP` decoder. It builds upon the LSTM-cell that improves the exploding and vanishing gradients of classic `RNN`'s. This network has been extensively used in sequential prediction tasks like language modeling, phonetic labeling, and forecasting.<br><br>**References**<br>-[Jeffrey L. Elman (1990). \"Finding Structure in Time\".](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1)<br>-[Haşim Sak, Andrew Senior, Françoise Beaufays (2014). \"Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition.\"](https://arxiv.org/abs/1402.1128)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1. Long Short-Term Memory Cell.](imgs_models/lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_recurrent import BaseRecurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LSTM(BaseRecurrent):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 h: int,\n",
    "                 state_hsize: int = 200, \n",
    "                 step_size: int = 1,\n",
    "                 n_layers: int = 2,\n",
    "                 activation: str = 'tanh',\n",
    "                 bias: bool = True,\n",
    "                 dropout: float = 0.,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 loss=MAE(),\n",
    "                 batch_size=32,\n",
    "                 scaler_type: str='robust',\n",
    "                 num_workers_loader=0,\n",
    "                 drop_last_loader=False,\n",
    "                 random_seed=1,\n",
    "                 **trainer_kwargs):\n",
    "        super(LSTM, self).__init__(\n",
    "            h = h,\n",
    "            input_size = input_size,\n",
    "            loss=loss,\n",
    "            learning_rate = learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            scaler_type=scaler_type,\n",
    "            futr_exog_list=futr_exog_list,\n",
    "            hist_exog_list=hist_exog_list,\n",
    "            stat_exog_list=stat_exog_list,\n",
    "            num_workers_loader=num_workers_loader,\n",
    "            drop_last_loader=drop_last_loader,\n",
    "            random_seed=random_seed,\n",
    "            **trainer_kwargs\n",
    "        )\n",
    "\n",
    "        # Architecture\n",
    "        self.state_hsize = state_hsize\n",
    "        self.step_size = step_size\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.futr_exog_size = len(self.futr_exog_list)\n",
    "        self.hist_exog_size = len(self.hist_exog_list)\n",
    "        self.stat_exog_size = len(self.stat_exog_list)\n",
    "\n",
    "        input_rnn = input_size + self.hist_exog_size*input_size + \\\n",
    "                    self.futr_exog_size*(input_size + h) + self.stat_exog_size\n",
    "\n",
    "        # Instantiate model\n",
    "        self.lstm = nn.LSTM(input_size=input_rnn,\n",
    "                            hidden_size=self.state_hsize,\n",
    "                            num_layers=self.n_layers,\n",
    "                            bias=self.bias,\n",
    "                            dropout=self.dropout,\n",
    "                            batch_first=True)\n",
    "        self.adapterW  = nn.Linear(self.state_hsize, self.h)\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        \n",
    "        # Parse windows_batch\n",
    "        insample_y    = windows_batch['insample_y']\n",
    "        futr_exog     = windows_batch['futr_exog']\n",
    "        hist_exog     = windows_batch['hist_exog']\n",
    "        stat_exog     = windows_batch['stat_exog']\n",
    "\n",
    "        # Flatten inputs [B, W, C, L+H] -> [B, W, C*(L+H)]\n",
    "        # Contatenate [ Y_t, | X_{t-L},..., X_{t} | F_{t-L},..., F_{t+H} | S ]\n",
    "        batch_size, windows_size = insample_y.shape[:2]\n",
    "        if self.hist_exog_size > 0:\n",
    "            hist_exog = hist_exog.permute(0,2,1,3) # [B, C, W, L] -> [B, W, C, L]\n",
    "            insample_y = torch.cat(( insample_y, hist_exog.reshape(batch_size, windows_size, -1) ), dim=2)\n",
    "\n",
    "        if self.futr_exog_size > 0:\n",
    "            futr_exog = futr_exog.permute(0,2,1,3) # [B, C, W, L] -> [B, W, C, L]\n",
    "            insample_y = torch.cat(( insample_y, futr_exog.reshape(batch_size, windows_size, -1) ), dim=2)\n",
    "\n",
    "        if self.stat_exog_size > 0:\n",
    "            stat_exog = stat_exog.unsqueeze(1).repeat(1, windows_size, 1) # [B, C] -> [B, W, C]\n",
    "            insample_y = torch.cat(( insample_y, stat_exog ), dim=2)\n",
    "\n",
    "        # LSTM forward\n",
    "        insample_y, _ = self.lstm(insample_y)\n",
    "        insample_y = self.adapterW(insample_y)\n",
    "        \n",
    "        return insample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LSTM.fit, name='LSTM.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(LSTM.predict, name='LSTM.predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MQLoss\n",
    "from neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n",
    "\n",
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n",
    "\n",
    "fcst = NeuralForecast(\n",
    "    models=[LSTM(h=12,\n",
    "                input_size=12,\n",
    "                state_hsize=100,\n",
    "                max_epochs=200,\n",
    "                futr_exog_list=['trend'],\n",
    "                hist_exog_list=['y_[lag12]'],\n",
    "                stat_exog_list=['airline1']),\n",
    "    ],\n",
    "    freq='M'\n",
    ")\n",
    "fcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\n",
    "forecasts = fcst.predict(futr_df=Y_test_df)\n",
    "\n",
    "# Plot\n",
    "plot_df = Y_test_df.copy()\n",
    "plot_df['LSTM'] = forecasts.reset_index(drop=False)['LSTM']\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['LSTM'], c='blue', label='median')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('neuralforecast')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
