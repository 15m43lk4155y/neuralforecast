{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# MLP\n",
    "> One of the simplest neural architectures are Multi Layer Perceptrons (`MLP`) composed of stacked Fully Connected Neural Networks trained with backpropagation. Each node in the architecture is capable of modeling non-linear relationships granted by their activation functions. Novel activations like Rectified Linear Units (`ReLU`) have greatly improved the ability to fit deeper networks overcoming gradient vanishing problems that were associated with `Sigmoid` and `TanH` activations. For the forecasting task the last layer is changed to follow a auto-regression problem.<br><br>**References**<br>-[Rosenblatt, F. (1958). \"The perceptron: A probabilistic model for information storage and organization in the brain.\"](https://psycnet.apa.org/record/1959-09865-001)<br>-[Fukushima, K. (1975). \"Cognitron: A self-organizing multilayered neural network.\"](https://pascal-francis.inist.fr/vibad/index.php?action=getRecordDetail&idt=PASCAL7750396723)<br>-[Vinod Nair, Geoffrey E. Hinton (2010). \"Rectified Linear Units Improve Restricted Boltzmann Machines\"](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6036ce9",
   "metadata": {},
   "source": [
    "![Figure 1. Three layer MLP with autorregresive inputs.](imgs_models/mlp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_windows import BaseWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70cd14-ecb1-4205-8511-fecbd26c8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLP(BaseWindows):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 h,\n",
    "                 step_size=1,\n",
    "                 hidden_size=1024, \n",
    "                 num_layers=2, \n",
    "                 learning_rate=1e-3,\n",
    "                 normalize=False,\n",
    "                 loss=MAE(),\n",
    "                 batch_size=32, \n",
    "                 num_workers_loader=0,\n",
    "                 drop_last_loader=False,\n",
    "                 random_seed=1,\n",
    "                 **trainer_kwargs):\n",
    "        \n",
    "        # Inherit BaseWindows class\n",
    "        super(MLP, self).__init__(h=h, \n",
    "                                  loss=loss,\n",
    "                                  batch_size=batch_size,\n",
    "                                  normalize=normalize,\n",
    "                                  num_workers_loader=num_workers_loader,\n",
    "                                  drop_last_loader=drop_last_loader,\n",
    "                                  random_seed=random_seed,\n",
    "                                  **trainer_kwargs)\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.step_size = step_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        \n",
    "        # MultiLayer Perceptron\n",
    "        layers = [nn.Linear(in_features=input_size, out_features=hidden_size)]\n",
    "        for i in range(num_layers - 1):\n",
    "            layers += [nn.Linear(in_features=hidden_size, out_features=hidden_size)]\n",
    "        self.mlp = nn.ModuleList(layers)\n",
    "        \n",
    "        # Adapter with Loss dependent dimensions\n",
    "        self.out = nn.Linear(in_features=hidden_size, \n",
    "                             out_features=h * self.loss.outputsize_multiplier)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        y_pred = x\n",
    "        for layer in self.mlp:\n",
    "             y_pred = torch.relu(layer(y_pred))\n",
    "        y_pred = self.out(y_pred)\n",
    "        y_pred = self.loss.adapt_output(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34472d-5670-45b5-a7c5-1dba54f8e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06414f-117e-42ce-abad-46ad4b8372b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test performance fit/predict method\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast.utils import AirPassengersDF as Y_df\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "\n",
    "\n",
    "Y_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\n",
    "Y_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test\n",
    "\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n",
    "model = MLP(input_size=24, h=12, max_epochs=100)\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)\n",
    "Y_test_df['MLP'] = y_hat\n",
    "\n",
    "#test we recover the same forecast\n",
    "y_hat2 = model.predict(dataset=dataset)\n",
    "test_eq(y_hat, y_hat2)\n",
    "\n",
    "pd.concat([Y_train_df, Y_test_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f70ba2-2c57-4710-808b-727f7e0169a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test no leakage with test_size\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_df)\n",
    "model = MLP(input_size=24, h=12, max_epochs=100)\n",
    "model.fit(dataset=dataset, test_size=12)\n",
    "y_hat_test = model.predict(dataset=dataset, step_size=1)\n",
    "np.testing.assert_almost_equal(\n",
    "    y_hat, \n",
    "    y_hat_test,\n",
    "    decimal=4\n",
    ")\n",
    "#test we recover the same forecast\n",
    "y_hat_test2 = model.predict(dataset=dataset, step_size=1)\n",
    "test_eq(y_hat_test, y_hat_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a481bb-65df-444f-9ade-ec23c71d5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test validation step\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n",
    "model = MLP(input_size=24, h=12, step_size=1, \n",
    "            hidden_size=1024, num_layers=2,\n",
    "            max_epochs=1)\n",
    "model.fit(dataset=dataset, val_size=12)\n",
    "y_hat_w_val = model.predict(dataset=dataset)\n",
    "Y_test_df['MLP'] = y_hat_w_val\n",
    "\n",
    "pd.concat([Y_train_df, Y_test_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210aa762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from neuralforecast.losses.pytorch import MQLoss\n",
    "\n",
    "Y_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\n",
    "Y_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test\n",
    "\n",
    "# Fit MQ-MLP\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n",
    "model = MLP(input_size=24, h=12, \n",
    "            loss=MQLoss(level=[80,90]),\n",
    "            max_epochs=1)\n",
    "model.fit(dataset=dataset)\n",
    "\n",
    "# Parse quantile predictions\n",
    "y_hat = model.predict(dataset=dataset)\n",
    "Y_hat_df = pd.DataFrame.from_records(data=y_hat,\n",
    "    columns=['MLP'+q for q in model.loss.output_names],\n",
    "    index=Y_test_df.index)\n",
    "\n",
    "# Plot quantile predictions\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df]).drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['MLP-median'], c='blue', label='median')\n",
    "plt.fill_between(x=plot_df['ds'], \n",
    "                 y1=plot_df['MLP-lo-90'], y2=plot_df['MLP-hi-90'],\n",
    "                 alpha=0.4, label='level 90')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5df26-2bc7-405b-a421-2cace4a39d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test no leakage with test_size and val_size\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_df)\n",
    "model = MLP(input_size=24, h=12, step_size=1, \n",
    "            hidden_size=1024, num_layers=2,\n",
    "            max_epochs=1)\n",
    "model.fit(dataset=dataset, val_size=12, test_size=12)\n",
    "y_hat_test_w_val = model.predict(dataset=dataset, step_size=1)\n",
    "np.testing.assert_almost_equal(y_hat_test_w_val, \n",
    "                               y_hat_w_val, decimal=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
