{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.nbeatsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# NBEATSx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1822c9e8",
   "metadata": {},
   "source": [
    "The Neural Basis Expansion Analysis (`NBEATS`) is an `MLP`-based deep neural architecture with backward and forward residual links. The network has two variants: (1) in its interpretable configuration, `NBEATS` sequentially projects the signal into polynomials and harmonic basis to learn trend and seasonality components; (2) in its generic configuration, it substitutes the polynomial and harmonic basis for identity basis and larger network's depth. The Neural Basis Expansion Analysis with Exogenous (`NBEATSx`), incorporates projections to exogenous temporal variables available at the time of the prediction.<br><br> This method proved state-of-the-art performance on the M3, M4, and Tourism Competition datasets, improving accuracy by 3% over the `ESRNN` M4 competition winner. For Electricity Price Forecasting tasks `NBEATSx` model improved accuracy by 20% and 5% over `ESRNN` and `NBEATS`, and 5% on task-specialized architectures.<br><br>**References**<br>-[Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2019). \"N-BEATS: Neural basis expansion analysis for interpretable time series forecasting\".](https://arxiv.org/abs/1905.10437)<br>-[Kin G. Olivares, Cristian Challu, Grzegorz Marcjasz, Rafa≈Ç Weron, Artur Dubrawski (2021). \"Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx\".](https://arxiv.org/abs/2104.05522)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddd17a6",
   "metadata": {},
   "source": [
    "![Figure 1. Neural Basis Expansion Analysis with Exogenous Variables.](imgs_models/nbeatsx.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508f7a9-1433-4ad8-8f2f-0078c6ed6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44065066-e72a-431f-938f-1528adef9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_windows import BaseWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a9fae-2c29-47e2-874e-ca1f20bf7040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class IdentityBasis(nn.Module):\n",
    "    def __init__(self, backcast_size: int, forecast_size: int):\n",
    "        super().__init__()\n",
    "        self.forecast_size = forecast_size\n",
    "        self.backcast_size = backcast_size\n",
    " \n",
    "    def forward(self, theta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        backcast = theta[:, :self.backcast_size]\n",
    "        forecast = theta[:, -self.forecast_size:]\n",
    "        return backcast, forecast\n",
    "\n",
    "class TrendBasis(nn.Module):\n",
    "    def __init__(self, degree_of_polynomial: int, backcast_size: int, forecast_size: int):\n",
    "        super().__init__()\n",
    "        polynomial_size = degree_of_polynomial + 1\n",
    "        self.backcast_basis = nn.Parameter(\n",
    "            torch.tensor(np.concatenate([np.power(np.arange(backcast_size, dtype=float) / backcast_size, i)[None, :]\n",
    "                                    for i in range(polynomial_size)]), dtype=torch.float32), requires_grad=False)\n",
    "        self.forecast_basis = nn.Parameter(\n",
    "            torch.tensor(np.concatenate([np.power(np.arange(forecast_size, dtype=float) / forecast_size, i)[None, :]\n",
    "                                    for i in range(polynomial_size)]), dtype=torch.float32), requires_grad=False)\n",
    "    \n",
    "    def forward(self, theta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        cut_point = self.forecast_basis.shape[0]\n",
    "        backcast = torch.einsum('bp,pt->bt', theta[:, cut_point:], self.backcast_basis)\n",
    "        forecast = torch.einsum('bp,pt->bt', theta[:, :cut_point], self.forecast_basis)\n",
    "        return backcast, forecast\n",
    "\n",
    "class SeasonalityBasis(nn.Module):\n",
    "    def __init__(self, harmonics: int, backcast_size: int, forecast_size: int):\n",
    "        super().__init__()\n",
    "        frequency = np.append(np.zeros(1, dtype=float),\n",
    "                                        np.arange(harmonics, harmonics / 2 * forecast_size,\n",
    "                                                    dtype=float) / harmonics)[None, :]\n",
    "        backcast_grid = -2 * np.pi * (\n",
    "                np.arange(backcast_size, dtype=float)[:, None] / forecast_size) * frequency\n",
    "        forecast_grid = 2 * np.pi * (\n",
    "                np.arange(forecast_size, dtype=float)[:, None] / forecast_size) * frequency\n",
    "\n",
    "        backcast_cos_template = torch.tensor(np.transpose(np.cos(backcast_grid)), dtype=torch.float32)\n",
    "        backcast_sin_template = torch.tensor(np.transpose(np.sin(backcast_grid)), dtype=torch.float32)\n",
    "        backcast_template = torch.cat([backcast_cos_template, backcast_sin_template], dim=0)\n",
    "\n",
    "        forecast_cos_template = torch.tensor(np.transpose(np.cos(forecast_grid)), dtype=torch.float32)\n",
    "        forecast_sin_template = torch.tensor(np.transpose(np.sin(forecast_grid)), dtype=torch.float32)\n",
    "        forecast_template = torch.cat([forecast_cos_template, forecast_sin_template], dim=0)\n",
    "\n",
    "        self.backcast_basis = nn.Parameter(backcast_template, requires_grad=False)\n",
    "        self.forecast_basis = nn.Parameter(forecast_template, requires_grad=False)\n",
    "\n",
    "    def forward(self, theta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        cut_point = self.forecast_basis.shape[0]\n",
    "        backcast = torch.einsum('bp,pt->bt', theta[:, cut_point:], self.backcast_basis)\n",
    "        forecast = torch.einsum('bp,pt->bt', theta[:, :cut_point], self.forecast_basis)\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17382790-7d84-4a89-959b-5676afa46392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "ACTIVATIONS = ['ReLU',\n",
    "               'Softplus',\n",
    "               'Tanh',\n",
    "               'SELU',\n",
    "               'LeakyReLU',\n",
    "               'PReLU',\n",
    "               'Sigmoid']\n",
    "\n",
    "class NBEATSBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    N-BEATS block which takes a basis function as an argument.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_size: int,\n",
    "                 h: int,\n",
    "                 futr_exog_size: int,\n",
    "                 hist_exog_size: int,\n",
    "                 stat_exog_size: int,\n",
    "                 n_theta: int, \n",
    "                 mlp_units: list,\n",
    "                 basis: nn.Module, \n",
    "                 dropout_prob: float, \n",
    "                 activation: str):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.futr_exog_size = futr_exog_size\n",
    "        self.hist_exog_size = hist_exog_size\n",
    "        self.stat_exog_size = stat_exog_size\n",
    "        \n",
    "        assert activation in ACTIVATIONS, f'{activation} is not in {ACTIVATIONS}'\n",
    "        activ = getattr(nn, activation)()\n",
    "\n",
    "        # Input vector for the block is y_lags (input_size) + historical exogenous (hist_exog_size*input_size)\n",
    "        #  + future exogenous (futr_exog_size*input_size) + static exogenous (stat_exog_size)\n",
    "        # [ Y_[t-L:t], X_[t-L:t], F_[t-L:t+H], S ]\n",
    "        input_size = input_size + hist_exog_size*input_size + \\\n",
    "                     futr_exog_size*(input_size + h) + stat_exog_size\n",
    "        \n",
    "        hidden_layers = [nn.Linear(in_features=input_size,\n",
    "                                   out_features=mlp_units[0][0])]\n",
    "        for layer in mlp_units:\n",
    "            hidden_layers.append(nn.Linear(in_features=layer[0], \n",
    "                                           out_features=layer[1]))\n",
    "            hidden_layers.append(activ)\n",
    "\n",
    "            if self.dropout_prob>0:\n",
    "                raise NotImplementedError('dropout')\n",
    "                #hidden_layers.append(nn.Dropout(p=self.dropout_prob))\n",
    "\n",
    "        output_layer = [nn.Linear(in_features=mlp_units[-1][1], out_features=n_theta)]\n",
    "        layers = hidden_layers + output_layer\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.basis = basis\n",
    "\n",
    "    def forward(self, insample_y: torch.Tensor, futr_exog: torch.Tensor,\n",
    "                hist_exog: torch.Tensor, stat_exog: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        # Flatten MLP inputs [B, L+H, C] -> [B, (L+H)*C]\n",
    "        # Contatenate [ Y_t, | X_{t-L},..., X_{t} | F_{t-L},..., F_{t+H} | S ]\n",
    "        batch_size = len(insample_y)\n",
    "        if self.hist_exog_size > 0:\n",
    "            insample_y = torch.cat(( insample_y, hist_exog.reshape(batch_size,-1) ), dim=1)\n",
    "\n",
    "        if self.futr_exog_size > 0:\n",
    "            insample_y = torch.cat(( insample_y, futr_exog.reshape(batch_size,-1) ), dim=1)\n",
    "\n",
    "        if self.stat_exog_size > 0:\n",
    "            insample_y = torch.cat(( insample_y, stat_exog.reshape(batch_size,-1) ), dim=1)\n",
    "            \n",
    "        # Compute local projection weights and projection\n",
    "        theta = self.layers(insample_y)\n",
    "        backcast, forecast = self.basis(theta)\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be997aeb-778f-442d-a97a-ff47de2deab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NBEATSx(BaseWindows):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 h,\n",
    "                 n_polynomials=2,\n",
    "                 n_harmonics=2,\n",
    "                 stack_types: list = ['identity', 'trend', 'seasonality'],\n",
    "                 n_blocks: list = [1, 1, 1],\n",
    "                 mlp_units: list = 3 * [[512, 512]],\n",
    "                 dropout_prob_theta = 0.,\n",
    "                 activation = 'ReLU',\n",
    "                 shared_weights=False,\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 learning_rate=1e-3,\n",
    "                 scaler_type=None,\n",
    "                 windows_batch_size: int = 1024,\n",
    "                 step_size: int = 1,\n",
    "                 loss=MAE(),\n",
    "                 batch_size=32, \n",
    "                 num_workers_loader=0,\n",
    "                 drop_last_loader=False,\n",
    "                 random_seed=1,\n",
    "                 **trainer_kwargs):\n",
    "        \"\"\"\n",
    "        N-BEATSx Model.\n",
    "        \n",
    "        **Parameters:**<br>\n",
    "        `input_size`: int, insample_size.<br>\n",
    "        `h`: int, Forecast horizon. <br>\n",
    "        `shared_weights`: bool, If True, all blocks within each stack will share parameters. <br>\n",
    "        `activation`: str, Activation function. An item from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid']. <br>\n",
    "        `stack_types`: List[str], List of stack types. Subset from ['seasonality', 'trend', 'identity'].<br>\n",
    "        `n_blocks`: List[int], Number of blocks for each stack. Note that len(n_blocks) = len(stack_types).<br>\n",
    "        `mlp_units`: List[List[int]], Structure of hidden layers for each stack type. Each internal list should contain the number of units of each hidden layer. Note that len(n_hidden) = len(stack_types).<br>\n",
    "        `n_harmonics`: int, Number of harmonic terms for trend stack type. Note that len(n_harmonics) = len(stack_types). Note that it will only be used if a trend stack is used.<br>\n",
    "        `n_polynomials`: int, Number of polynomial terms for seasonality stack type. Note that len(n_polynomials) = len(stack_types). Note that it will only be used if a seasonality stack is used.<br>\n",
    "        `futr_exog_list`: List[str], List of future exogenous variables. <br>\n",
    "        `hist_exog_list`: List[str], List of historic exogenous variables. <br>\n",
    "        `dropout_prob_theta`: float, Float between (0, 1). Dropout for N-BEATS basis.<br>\n",
    "        `learning_rate`: float, Learning rate between (0, 1).<br>\n",
    "        `loss`: Callable, Loss to optimize.<br>\n",
    "        `random_seed`: int, random_seed for pseudo random pytorch initializer and numpy random generator.<br>\n",
    "        \"\"\"\n",
    "        # Inherit BaseWindows class\n",
    "        super(NBEATSx, self).__init__(h=h, \n",
    "                                     loss=loss,\n",
    "                                     batch_size=batch_size,\n",
    "                                     scaler_type=scaler_type,\n",
    "                                     futr_exog_list=futr_exog_list,\n",
    "                                     hist_exog_list=hist_exog_list,\n",
    "                                     stat_exog_list=stat_exog_list,\n",
    "                                     num_workers_loader=num_workers_loader,\n",
    "                                     drop_last_loader=drop_last_loader,\n",
    "                                     random_seed=random_seed,\n",
    "                                     **trainer_kwargs)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.windows_batch_size = windows_batch_size\n",
    "        self.step_size = step_size\n",
    "        self.futr_exog_size = len(self.futr_exog_list)\n",
    "        self.hist_exog_size = len(self.hist_exog_list)\n",
    "        self.stat_exog_size = len(self.stat_exog_list)\n",
    "\n",
    "        blocks = self.create_stack(stack_types=stack_types, \n",
    "                                   n_blocks=n_blocks,\n",
    "                                   input_size=input_size,\n",
    "                                   h=h,\n",
    "                                   mlp_units=mlp_units,\n",
    "                                   dropout_prob_theta=dropout_prob_theta,\n",
    "                                   activation=activation,\n",
    "                                   shared_weights=shared_weights,\n",
    "                                   n_polynomials=n_polynomials, \n",
    "                                   n_harmonics=n_harmonics,\n",
    "                                   futr_exog_size=self.futr_exog_size,\n",
    "                                   hist_exog_size=self.hist_exog_size,\n",
    "                                   stat_exog_size=self.stat_exog_size)\n",
    "        self.blocks = torch.nn.ModuleList(blocks)\n",
    "        \n",
    "        # Adapter with Loss dependent dimensions\n",
    "        if self.loss.outputsize_multiplier > 1:\n",
    "            self.out = nn.Linear(in_features=h,\n",
    "                        out_features=h*self.loss.outputsize_multiplier)\n",
    "\n",
    "    def create_stack(self, stack_types, \n",
    "                     n_blocks, \n",
    "                     input_size, \n",
    "                     h, \n",
    "                     mlp_units, \n",
    "                     dropout_prob_theta, \n",
    "                     activation, shared_weights,\n",
    "                     n_polynomials, n_harmonics,\n",
    "                     futr_exog_size, hist_exog_size, stat_exog_size):                    \n",
    "\n",
    "        block_list = []\n",
    "        for i in range(len(stack_types)):\n",
    "            for block_id in range(n_blocks[i]):\n",
    "\n",
    "                # Shared weights\n",
    "                if shared_weights and block_id>0:\n",
    "                    nbeats_block = block_list[-1]\n",
    "                else:\n",
    "                    if stack_types[i] == 'seasonality':\n",
    "                        n_theta = 4 * int(np.ceil(n_harmonics / 2 * h) - (n_harmonics - 1))\n",
    "                        basis = SeasonalityBasis(harmonics=n_harmonics,\n",
    "                                                 backcast_size=input_size,\n",
    "                                                 forecast_size=h)\n",
    "\n",
    "                    elif stack_types[i] == 'trend':\n",
    "                        n_theta = 2 * (n_polynomials + 1)\n",
    "                        basis = TrendBasis(degree_of_polynomial=n_polynomials,\n",
    "                                           backcast_size=input_size,\n",
    "                                           forecast_size=h)\n",
    "\n",
    "                    elif stack_types[i] == 'identity':\n",
    "                        n_theta = input_size + h\n",
    "                        basis = IdentityBasis(backcast_size=input_size,\n",
    "                                              forecast_size=h)                      \n",
    "                    else:\n",
    "                        raise ValueError(f'Block type {stack_types[i]} not found!')\n",
    "\n",
    "                    nbeats_block = NBEATSBlock(input_size=input_size,\n",
    "                                               h=h,\n",
    "                                               futr_exog_size=futr_exog_size,\n",
    "                                               hist_exog_size=hist_exog_size,\n",
    "                                               stat_exog_size=stat_exog_size,\n",
    "                                               n_theta=n_theta,\n",
    "                                               mlp_units=mlp_units,\n",
    "                                               basis=basis,\n",
    "                                               dropout_prob=dropout_prob_theta,\n",
    "                                               activation=activation)\n",
    "\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                block_list.append(nbeats_block)\n",
    "                \n",
    "        return block_list\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        \n",
    "        # Parse windows_batch\n",
    "        insample_y    = windows_batch['insample_y']\n",
    "        insample_mask = windows_batch['insample_mask']\n",
    "        futr_exog     = windows_batch['futr_exog']\n",
    "        hist_exog     = windows_batch['hist_exog']\n",
    "        stat_exog     = windows_batch['stat_exog']\n",
    "\n",
    "        # insample\n",
    "        residuals = insample_y.flip(dims=(-1,)) #backcast init\n",
    "        insample_mask = insample_mask.flip(dims=(-1,))\n",
    "        forecast = insample_y[:, -1:] # Level with Naive1\n",
    "\n",
    "        block_forecasts = [ forecast.repeat(1, self.h) ]\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            backcast, block_forecast = block(insample_y=residuals, futr_exog=futr_exog,\n",
    "                                             hist_exog=hist_exog, stat_exog=stat_exog)\n",
    "            residuals = (residuals - backcast) * insample_mask\n",
    "            forecast = forecast + block_forecast\n",
    "            \n",
    "            if self.decompose_forecast:\n",
    "                block_forecasts.append(block_forecast)\n",
    "\n",
    "        if self.decompose_forecast:\n",
    "            # (n_batch, n_blocks, h)\n",
    "            block_forecasts = torch.stack(block_forecasts)\n",
    "            block_forecasts = block_forecasts.permute(1,0,2)\n",
    "            return block_forecasts\n",
    "        else:\n",
    "            \n",
    "            # Last dimension Adapter\n",
    "            if self.loss.outputsize_multiplier > 1:\n",
    "                forecast = forecast[:,:,None] + \\\n",
    "                           self.loss.adapt_output(self.out(forecast))\n",
    "            return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a831f-94bc-4616-b579-c114c3fc57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NBEATSx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9013b63-f65b-4a92-913c-b696e6e69914",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NBEATSx.fit, name='NBEATSx.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66184ee-7a71-4598-976c-c79b83089a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NBEATSx.predict, name='NBEATSx.predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf3e1d-2935-4503-afb6-fe3d6f52622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4c6c6-ef60-47c9-8c90-4002e68410d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from neuralforecast.utils import AirPassengersDF as Y_df\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n",
    "\n",
    "# Month\n",
    "Y_df['month'] = Y_df['ds'].dt.month\n",
    "Y_df['year'] = Y_df['ds'].dt.year\n",
    "\n",
    "Y_train_df = Y_df[Y_df.ds<Y_df['ds'].values[-12]] # 132 train\n",
    "Y_test_df = Y_df[Y_df.ds>=Y_df['ds'].values[-12]]   # 12 test\n",
    "\n",
    "dataset, *_ = TimeSeriesDataset.from_df(df = Y_train_df)\n",
    "model = NBEATSx(input_size=24,\n",
    "                 h=12,\n",
    "                 scaler_type='robust',\n",
    "                 futr_exog_list=['month','year'],\n",
    "                 windows_batch_size=None,\n",
    "                 max_epochs=1)\n",
    "model.fit(dataset=dataset)\n",
    "dataset._update_futr_exog(Y_test_df)\n",
    "model.set_test_size(12)\n",
    "y_hat = model.predict(dataset=dataset)\n",
    "Y_test_df['NBEATSx'] = y_hat\n",
    "\n",
    "pd.concat([Y_train_df, Y_test_df]).drop(['unique_id','month'], axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94b63e-d82c-423f-8f75-184ae285904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test we recover the same forecast\n",
    "y_hat2 = model.predict(dataset=dataset)\n",
    "test_eq(y_hat, y_hat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46090447-8e67-4f08-8a3d-9547183983f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test no leakage with test_size\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_df)\n",
    "model = NBEATSx(input_size=24,\n",
    "                 h=12,\n",
    "                 scaler_type='robust',\n",
    "                 futr_exog_list=['month','year'],\n",
    "                 windows_batch_size=None,\n",
    "                 max_epochs=1)\n",
    "model.fit(dataset=dataset, test_size=12)\n",
    "y_hat_test = model.predict(dataset=dataset, step_size=1)\n",
    "np.testing.assert_almost_equal(y_hat, y_hat_test, decimal=4)\n",
    "#test we recover the same forecast\n",
    "y_hat_test2 = model.predict(dataset=dataset, step_size=1)\n",
    "test_eq(y_hat_test, y_hat_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from neuralforecast.losses.pytorch import MQLoss\n",
    "\n",
    "Y_train_df = Y_df[Y_df.ds<Y_df['ds'].values[-12]] # 132 train\n",
    "Y_test_df = Y_df[Y_df.ds>=Y_df['ds'].values[-12]]   # 12 test\n",
    "\n",
    "# Fit MQ-MLP\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n",
    "model = NBEATSx(input_size=24, h=12, max_epochs=1,\n",
    "                scaler_type='robust',\n",
    "                futr_exog_list=['month','year'],\n",
    "                loss=MQLoss(level=[80, 90]))\n",
    "model.fit(dataset=dataset, val_size=12)\n",
    "\n",
    "# Parse quantile predictions\n",
    "dataset._update_futr_exog(Y_test_df)\n",
    "model.set_test_size(12)\n",
    "y_hat = model.predict(dataset=dataset)\n",
    "Y_hat_df = pd.DataFrame.from_records(data=y_hat,\n",
    "                columns=['NBEATS'+q for q in model.loss.output_names],\n",
    "                index=Y_test_df.index)\n",
    "\n",
    "# Plot quantile predictions\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df]).drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['NBEATS-median'], c='blue', label='median')\n",
    "plt.fill_between(x=plot_df['ds'], \n",
    "                 y1=plot_df['NBEATS-lo-90'], y2=plot_df['NBEATS-hi-90'],\n",
    "                 alpha=0.4, label='level 90')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298fce5-eb13-40dc-9964-b026fd2a8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test validation step\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n",
    "model = NBEATSx(input_size=24, h=12, windows_batch_size=None, max_epochs=1, scaler_type='robust',\n",
    "                futr_exog_list=['month','year'])\n",
    "model.fit(dataset=dataset, val_size=12)\n",
    "dataset._update_futr_exog(Y_test_df)\n",
    "model.set_test_size(12)\n",
    "y_hat_w_val = model.predict(dataset=dataset)\n",
    "Y_test_df['N-BEATS'] = y_hat_w_val\n",
    "\n",
    "pd.concat([Y_train_df, Y_test_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f987ed0-ee6e-4f66-bd8f-96acc6fbd56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test no leakage with test_size and val_size\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n",
    "model = NBEATSx(input_size=24, h=12, windows_batch_size=None, max_epochs=1,\n",
    "                scaler_type='robust',futr_exog_list=['month','year'])\n",
    "model.fit(dataset=dataset, val_size=12)\n",
    "dataset._update_futr_exog(Y_test_df)\n",
    "model.set_test_size(12)\n",
    "y_hat_w_val = model.predict(dataset=dataset)\n",
    "\n",
    "dataset, *_ = TimeSeriesDataset.from_df(Y_df)\n",
    "model = NBEATSx(input_size=24, h=12, windows_batch_size=None, max_epochs=1,\n",
    "                scaler_type='robust', futr_exog_list=['month','year'])\n",
    "model.fit(dataset=dataset, val_size=12, test_size=12)\n",
    "\n",
    "y_hat_test_w_val = model.predict(dataset=dataset, step_size=1)\n",
    "\n",
    "np.testing.assert_almost_equal(y_hat_test_w_val, y_hat_w_val, decimal=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036be4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.decompose(dataset=dataset)\n",
    "\n",
    "fig, ax = plt.subplots(5, 1, figsize=(10, 15))\n",
    "\n",
    "ax[0].plot(Y_test_df['y'].values, label='True', color=\"#9C9DB2\", linewidth=4)\n",
    "ax[0].plot(y_hat.sum(axis=1).flatten(), label='Forecast', color=\"#7B3841\")\n",
    "ax[0].grid()\n",
    "ax[0].legend(prop={'size': 20})\n",
    "for label in (ax[0].get_xticklabels() + ax[0].get_yticklabels()):\n",
    "    label.set_fontsize(18)\n",
    "ax[0].set_ylabel('y', fontsize=20)\n",
    "\n",
    "ax[1].plot(y_hat[0,0], label='level', color=\"#7B3841\")\n",
    "ax[1].grid()\n",
    "ax[1].set_ylabel('Level', fontsize=20)\n",
    "\n",
    "ax[2].plot(y_hat[0,1], label='stack1', color=\"#7B3841\")\n",
    "ax[2].grid()\n",
    "ax[2].set_ylabel('Identity', fontsize=20)\n",
    "\n",
    "ax[3].plot(y_hat[0,2], label='stack2', color=\"#D9AE9E\")\n",
    "ax[3].grid()\n",
    "ax[3].set_ylabel('Trend', fontsize=20)\n",
    "\n",
    "ax[4].plot(y_hat[0,3], label='stack3', color=\"#D9AE9E\")\n",
    "ax[4].grid()\n",
    "ax[4].set_ylabel('Seasonality', fontsize=20)\n",
    "\n",
    "ax[4].set_xlabel('Prediction \\u03C4 \\u2208 {t+1,..., t+H}', fontsize=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
