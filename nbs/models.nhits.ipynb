{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.nhits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-HiTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_windows import BaseWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _IdentityBasis(nn.Module):\n",
    "    def __init__(self, backcast_size: int, forecast_size: int, interpolation_mode: str):\n",
    "        super().__init__()\n",
    "        assert (interpolation_mode in ['linear','nearest']) or ('cubic' in interpolation_mode)\n",
    "        self.forecast_size = forecast_size\n",
    "        self.backcast_size = backcast_size\n",
    "        self.interpolation_mode = interpolation_mode\n",
    " \n",
    "    def forward(self, theta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        backcast = theta[:, :self.backcast_size]\n",
    "        knots = theta[:, self.backcast_size:]\n",
    "\n",
    "        if self.interpolation_mode=='nearest':\n",
    "            knots = knots[:,None,:]\n",
    "            forecast = F.interpolate(knots, size=self.forecast_size, mode=self.interpolation_mode)\n",
    "            forecast = forecast[:,0,:]\n",
    "        elif self.interpolation_mode=='linear':\n",
    "            knots = knots[:,None,:]\n",
    "            forecast = F.interpolate(knots, size=self.forecast_size, mode=self.interpolation_mode)\n",
    "            forecast = forecast[:,0,:]\n",
    "        elif 'cubic' in self.interpolation_mode:\n",
    "            batch_size = len(backcast)\n",
    "            knots = knots[:,None,None,:]\n",
    "            forecast = torch.zeros((len(knots), self.forecast_size)).to(knots.device)\n",
    "            n_batches = int(np.ceil(len(knots)/batch_size))\n",
    "            for i in range(n_batches):\n",
    "                forecast_i = F.interpolate(knots[i*batch_size:(i+1)*batch_size], size=self.forecast_size, mode='bicubic')\n",
    "                forecast[i*batch_size:(i+1)*batch_size] += forecast_i[:,0,0,:]\n",
    "\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "ACTIVATIONS = ['ReLU',\n",
    "               'Softplus',\n",
    "               'Tanh',\n",
    "               'SELU',\n",
    "               'LeakyReLU',\n",
    "               'PReLU',\n",
    "               'Sigmoid']\n",
    "\n",
    "POOLING = ['MaxPool1d',\n",
    "           'AvgPool1d']\n",
    "\n",
    "class NHITSBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    N-HiTS block which takes a basis function as an argument.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_size: int,\n",
    "                 h: int,\n",
    "                 n_theta: int,\n",
    "                 mlp_units: list,\n",
    "                 basis: nn.Module,\n",
    "                 futr_exog_size: int,\n",
    "                 hist_exog_size: int,\n",
    "                 stat_exog_size: int,\n",
    "                 n_pool_kernel_size: int,\n",
    "                 pooling_mode: str,\n",
    "                 dropout_prob: float,\n",
    "                 activation: str):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        pooled_hist_size = int(np.ceil(input_size/n_pool_kernel_size))\n",
    "        pooled_futr_size = int(np.ceil((input_size+h)/n_pool_kernel_size))\n",
    "\n",
    "        input_size = pooled_hist_size + \\\n",
    "                     hist_exog_size*pooled_hist_size + \\\n",
    "                     futr_exog_size*(pooled_futr_size) + stat_exog_size\n",
    "\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.futr_exog_size = futr_exog_size\n",
    "        self.hist_exog_size = hist_exog_size\n",
    "        self.stat_exog_size = stat_exog_size\n",
    "        \n",
    "        assert activation in ACTIVATIONS, f'{activation} is not in {ACTIVATIONS}'\n",
    "        assert pooling_mode in POOLING, f'{pooling_mode} is not in {POOLING}'\n",
    "\n",
    "        activ = getattr(nn, activation)()\n",
    "\n",
    "        self.pooling_layer = getattr(nn, pooling_mode)(kernel_size=n_pool_kernel_size,\n",
    "                                                       stride=n_pool_kernel_size, ceil_mode=True)\n",
    "\n",
    "        # Block MLPs\n",
    "        hidden_layers = [nn.Linear(in_features=input_size, \n",
    "                                   out_features=mlp_units[0][0])]\n",
    "        for layer in mlp_units:\n",
    "            hidden_layers.append(nn.Linear(in_features=layer[0], \n",
    "                                           out_features=layer[1]))\n",
    "            hidden_layers.append(activ)\n",
    "\n",
    "            if self.dropout_prob>0:\n",
    "                raise NotImplementedError('dropout')\n",
    "                #hidden_layers.append(nn.Dropout(p=self.dropout_prob))\n",
    "\n",
    "        output_layer = [nn.Linear(in_features=mlp_units[-1][1], out_features=n_theta)]\n",
    "        layers = hidden_layers + output_layer\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.basis = basis\n",
    "\n",
    "    def forward(self, insample_y: torch.Tensor, futr_exog: torch.Tensor,\n",
    "                hist_exog: torch.Tensor, stat_exog: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        # Pooling\n",
    "        # Pool1d needs 3D input, (B,C,L), adding C dimension\n",
    "        insample_y = insample_y.unsqueeze(1)\n",
    "        insample_y = self.pooling_layer(insample_y)\n",
    "        insample_y = insample_y.squeeze(1)\n",
    "\n",
    "        # Flatten MLP inputs [B, L+H, C] -> [B, (L+H)*C]\n",
    "        # Contatenate [ Y_t, | X_{t-L},..., X_{t} | F_{t-L},..., F_{t+H} | S ]\n",
    "        batch_size = len(insample_y)\n",
    "        if self.hist_exog_size > 0:\n",
    "            hist_exog = hist_exog.permute(0,2,1) # [B, L, C] -> [B, C, L]\n",
    "            hist_exog = self.pooling_layer(hist_exog)\n",
    "            hist_exog = hist_exog.permute(0,2,1) # [B, C, L] -> [B, L, C]\n",
    "            insample_y = torch.cat(( insample_y, hist_exog.reshape(batch_size,-1) ), dim=1)\n",
    "\n",
    "        if self.futr_exog_size > 0:\n",
    "            futr_exog = futr_exog.permute(0,2,1) # [B, L, C] -> [B, C, L]\n",
    "            futr_exog = self.pooling_layer(futr_exog)\n",
    "            futr_exog = futr_exog.permute(0,2,1) # [B, C, L] -> [B, L, C]\n",
    "            insample_y = torch.cat(( insample_y, futr_exog.reshape(batch_size,-1) ), dim=1)\n",
    "\n",
    "        if self.stat_exog_size > 0:\n",
    "            insample_y = torch.cat(( insample_y, stat_exog.reshape(batch_size,-1) ), dim=1)\n",
    "            \n",
    "        # Compute local projection weights and projection\n",
    "        theta = self.layers(insample_y)\n",
    "        backcast, forecast = self.basis(theta)\n",
    "        return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NHITS(BaseWindows):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 h,\n",
    "                 stack_types: list = ['identity', 'identity', 'identity'],\n",
    "                 n_blocks: list = [1, 1, 1],\n",
    "                 mlp_units: list = 3 * [[512, 512]],\n",
    "                 n_pool_kernel_size: list = [2, 2, 1],\n",
    "                 n_freq_downsample: list = [4, 2, 1],\n",
    "                 pooling_mode: str = 'MaxPool1d',\n",
    "                 interpolation_mode: str = 'linear',\n",
    "                 dropout_prob_theta = 0.,\n",
    "                 activation = 'ReLU',\n",
    "                 futr_exog_list = None,\n",
    "                 hist_exog_list = None,\n",
    "                 stat_exog_list = None,\n",
    "                 learning_rate=1e-3,\n",
    "                 normalize=False,\n",
    "                 windows_batch_size: int = 1024,\n",
    "                 step_size: int = 1,\n",
    "                 loss=MAE(),\n",
    "                 batch_size=32, \n",
    "                 num_workers_loader=0,\n",
    "                 drop_last_loader=False,\n",
    "                 random_seed=1,\n",
    "                 **trainer_kwargs):\n",
    "        \"\"\"\n",
    "        N-HiTS Model.\n",
    "        \n",
    "        **Parameters:**<br>\n",
    "        `input_size`: int, insample_size.<br>\n",
    "        `h`: int, Forecast horizon. <br>\n",
    "        `shared_weights`: bool, If True, all blocks within each stack will share parameters. <br>\n",
    "        `activation`: str, Activation function. An item from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid']. <br>\n",
    "        `futr_exog_list`: List[str], List of future exogenous variables. <br>\n",
    "        `hist_exog_list`: List[str], List of historic exogenous variables. <br>\n",
    "        `stack_types`: List[str], List of stack types. Subset from ['seasonality', 'trend', 'identity'].<br>\n",
    "        `n_blocks`: List[int], Number of blocks for each stack. Note that len(n_blocks) = len(stack_types).<br>\n",
    "        `mlp_units`: List[List[int]], Structure of hidden layers for each stack type. Each internal list should contain the number of units of each hidden layer. Note that len(n_hidden) = len(stack_types).<br>\n",
    "        `n_harmonics`: int, Number of harmonic terms for trend stack type. Note that len(n_harmonics) = len(stack_types). Note that it will only be used if a trend stack is used.<br>\n",
    "        `n_polynomials`: int, Number of polynomial terms for seasonality stack type. Note that len(n_polynomials) = len(stack_types). Note that it will only be used if a seasonality stack is used.<br>\n",
    "        `dropout_prob_theta`: float, Float between (0, 1). Dropout for N-BEATS basis.<br>\n",
    "        `learning_rate`: float, Learning rate between (0, 1).<br>\n",
    "        `loss`: Callable, Loss to optimize.<br>\n",
    "        `random_seed`: int, random_seed for pseudo random pytorch initializer and numpy random generator.<br>\n",
    "        \"\"\"\n",
    "        super(NHITS, self).__init__(h=h, \n",
    "                                    loss=loss,\n",
    "                                    batch_size=batch_size,\n",
    "                                    normalize=normalize,\n",
    "                                    futr_exog_list=futr_exog_list,\n",
    "                                     hist_exog_list=hist_exog_list,\n",
    "                                     stat_exog_list=stat_exog_list,\n",
    "                                    num_workers_loader=num_workers_loader,\n",
    "                                    drop_last_loader=drop_last_loader,\n",
    "                                    random_seed=random_seed,\n",
    "                                    **trainer_kwargs)\n",
    "        self.input_size = input_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.windows_batch_size = windows_batch_size\n",
    "        self.step_size = step_size\n",
    "        self.futr_exog_size = len(self.futr_exog_list)\n",
    "        self.hist_exog_size = len(self.hist_exog_list)\n",
    "        self.stat_exog_size = len(self.stat_exog_list)\n",
    "\n",
    "        blocks = self.create_stack(stack_types=stack_types, \n",
    "                                   n_blocks=n_blocks,\n",
    "                                   input_size=input_size,\n",
    "                                   h=h,\n",
    "                                   mlp_units=mlp_units,\n",
    "                                   n_pool_kernel_size=n_pool_kernel_size,\n",
    "                                   n_freq_downsample=n_freq_downsample,\n",
    "                                   pooling_mode=pooling_mode,\n",
    "                                   interpolation_mode=interpolation_mode,\n",
    "                                   dropout_prob_theta=dropout_prob_theta,\n",
    "                                   activation=activation,\n",
    "                                   futr_exog_size=self.futr_exog_size,\n",
    "                                   hist_exog_size=self.hist_exog_size,\n",
    "                                   stat_exog_size=self.stat_exog_size)\n",
    "        self.blocks = torch.nn.ModuleList(blocks)\n",
    "        \n",
    "        # Adapter with Loss dependent dimensions\n",
    "        if self.loss.outputsize_multiplier > 1:\n",
    "            self.out = nn.Linear(in_features=h,\n",
    "                        out_features=h*self.loss.outputsize_multiplier)\n",
    "\n",
    "    def create_stack(self, stack_types, \n",
    "                     n_blocks, \n",
    "                     input_size, \n",
    "                     h, \n",
    "                     mlp_units,\n",
    "                     n_pool_kernel_size,\n",
    "                     n_freq_downsample,\n",
    "                     pooling_mode,\n",
    "                     interpolation_mode,\n",
    "                     dropout_prob_theta, \n",
    "                     activation,\n",
    "                     futr_exog_size, hist_exog_size, stat_exog_size):                     \n",
    "\n",
    "        block_list = []\n",
    "        for i in range(len(stack_types)):\n",
    "            for block_id in range(n_blocks[i]):\n",
    "\n",
    "                assert stack_types[i] == 'identity', f'Block type {stack_types[i]} not found!'\n",
    "\n",
    "                n_theta = (input_size + max(h//n_freq_downsample[i], 1) )\n",
    "                basis = _IdentityBasis(backcast_size=input_size,\n",
    "                                       forecast_size=h,\n",
    "                                       interpolation_mode=interpolation_mode)                 \n",
    "\n",
    "                nbeats_block = NHITSBlock(input_size=input_size,\n",
    "                                          h=h,\n",
    "                                          n_theta=n_theta,\n",
    "                                          mlp_units=mlp_units,\n",
    "                                          n_pool_kernel_size=n_pool_kernel_size[i],\n",
    "                                          pooling_mode=pooling_mode,\n",
    "                                          basis=basis,\n",
    "                                          dropout_prob=dropout_prob_theta,\n",
    "                                          activation=activation,\n",
    "                                          futr_exog_size=futr_exog_size,\n",
    "                                          hist_exog_size=hist_exog_size,\n",
    "                                          stat_exog_size=stat_exog_size)\n",
    "\n",
    "                # Select type of evaluation and apply it to all layers of block\n",
    "                block_list.append(nbeats_block)\n",
    "                \n",
    "        return block_list\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        \n",
    "        # Parse windows_batch\n",
    "        insample_y    = windows_batch['insample_y']\n",
    "        insample_mask = windows_batch['insample_mask']\n",
    "        futr_exog     = windows_batch['futr_exog']\n",
    "        hist_exog     = windows_batch['hist_exog']\n",
    "        stat_exog     = windows_batch['stat_exog']\n",
    "        \n",
    "        # insample\n",
    "        residuals = insample_y.flip(dims=(-1,)) #backcast init\n",
    "        insample_mask = insample_mask.flip(dims=(-1,))\n",
    "        forecast = insample_y[:, -1:] # Level with Naive1\n",
    "\n",
    "        block_forecasts = [ forecast.repeat(1, self.h) ]\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            backcast, block_forecast = block(insample_y=residuals, futr_exog=futr_exog,\n",
    "                                             hist_exog=hist_exog, stat_exog=stat_exog)\n",
    "            residuals = (residuals - backcast) * insample_mask\n",
    "            forecast = forecast + block_forecast\n",
    "            if self.decompose_forecast:\n",
    "                block_forecasts.append(block_forecast)\n",
    "\n",
    "        if self.decompose_forecast:\n",
    "            # (n_batch, n_blocks, h)\n",
    "            block_forecasts = torch.stack(block_forecasts)\n",
    "            block_forecasts = block_forecasts.permute(1,0,2)\n",
    "            return block_forecasts\n",
    "        else:\n",
    "            \n",
    "            # Last dimension Adapter\n",
    "            if self.loss.outputsize_multiplier > 1:\n",
    "                forecast = forecast[:,:,None] + \\\n",
    "                          self.loss.adapt_output(self.out(forecast))\n",
    "            return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NHITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NHITS.fit, name='NHITS.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(NHITS.predict, name='NHITS.predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from neuralforecast.utils import AirPassengersDF as Y_df\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n",
    "\n",
    "Y_train_df = Y_df[Y_df.ds<Y_df['ds'].values[-24]] # 132 train\n",
    "Y_test_df = Y_df[Y_df.ds>=Y_df['ds'].values[-24]]   # 12 test\n",
    "\n",
    "dataset, *_ = TimeSeriesDataset.from_df(df = Y_train_df)\n",
    "model = NHITS(input_size=24*2, h=24, \n",
    "              max_epochs=1,\n",
    "              windows_batch_size=None, \n",
    "              n_freq_downsample=[12,4,1], \n",
    "              pooling_mode='MaxPool1d')\n",
    "model.fit(dataset=dataset)\n",
    "y_hat = model.predict(dataset=dataset)\n",
    "Y_test_df['N-HiTS'] = y_hat\n",
    "\n",
    "pd.concat([Y_train_df, Y_test_df]).drop('unique_id', axis=1).set_index('ds').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(5, 1, figsize=(10, 15))\n",
    "\n",
    "y_hat = model.decompose(dataset=dataset)\n",
    "ax[0].plot(Y_test_df['y'].values, label='True', color=\"#9C9DB2\", linewidth=4)\n",
    "ax[0].plot(y_hat.sum(axis=1).flatten(), label='Forecast', color=\"#7B3841\")\n",
    "ax[0].grid()\n",
    "ax[0].legend(prop={'size': 20})\n",
    "for label in (ax[0].get_xticklabels() + ax[0].get_yticklabels()):\n",
    "    label.set_fontsize(18)\n",
    "ax[0].set_ylabel('y', fontsize=20)\n",
    "\n",
    "ax[1].plot(y_hat[0,0], label='level', color=\"#7B3841\")\n",
    "ax[1].grid()\n",
    "ax[1].set_ylabel('Level', fontsize=20)\n",
    "\n",
    "ax[2].plot(y_hat[0,1], label='stack1', color=\"#7B3841\")\n",
    "ax[2].grid()\n",
    "ax[2].set_ylabel('Stack 1', fontsize=20)\n",
    "\n",
    "ax[3].plot(y_hat[0,2], label='stack2', color=\"#D9AE9E\")\n",
    "ax[3].grid()\n",
    "ax[3].set_ylabel('Stack 2', fontsize=20)\n",
    "\n",
    "ax[4].plot(y_hat[0,3], label='stack3', color=\"#D9AE9E\")\n",
    "ax[4].grid()\n",
    "ax[4].set_ylabel('Stack 3', fontsize=20)\n",
    "\n",
    "ax[4].set_xlabel('Prediction \\u03C4 \\u2208 {t+1,..., t+H}', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast.utils import AirPassengersDF as Y_df\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n",
    "from neuralforecast.losses.pytorch import MQLoss\n",
    "\n",
    "Y_train_df = Y_df[Y_df.ds<Y_df['ds'].values[-24]] # 132 train\n",
    "Y_test_df = Y_df[Y_df.ds>=Y_df['ds'].values[-24]]   # 12 test\n",
    "\n",
    "dataset, *_ = TimeSeriesDataset.from_df(df = Y_train_df)\n",
    "model = NHITS(input_size=24*2, h=24, \n",
    "              max_epochs=1, \n",
    "              windows_batch_size=None,\n",
    "              loss=MQLoss(level=[80,90]),\n",
    "              n_freq_downsample=[12,4,1],\n",
    "              pooling_mode='MaxPool1d')\n",
    "model.fit(dataset=dataset)\n",
    "yq_hat = model.predict(dataset=dataset)\n",
    "\n",
    "# Parse quantile predictions\n",
    "Y_hat_df = pd.DataFrame.from_records(data=yq_hat,\n",
    "                columns=['NHITS'+q for q in model.loss.output_names],\n",
    "                index=Y_test_df.index)\n",
    "\n",
    "# Plot quantile predictions\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = pd.concat([Y_train_df, plot_df]).drop('unique_id', axis=1)\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "plt.plot(plot_df['ds'], plot_df['NHITS-median'], c='blue', label='median')\n",
    "plt.fill_between(x=plot_df['ds'], \n",
    "                 y1=plot_df['NHITS-lo-90'], y2=plot_df['NHITS-hi-90'],\n",
    "                 alpha=0.4, label='level 90')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
