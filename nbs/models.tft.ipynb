{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Fusion Transformers\n",
    "\n",
    "> In summary Temporal Fusion Transformer (TFT) combines gating layers, an LSTM recurrent encoder, with multi-head attention layers for a multi-step forecasting strategy decoder.\n",
    "> TFT's inputs are static exogenous $\\mathbf{x}^{(s)}$, historic exogenous $\\mathbf{x}^{(h)}_{[:t]}$, exogenous available at the time of the prediction $\\mathbf{x}^{(f)}_{[:t+H]}$ and autorregresive features $\\mathbf{y}_{[:t]}$, each of these inputs is further decomposed into categorical and continuous. The network uses a multi-quantile regression to model the following conditional probability:\n",
    "$$\\mathbb{P}(\\mathbf{y}_{[t+1:t+H]}|\\;\\mathbf{y}_{[:t]},\\; \\mathbf{x}^{(h)}_{[:t]},\\; \\mathbf{x}^{(f)}_{[:t+H]},\\; \\mathbf{x}^{(s)})$$\n",
    "> <br>The first TFT's step is embed the original input $\\{\\mathbf{x}^{(s)}, \\mathbf{x}^{(h)}, \\mathbf{x}^{(f)}\\}$ into a high dimensional space $\\{\\mathbf{E}^{(s)}, \\mathbf{E}^{(h)}, \\mathbf{E}^{(f)}\\}$, after which each embedding is gated by a variable selection network (VSN). The static embedding $\\mathbf{E}^{(s)}$ is used as context for variable selection and as initial condition to the LSTM. Finally the encoded variables are fed into the multi-head attention decoder.<br>\n",
    "> - [Jan Golda, Krzysztof Kudrynski. \"NVIDIA, Deep Learning Forecasting Examples\"](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Forecasting/TFT)\n",
    "> - [Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister, \"Temporal Fusion Transformers for interpretable multi-horizon time series forecasting\"](https://www.sciencedirect.com/science/article/pii/S0169207021000637)<br>\n",
    "\n",
    "Table of Contents\n",
    "1.   [Auxiliary Functions](#cell-1)<br>\n",
    "   1.1 [Gating Mechanisms](#cell-1.1)<br>\n",
    "   1.2 [Variable Selection Network](#cell-1.2)<br>\n",
    "   1.3 [Interpretable Multi-Head Attention](#cell-1.3)<br>\n",
    "2.   [TFT Encoders/Decoders](#cell-2)\n",
    "3.   [TFT PyTorch-Lightning Class](#cell-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Tuple, Optional, Iterable, Any\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.common._base_windows import BaseWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-1\"></a>\n",
    "# 1. Auxiliary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-1.1\"></a>\n",
    "## 1.1 Gating Mechanisms\n",
    "\n",
    "The Gated Residual Network (GRN) provides adaptive depth and network complexity capable of accommodating different size datasets. As residual connections allow for the network to skip the non-linear transformation of input $\\mathbf{a}$ and context $\\mathbf{c}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\eta_{1} &= \\mathrm{ELU}(\\mathbf{W}_{1}\\mathbf{a}+\\mathbf{W}_{2}\\mathbf{c}+\\mathbf{b}_{1}) \\\\\n",
    "\\eta_{2} &= \\mathbf{W}_{2}\\eta_{1}+b_{2} \\\\\n",
    "\\mathrm{GRN}(\\mathbf{a}, \\mathbf{c}) &= \\mathrm{LayerNorm}(a + \\textrm{GLU}(\\eta_{2}))\n",
    "\\end{align}\n",
    "\n",
    "The Gated Linear Unit (GLU) provides the flexibility of supressing unnecesary parts of the GRN. Consider GRN's output $\\gamma$ then GLU transformation is defined by:\n",
    "\n",
    "$$\\mathrm{GLU}(\\gamma) = \\sigma(\\mathbf{W}_{4}\\gamma +b_{4}) \\odot (\\mathbf{W}_{5}\\gamma +b_{5})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class MaybeLayerNorm(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, eps):\n",
    "        super().__init__()\n",
    "        if output_size and output_size == 1:\n",
    "            self.ln = nn.Identity()\n",
    "        else:\n",
    "            self.ln = LayerNorm(output_size if output_size else hidden_size,\n",
    "                                eps=eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ln(x)\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(hidden_size, output_size * 2)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.lin(x)\n",
    "        x = F.glu(x)\n",
    "        return x\n",
    "\n",
    "class GRN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size, \n",
    "                 output_size=None,\n",
    "                 context_hidden_size=None,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = MaybeLayerNorm(output_size, hidden_size, eps=1e-3)\n",
    "        self.lin_a = nn.Linear(input_size, hidden_size)\n",
    "        if context_hidden_size is not None:\n",
    "            self.lin_c = nn.Linear(context_hidden_size, hidden_size, bias=False)\n",
    "        self.lin_i = nn.Linear(hidden_size, hidden_size)\n",
    "        self.glu = GLU(hidden_size, output_size if output_size else hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(input_size, output_size) if output_size else None\n",
    "\n",
    "    def forward(self, a: Tensor, c: Optional[Tensor] = None):\n",
    "        x = self.lin_a(a)\n",
    "        if c is not None:\n",
    "            x = x + self.lin_c(c).unsqueeze(1)\n",
    "        x = F.elu(x)\n",
    "        x = self.lin_i(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.glu(x)\n",
    "        y = a if not self.out_proj else self.out_proj(a)\n",
    "        x = x + y\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-1.2\"></a>\n",
    "## 1.2 Variable Selection Networks\n",
    "\n",
    "TFT includes automated variable selection capabilities, through its variable selection network (VSN) components.\n",
    "\n",
    "The VSN takes the original input $\\{\\mathbf{x}^{(s)}, \\mathbf{x}^{(h)}_{[:t]}, \\mathbf{x}^{(f)}_{[:t]}\\}$ and transforms it through categorical embeddings or linear transformations into a high dimensional space\n",
    "$\\{\\mathbf{E}^{(s)}, \\mathbf{E}^{(h)}_{[:t]}, \\mathbf{E}^{(f)}_{[:t+H]}\\}$. \n",
    "\n",
    "For the observed historic data, the embedding matrix $E^{(h)}_{t}$ at time $t$ is a concatenation of $j$ variable $e^{(h)}_{t,j}$ embeddings:\n",
    "\\begin{align}\n",
    "\\mathbf{E}^{(h)}_{t} &= [e^{(h)}_{t,1},\\dots,e^{(h)}_{t,j},\\dots,e^{(h)}_{t,n_{h}}] \\\\\n",
    "\\mathbf{\\tilde{e}}^{(h)}_{t,j} &= \\mathrm{GRN}(e^{(h)}_{t,j})\n",
    "\\end{align}\n",
    "\n",
    "The variable selection weights are given by:\n",
    "$$s^{(h)}_{t}=\\mathrm{SoftMax}(\\mathrm{GRN}(E^{(h)}_{t},\\mathbf{E}^{(s)}))$$\n",
    "\n",
    "The VSN processed features are then:\n",
    "$$\\tilde{E}^{(h)}_{t}= \\sum_{j} s^{(h)}_{j} \\tilde{e}^{(h)}_{t,j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class TFTEmbedding(nn.Module):\n",
    "    def __init__(self, hidden_size,\n",
    "                 s_cat_inp_lens, s_cont_inp_size,\n",
    "                 k_cat_inp_lens, k_cont_inp_size,\n",
    "                 o_cat_inp_lens, o_cont_inp_size,\n",
    "                 tgt_size):\n",
    "        super().__init__()\n",
    "        # There are 7 types of input:\n",
    "        # 1. Static categorical\n",
    "        # 2. Static continuous\n",
    "        # 3. Temporal known a priori categorical\n",
    "        # 4. Temporal known a priori continuous\n",
    "        # 5. Temporal observed categorical\n",
    "        # 6. Temporal observed continuous\n",
    "        # 7. Temporal observed targets (time series obseved so far)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.s_cat_inp_lens  = s_cat_inp_lens\n",
    "        self.s_cont_inp_size = s_cont_inp_size\n",
    "        self.k_cat_inp_lens  = k_cat_inp_lens\n",
    "        self.k_cont_inp_size = k_cont_inp_size\n",
    "        self.o_cat_inp_lens  = o_cat_inp_lens\n",
    "        self.o_cont_inp_size = o_cont_inp_size\n",
    "        self.tgt_size        = tgt_size\n",
    "\n",
    "        # Instantiate Categorical Embeddings if lens is not None\n",
    "        for attr, lens in [('s_cat_embed', s_cat_inp_lens), \n",
    "                           ('k_cat_embed', k_cat_inp_lens),\n",
    "                           ('o_cat_embed', o_cat_inp_lens)]:\n",
    "            if lens:\n",
    "                embed = nn.ModuleList([nn.Embedding(n, hidden_size) for n in lens])\n",
    "                setattr(self, attr, embed)\n",
    "            else:\n",
    "                setattr(self, attr, None)\n",
    "\n",
    "        # Instantiate Continuous Embeddings if size is not None\n",
    "        for attr, size in [('s_cont_embedding', s_cont_inp_size), \n",
    "                           ('k_cont_embedding', k_cont_inp_size),\n",
    "                           ('o_cont_embedding', o_cont_inp_size),\n",
    "                           ('tgt_embedding', tgt_size)]:\n",
    "            if size:\n",
    "                vectors = nn.Parameter(torch.Tensor(size, hidden_size))\n",
    "                bias = nn.Parameter(torch.zeros(size, hidden_size))\n",
    "                torch.nn.init.xavier_normal_(vectors)\n",
    "                setattr(self, attr+'_vectors', vectors)\n",
    "                setattr(self, attr+'_bias', bias)\n",
    "            else:\n",
    "                setattr(self, attr+'_vectors', None)\n",
    "                setattr(self, attr+'_bias', None)\n",
    "\n",
    "    def _apply_embedding(self,\n",
    "                         cat: Optional[Tensor],\n",
    "                         cont: Optional[Tensor],\n",
    "                         cat_emb: Iterable[Any], \n",
    "                         cont_emb: Tensor,\n",
    "                         cont_bias: Tensor,\n",
    "                         ):\n",
    "\n",
    "        if (cat is None) and (cont is None):\n",
    "            return None\n",
    "\n",
    "        if (cat is not None):\n",
    "            e_cat = torch.stack([embed(cat[...,i]) \\\n",
    "                               for i, embed in enumerate(cat_emb)], dim=-2)\n",
    "        if (cont is not None):\n",
    "            #the line below is equivalent to following einsums\n",
    "            #e_cont = torch.einsum('btf,fh->bthf', cont, cont_emb)\n",
    "            #e_cont = torch.einsum('bf,fh->bhf', cont, cont_emb)          \n",
    "            e_cont = torch.mul(cont.unsqueeze(-1), cont_emb)\n",
    "            e_cont = e_cont + cont_bias\n",
    "\n",
    "        if (cat is not None) and (cont is None):\n",
    "            return e_cat\n",
    "\n",
    "        if (cat is None) and (cont is not None):\n",
    "            return e_cont\n",
    "\n",
    "        if (cat is not None) and (cont is not None):\n",
    "            return torch.cat([e_cat, e_cont], dim=-2)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def forward(self, target_inp, s_cat_inp=None, s_cont_inp=None, k_cat_inp=None, k_cont_inp=None, o_cat_inp=None, o_cont_inp=None):\n",
    "        # temporal/static categorical/continuous known/observed input \n",
    "        # tries to get input, if fails returns None\n",
    "\n",
    "        # Static inputs are expected to be equal for all timesteps\n",
    "        # For memory efficiency there is no assert statement\n",
    "        s_cat_inp = s_cat_inp[:,0,:] if s_cat_inp is not None else None\n",
    "        s_cont_inp = s_cont_inp[:,0,:] if s_cont_inp is not None else None\n",
    "\n",
    "        s_inp = self._apply_embedding(s_cat_inp, s_cont_inp,\n",
    "                                      cat_emb=self.s_cat_embed,\n",
    "                                      cont_emb=self.s_cont_embedding_vectors,\n",
    "                                      cont_bias=self.s_cont_embedding_bias)\n",
    "        k_inp = self._apply_embedding(k_cat_inp, k_cont_inp,\n",
    "                                      cat_emb=self.k_cat_embed,\n",
    "                                      cont_emb=self.k_cont_embedding_vectors,\n",
    "                                      cont_bias=self.k_cont_embedding_bias)\n",
    "        o_inp = self._apply_embedding(o_cat_inp, o_cont_inp,\n",
    "                                      cat_emb=self.o_cat_embed,\n",
    "                                      cont_emb=self.o_cont_embedding_vectors,\n",
    "                                      cont_bias=self.o_cont_embedding_bias)\n",
    "\n",
    "        # Temporal observed targets\n",
    "        # t_observed_tgt = torch.einsum('btf,fh->btfh', \n",
    "        #                               target_inp, self.tgt_embedding_vectors)        \n",
    "        target_inp = torch.matmul(target_inp.unsqueeze(3).unsqueeze(4),\n",
    "                          self.tgt_embedding_vectors.unsqueeze(1)).squeeze(3)\n",
    "        target_inp = target_inp + self.tgt_embedding_bias\n",
    "\n",
    "        return s_inp, k_inp, o_inp, target_inp\n",
    "\n",
    "class VariableSelectionNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, dropout):\n",
    "        super().__init__()\n",
    "        self.joint_grn = GRN(input_size=hidden_size*num_inputs, \n",
    "                             hidden_size=hidden_size, \n",
    "                             output_size=num_inputs, \n",
    "                             context_hidden_size=hidden_size)\n",
    "        self.var_grns = nn.ModuleList(\n",
    "                        [GRN(input_size=hidden_size, \n",
    "                             hidden_size=hidden_size, dropout=dropout)\n",
    "                         for _ in range(num_inputs)])\n",
    "\n",
    "    def forward(self, x: Tensor, context: Optional[Tensor] = None):\n",
    "        Xi = x.reshape(*x.shape[:-2], -1)\n",
    "        grn_outputs = self.joint_grn(Xi, c=context)\n",
    "        sparse_weights = F.softmax(grn_outputs, dim=-1)\n",
    "        transformed_embed_list = [m(x[...,i,:])\n",
    "                                     for i, m in enumerate(self.var_grns)]\n",
    "        transformed_embed = torch.stack(transformed_embed_list, dim=-1)\n",
    "        #the line below performs batched matrix vector multiplication\n",
    "        #for temporal features it's bthf,btf->bth\n",
    "        #for static features it's bhf,bf->bh\n",
    "        variable_ctx = torch.matmul(transformed_embed, \n",
    "                                    sparse_weights.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        return variable_ctx, sparse_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-1.3\"></a>\n",
    "## 1.3. Interpretable MultiHead Attention\n",
    "\n",
    "To avoid information bottlenecks from the classic Seq2Seq architecture, TFT \n",
    "incorporates a decoder-encoder attention mechanism inherited transformer architectures ([Li et. al 2019](https://arxiv.org/abs/1907.00235), [Vaswani et. al 2017](https://arxiv.org/abs/1706.03762)). It transform the the outputs of the LSTM encoded temporal features, and helps the decoder better capture long-term relationships.\n",
    "\n",
    "The original multihead attention for each component $H_{m}$ and its query, key, and value representations are denoted by $Q_{m}, K_{m}, V_{m}$, its transformation is given by:\n",
    "\n",
    "\\begin{align}\n",
    "Q_{m} = Q W_{Q,m} \\quad K_{m} = K W_{K,h} \\quad V_{m} = V W_{V,m} \\\\\n",
    "H_{m}=\\mathrm{Attention}(Q_{m}, K_{m}, V_{m}) = \\mathrm{SoftMax}(Q_{m} K^{\\intercal}_{m}/\\mathrm{scale}) \\; V_{m} \\\\\n",
    "\\mathrm{MultiHead}(Q, K, V) = [H_{1},\\dots,H_{M}] W_{M}\n",
    "\\end{align}\n",
    "\n",
    "TFT modifies the original multihead attention to improve its interpretability. To do it it uses shared values $\\tilde{V}$ across heads and employs additive aggregation, $\\mathrm{InterpretableMultiHead}(Q,K,V) = \\tilde{H} W_{M}$. The mechanism has a great resemblence to a single attention layer, but it allows for $M$ multiple attention weights, and can be therefore be interpreted as the average ensemble of $M$ single attention layers.\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{H} &= \\left(\\frac{1}{M} \\sum_{m} \\mathrm{SoftMax}(Q_{m} K^{\\intercal}_{m}/\\mathrm{scale}) \\right) \\tilde{V} \n",
    "          = \\frac{1}{M} \\sum_{m} \\mathrm{Attention}(Q_{m}, K_{m}, \\tilde{V}) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class InterpretableMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, hidden_size, example_length,\n",
    "                 attn_dropout, dropout):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        assert hidden_size % n_head == 0\n",
    "        self.d_head = hidden_size // n_head\n",
    "        self.qkv_linears = nn.Linear(hidden_size, \n",
    "                                     (2 * self.n_head + 1) * self.d_head,\n",
    "                                     bias=False)\n",
    "        self.out_proj = nn.Linear(self.d_head, hidden_size, bias=False)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.out_dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.d_head**-0.5\n",
    "        self.register_buffer(\"_mask\",\n",
    "          torch.triu(torch.full((example_length, example_length), \n",
    "                                float('-inf')), 1).unsqueeze(0))\n",
    "\n",
    "    def forward(self, x: Tensor, \n",
    "                mask_future_timesteps: bool = True) -> Tuple[Tensor, Tensor]:\n",
    "        # [Batch,Time,MultiHead,AttDim] := [N,T,M,AD]\n",
    "        bs, t, h_size = x.shape\n",
    "        qkv = self.qkv_linears(x)\n",
    "        q, k, v = qkv.split((self.n_head * self.d_head, \n",
    "                             self.n_head * self.d_head, self.d_head), dim=-1)\n",
    "        q = q.view(bs, t, self.n_head, self.d_head)\n",
    "        k = k.view(bs, t, self.n_head, self.d_head)\n",
    "        v = v.view(bs, t, self.d_head)\n",
    "        \n",
    "        # [N,T1,M,Ad] x [N,T2,M,Ad] -> [N,M,T1,T2]\n",
    "        # attn_score = torch.einsum('bind,bjnd->bnij', q, k)\n",
    "        attn_score = torch.matmul(q.permute((0, 2, 1, 3)), \n",
    "                                  k.permute((0, 2, 3, 1)))\n",
    "        attn_score.mul_(self.scale)\n",
    "\n",
    "        if mask_future_timesteps:\n",
    "            attn_score = attn_score + self._mask\n",
    "\n",
    "        attn_prob = F.softmax(attn_score, dim=3)\n",
    "        attn_prob = self.attn_dropout(attn_prob)\n",
    "\n",
    "        # [N,M,T1,T2] x [N,M,T1,Ad] -> [N,M,T1,Ad]\n",
    "        # attn_vec = torch.einsum('bnij,bjd->bnid', attn_prob, v)\n",
    "        attn_vec = torch.matmul(attn_prob, v.unsqueeze(1))\n",
    "        m_attn_vec = torch.mean(attn_vec, dim=1)\n",
    "        out = self.out_proj(m_attn_vec)\n",
    "        out = self.out_dropout(out)\n",
    "\n",
    "        return out, attn_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-2\"></a>\n",
    "# 2. TFT Architecture\n",
    "\n",
    "The first TFT's step is embed the original input $\\{\\mathbf{x}^{(s)}, \\mathbf{x}^{(h)}, \\mathbf{x}^{(f)}\\}$ into a high dimensional space $\\{\\mathbf{E}^{(s)}, \\mathbf{E}^{(h)}, \\mathbf{E}^{(f)}\\}$, after which each embedding is gated by a variable selection network (VSN). \n",
    "\n",
    "- The static embedding $\\mathbf{E}^{(s)}$ is transformed by the StaticCovariateEncoder into contexts $c_{c}, c_{e}, c_{h}, c_{c}$.\n",
    "- The TemporalCovariateEncoder transforms $c_{h}, c_{c}$ and temporal embeddings $\\mathbf{E}^{(h)}, \\mathbf{E}^{(f)}$ with an LSTM.\n",
    "- The TemporalFusionDecoder enriches the LSTM's outputs with $c_{e}$ and then uses an attention layer, and multi-step adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class StaticCovariateEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_static_vars, dropout):\n",
    "        super().__init__()\n",
    "        self.vsn = VariableSelectionNetwork(hidden_size=hidden_size,\n",
    "                                            num_inputs=num_static_vars,\n",
    "                                            dropout=dropout)\n",
    "        self.context_grns = nn.ModuleList(\n",
    "                              [GRN(input_size=hidden_size,\n",
    "                                   hidden_size=hidden_size,\n",
    "                                   dropout=dropout) for _ in range(4)])\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "        variable_ctx, sparse_weights = self.vsn(x)\n",
    "\n",
    "        # Context vectors:\n",
    "        # variable selection context\n",
    "        # enrichment context\n",
    "        # state_c context\n",
    "        # state_h context\n",
    "        cs, ce, ch, cc = tuple(m(variable_ctx) for m in self.context_grns)\n",
    "\n",
    "        return cs, ce, ch, cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class TemporalCovariateEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, \n",
    "                 num_historic_vars, num_future_vars, dropout):\n",
    "        super(TemporalCovariateEncoder, self).__init__()\n",
    "\n",
    "        self.history_vsn = VariableSelectionNetwork(\n",
    "                                       hidden_size=hidden_size,\n",
    "                                       num_inputs=num_historic_vars,\n",
    "                                       dropout=dropout)\n",
    "        self.history_encoder = nn.LSTM(input_size=hidden_size,\n",
    "                                       hidden_size=hidden_size,\n",
    "                                       batch_first=True)\n",
    "        \n",
    "        self.future_vsn = VariableSelectionNetwork(hidden_size=hidden_size,\n",
    "                                                   num_inputs=num_future_vars,\n",
    "                                                   dropout=dropout)\n",
    "        self.future_encoder = nn.LSTM(input_size=hidden_size,\n",
    "                                      hidden_size=hidden_size,\n",
    "                                      batch_first=True)\n",
    "        \n",
    "        # Shared Gated-Skip Connection\n",
    "        self.input_gate = GLU(hidden_size, hidden_size)\n",
    "        self.input_gate_ln = LayerNorm(hidden_size, eps=1e-3)\n",
    "    \n",
    "    def forward(self, historical_inputs, future_inputs, cs, ch, cc):\n",
    "        # [N,X_in,L] -> [N,hidden_size,L]\n",
    "        historical_features, _ = self.history_vsn(historical_inputs, cs)\n",
    "        history, state = self.history_encoder(historical_features, (ch, cc))\n",
    "\n",
    "        future_features, _ = self.future_vsn(future_inputs, cs)\n",
    "        future, _ = self.future_encoder(future_features, state)\n",
    "        #torch.cuda.synchronize() # this call gives prf boost for unknown reasons\n",
    "\n",
    "        input_embedding = torch.cat([historical_features, future_features], dim=1)\n",
    "        temporal_features = torch.cat([history, future], dim=1)\n",
    "        temporal_features = self.input_gate(temporal_features)\n",
    "        temporal_features = temporal_features + input_embedding\n",
    "        temporal_features = self.input_gate_ln(temporal_features)      \n",
    "        return temporal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class TemporalFusionDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_head, hidden_size, \n",
    "                 example_length, encoder_length,\n",
    "                 attn_dropout, dropout):\n",
    "        super(TemporalFusionDecoder, self).__init__()\n",
    "        self.encoder_length = encoder_length\n",
    "        \n",
    "        #------------- Encoder-Decoder Attention --------------#\n",
    "        self.enrichment_grn = GRN(input_size=hidden_size,\n",
    "                                  hidden_size=hidden_size,\n",
    "                                  context_hidden_size=hidden_size, \n",
    "                                  dropout=dropout)\n",
    "        self.attention = InterpretableMultiHeadAttention(\n",
    "                                       n_head=n_head,\n",
    "                                       hidden_size=hidden_size,\n",
    "                                       example_length=example_length,\n",
    "                                       attn_dropout=attn_dropout,\n",
    "                                       dropout=dropout)\n",
    "        self.attention_gate = GLU(hidden_size, hidden_size)\n",
    "        self.attention_ln = LayerNorm(normalized_shape=hidden_size, eps=1e-3)\n",
    "\n",
    "        self.positionwise_grn = GRN(input_size=hidden_size,\n",
    "                                    hidden_size=hidden_size,\n",
    "                                    dropout=dropout)\n",
    "        \n",
    "        #---------------------- Decoder -----------------------#\n",
    "        self.decoder_gate = GLU(hidden_size, hidden_size)\n",
    "        self.decoder_ln = LayerNorm(normalized_shape=hidden_size, eps=1e-3)\n",
    "        \n",
    "    \n",
    "    def forward(self, temporal_features, ce):\n",
    "        #------------- Encoder-Decoder Attention --------------#\n",
    "        # Static enrichment\n",
    "        enriched = self.enrichment_grn(temporal_features, c=ce)\n",
    "\n",
    "        # Temporal self attention\n",
    "        x, _ = self.attention(enriched, mask_future_timesteps=True)\n",
    "\n",
    "        # Don't compute hictorical quantiles\n",
    "        x = x[:, self.encoder_length:, :]\n",
    "        temporal_features = temporal_features[:, self.encoder_length:, :]\n",
    "        enriched = enriched[:, self.encoder_length:, :]\n",
    "\n",
    "        x = self.attention_gate(x)\n",
    "        x = x + enriched\n",
    "        x = self.attention_ln(x)\n",
    "\n",
    "        # Position-wise feed-forward\n",
    "        x = self.positionwise_grn(x)\n",
    "\n",
    "        #---------------------- Decoder ----------------------#\n",
    "        # Final skip connection\n",
    "        x = self.decoder_gate(x)\n",
    "        x = x + temporal_features\n",
    "        x = self.decoder_ln(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TFT(BaseWindows):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 h,\n",
    "                 tgt_size=1,\n",
    "                 hidden_size=128,\n",
    "                 s_cont_cols=None,\n",
    "                 s_cat_cols=None,\n",
    "                 o_cont_cols=None,\n",
    "                 o_cat_cols=None,\n",
    "                 k_cont_cols=None,\n",
    "                 k_cat_cols=None,\n",
    "                 s_cat_inp_lens=None,\n",
    "                 s_cont_inp_size=0,\n",
    "                 k_cat_inp_lens=None,\n",
    "                 k_cont_inp_size=1,\n",
    "                 o_cat_inp_lens=None,\n",
    "                 o_cont_inp_size=0,\n",
    "                 n_head=4,\n",
    "                 attn_dropout=0.0,\n",
    "                 dropout=0.1,\n",
    "                 windows_batch_size=1024,\n",
    "                 step_size=1,\n",
    "                 learning_rate=1e-3,\n",
    "                 scaler_type='robust',\n",
    "                 loss=MAE(),\n",
    "                 batch_size=32, \n",
    "                 num_workers_loader=0,\n",
    "                 drop_last_loader=False,\n",
    "                 random_seed=1,\n",
    "                 **trainer_kwargs\n",
    "                 ):\n",
    "\n",
    "        # Inherit BaseWindows class\n",
    "        super(TFT, self).__init__(h=h,\n",
    "                                  loss=loss,\n",
    "                                  batch_size=batch_size,\n",
    "                                  scaler_type=scaler_type,\n",
    "                                  num_workers_loader=num_workers_loader,\n",
    "                                  drop_last_loader=drop_last_loader,\n",
    "                                  random_seed=random_seed,\n",
    "                                  **trainer_kwargs)\n",
    "\n",
    "        self.windows_batch_size = windows_batch_size\n",
    "        self.step_size = step_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.example_length = input_size + h\n",
    "\n",
    "        # Parse lists hyperparameters\n",
    "        self.s_cont_cols = [] if s_cont_cols is None else s_cont_cols\n",
    "        self.s_cat_cols = [] if s_cat_cols is None else s_cat_cols\n",
    "        self.o_cont_cols = [] if o_cont_cols is None else o_cont_cols\n",
    "        self.o_cat_cols = [] if o_cat_cols is None else o_cat_cols\n",
    "        self.k_cont_cols = [] if k_cont_cols is None else k_cont_cols\n",
    "        self.k_cat_cols = [] if k_cat_cols is None else k_cat_cols\n",
    "\n",
    "        s_cat_inp_lens = [] if s_cat_inp_lens is None else s_cat_inp_lens\n",
    "        k_cat_inp_lens = [] if k_cat_inp_lens is None else k_cat_inp_lens\n",
    "        o_cat_inp_lens = [] if o_cat_inp_lens is None else o_cat_inp_lens\n",
    "\n",
    "        num_static_vars = s_cont_inp_size + len(s_cat_inp_lens)\n",
    "        num_future_vars = k_cont_inp_size + len(k_cat_inp_lens)\n",
    "        num_observ_vars = o_cont_inp_size + len(o_cat_inp_lens)\n",
    "        num_historic_vars = num_future_vars + num_observ_vars + tgt_size\n",
    "\n",
    "        #------------------------------- Encoders -----------------------------#\n",
    "        self.embedding = TFTEmbedding(hidden_size=hidden_size,\n",
    "                                      s_cat_inp_lens=s_cat_inp_lens,\n",
    "                                      s_cont_inp_size=s_cont_inp_size,\n",
    "                                      k_cat_inp_lens=k_cat_inp_lens, \n",
    "                                      k_cont_inp_size=k_cont_inp_size,\n",
    "                                      o_cat_inp_lens=o_cat_inp_lens,\n",
    "                                      o_cont_inp_size=o_cont_inp_size,\n",
    "                                      tgt_size=tgt_size)\n",
    "        \n",
    "        self.static_encoder = StaticCovariateEncoder(\n",
    "                                      hidden_size=hidden_size,\n",
    "                                      num_static_vars=num_static_vars,\n",
    "                                      dropout=dropout)\n",
    "\n",
    "        self.temporal_encoder = TemporalCovariateEncoder(\n",
    "                                      hidden_size=hidden_size,\n",
    "                                      num_historic_vars=num_historic_vars,\n",
    "                                      num_future_vars=num_future_vars,\n",
    "                                      dropout=dropout)\n",
    "\n",
    "        #------------------------------ Decoders -----------------------------#\n",
    "        self.temporal_fusion_decoder = TemporalFusionDecoder(\n",
    "                                      n_head=n_head,\n",
    "                                      hidden_size=hidden_size,\n",
    "                                      example_length=self.example_length,\n",
    "                                      encoder_length=self.input_size,\n",
    "                                      attn_dropout=attn_dropout,\n",
    "                                      dropout=dropout)\n",
    "\n",
    "        # Adapter with Loss dependent dimensions\n",
    "        self.output_adapter = nn.Linear(in_features=hidden_size,\n",
    "                                 out_features=self.loss.outputsize_multiplier)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Extract static and temporal features\n",
    "        y_idx = x['temporal_cols'].get_loc('y')\n",
    "        target_inp = x['temporal'][:, :, y_idx, None]\n",
    "\n",
    "        if len(self.k_cont_cols) > 0:\n",
    "            k_cont_inp = x['temporal'][:, :, x['temporal_cols'].get_indexer(self.k_cont_cols)]\n",
    "        else:\n",
    "            k_cont_inp = x['temporal'][:, [-self.h-1], y_idx]\n",
    "            k_cont_inp = k_cont_inp[:,:,None].repeat(1, self.example_length, 1)\n",
    "\n",
    "        # TODO: improve dictionary unpacking\n",
    "        s_inp, k_inp, o_inp, t_observed_tgt = self.embedding(target_inp=target_inp,\n",
    "                                                             k_cont_inp=k_cont_inp)\n",
    "\n",
    "        #-------------------------------- Inputs ------------------------------#\n",
    "        # Static context\n",
    "        if s_inp is not None:\n",
    "            cs, ce, ch, cc = self.static_encoder(s_inp)\n",
    "            ch, cc = ch.unsqueeze(0), cc.unsqueeze(0) # LSTM initial states\n",
    "        else:\n",
    "            # If None add zeros\n",
    "            batch_size, example_length, target_size, hidden_size = t_observed_tgt.shape\n",
    "            cs = torch.zeros(size=(batch_size, hidden_size)).to(target_inp.device)\n",
    "            ce = torch.zeros(size=(batch_size, hidden_size)).to(target_inp.device)\n",
    "            ch = torch.zeros(size=(1, batch_size, hidden_size)).to(target_inp.device)\n",
    "            cc = torch.zeros(size=(1, batch_size, hidden_size)).to(target_inp.device)\n",
    "\n",
    "        # Historical inputs\n",
    "        _historical_inputs = [k_inp[:,:self.input_size,:],\n",
    "                              t_observed_tgt[:,:self.input_size,:]]\n",
    "        if o_inp is not None:\n",
    "            _historical_inputs.insert(0,o_inp[:,:self.input_size,:])\n",
    "        historical_inputs = torch.cat(_historical_inputs, dim=-2)\n",
    "\n",
    "        # Future inputs\n",
    "        future_inputs = k_inp[:, self.input_size:]\n",
    "\n",
    "        #---------------------------- Encode/Decode ---------------------------#\n",
    "        # Embeddings + VSN + LSTM encoders\n",
    "        temporal_features = self.temporal_encoder(historical_inputs,\n",
    "                                                  future_inputs,\n",
    "                                                  cs, ch, cc)\n",
    "\n",
    "        # Static enrichment, Attention and decoders\n",
    "        temporal_features = self.temporal_fusion_decoder(temporal_features, ce)\n",
    "\n",
    "        # Adapt output to loss\n",
    "        y_hat = self.output_adapter(temporal_features)\n",
    "        if self.loss.outputsize_multiplier==1:\n",
    "            y_hat = y_hat.squeeze(-1)\n",
    "\n",
    "        return y_hat\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Deviates from orignal `BaseWindows.training_step` to \n",
    "        # allow the model to receive future exogenous available\n",
    "        # at the time of the prediction.\n",
    "        \n",
    "        # Create windows [Ws, L+H, C]\n",
    "        windows = self._create_windows(batch, step='train')\n",
    "\n",
    "        # Normalize windows\n",
    "        if self.scaler is not None:\n",
    "            windows = self._normalization(windows=windows)\n",
    "\n",
    "        # outsample\n",
    "        y_idx = batch['temporal_cols'].get_loc('y')\n",
    "        mask_idx = batch['temporal_cols'].get_loc('available_mask')\n",
    "        outsample_y = windows['temporal'][:, -self.h:, y_idx]\n",
    "        outsample_mask = windows['temporal'][:, -self.h:, mask_idx]\n",
    "\n",
    "        #batch_size, input_size\n",
    "        y_hat = self(x=windows)\n",
    "\n",
    "        loss = self.loss(y=outsample_y, y_hat=y_hat, mask=outsample_mask)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # Deviates from orignal `BaseWindows.training_step` to \n",
    "        # allow the model to receive future exogenous available\n",
    "        # at the time of the prediction.        \n",
    "        \n",
    "        # Create windows [Ws, L+H, C]\n",
    "        windows = self._create_windows(batch, step='predict')\n",
    "\n",
    "        # Normalize windows\n",
    "        if self.scaler is not None:\n",
    "            windows = self._normalization(windows=windows)\n",
    "\n",
    "        y_hat = self(x=windows)\n",
    "\n",
    "        # Inv Normalize\n",
    "        if self.scaler is not None:\n",
    "            y_hat = self._inv_normalization(y_hat=y_hat,\n",
    "                                            temporal_cols=batch['temporal_cols'])\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TFT.fit, name='TFT.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TFT.predict, name='TFT.predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast.utils import AirPassengers, AirPassengersPanel\n",
    "from neuralforecast.tsdataset import TimeSeriesDataset, TimeSeriesLoader\n",
    "\n",
    "from neuralforecast.losses.pytorch import MQLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n",
    "Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]] # 12 test\n",
    "\n",
    "dataset, *_ = TimeSeriesDataset.from_df(df = AirPassengersPanel)\n",
    "model = TFT(input_size=48, h=12,\n",
    "            hidden_size=100,\n",
    "            k_cont_cols=['trend'], #['trend', 'y_[lag12]'],\n",
    "            k_cont_inp_size=2,\n",
    "            max_epochs=1,\n",
    "            learning_rate=0.01,\n",
    "            scaler_type='robust',\n",
    "            loss=MQLoss(level=[80, 90]),\n",
    "            windows_batch_size=None,\n",
    "            enable_progress_bar=True)\n",
    "\n",
    "model.fit(dataset=dataset, test_size=12)\n",
    "\n",
    "# Parse quantile predictions\n",
    "y_hat = model.predict(dataset=dataset)\n",
    "Y_hat_df = pd.DataFrame.from_records(data=y_hat,\n",
    "                columns=['TFT'+q for q in model.loss.output_names],\n",
    "                index=Y_test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot quantile predictions\n",
    "plot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline2']\n",
    "plot_df = pd.concat([Y_train_df, plot_df])\n",
    "plot_df = plot_df[plot_df.unique_id=='Airline2'].drop('unique_id', axis=1)\n",
    "\n",
    "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
    "# plt.plot(plot_df['ds'], plot_df['TFT'], c='blue', label='median')\n",
    "plt.plot(plot_df['ds'], plot_df['TFT-median'], c='blue', label='median')\n",
    "plt.fill_between(x=plot_df['ds'], \n",
    "                 y1=plot_df['TFT-lo-90'], y2=plot_df['TFT-hi-90'],\n",
    "                 alpha=0.4, label='level 90')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('neuralforecast')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
