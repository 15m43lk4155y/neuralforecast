{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.esrnn.esrnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Union, List\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from nixtla.models.esrnn.esrnn_model import _ES, _ESI, _MedianResidual, _ESM, _RNN, _ESRNN\n",
    "from nixtla.losses.pytorch import (\n",
    "    MAPELoss, MASELoss, SMAPELoss, \n",
    "    MSELoss, MAELoss, SmylLoss, \n",
    "    PinballLoss, MQLoss, wMQLoss \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ESRNN(pl.LightningModule):\n",
    "    def __init__(self, n_series: int,\n",
    "                  n_t: int, n_s: int,\n",
    "                  input_size: int, output_size: int,\n",
    "                  es_component: str = 'multiplicative', \n",
    "                  cell_type: str = 'LSTM', state_hsize: int = 50, \n",
    "                  dilations: List[List[int]] = [[1, 2], [4, 8]], \n",
    "                  add_nl_layer: bool = False, seasonality: List[int] = [],\n",
    "                  learning_rate: float = 1e-3, lr_scheduler_step_size: int = 9,\n",
    "                  lr_decay: float = 0.9, per_series_lr_multip: float = 1.,\n",
    "                  gradient_eps: float = 1e-8, \n",
    "                  gradient_clipping_threshold: float = 20.,\n",
    "                  rnn_weight_decay: float = 0., noise_std: float = 1e-3,\n",
    "                  level_variability_penalty: float = 20.,\n",
    "                  testing_percentile: Union[int, List] = 50, \n",
    "                  training_percentile: Union[int, List] = 50,\n",
    "                  loss: str = 'SMYL', val_loss: str = 'MAE'):\n",
    "        super(ESRNN, self).__init__()\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.n_series = n_series\n",
    "        self.n_t = n_t\n",
    "        self.n_s = n_s \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.es_component = es_component\n",
    "        self.cell_type = cell_type\n",
    "        self.state_hsize = state_hsize\n",
    "        self.dilations = dilations\n",
    "        self.add_nl_layer = add_nl_layer\n",
    "        self.seasonality = seasonality\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_scheduler_step_size = lr_scheduler_step_size\n",
    "        self.lr_decay = lr_decay\n",
    "        self.per_series_lr_multip = per_series_lr_multip\n",
    "        self.gradient_eps = gradient_eps\n",
    "        self.gradient_clipping_threshold = gradient_clipping_threshold\n",
    "        self.rnn_weight_decay = rnn_weight_decay\n",
    "        self.noise_std = noise_std\n",
    "        self.level_variability_penalty = level_variability_penalty\n",
    "        self.testing_percentile = testing_percentile\n",
    "        self.training_percentile = training_percentile\n",
    "        self.loss = loss\n",
    "        self.val_loss = val_loss\n",
    "        self.loss_fn = self.__loss_fn(loss)\n",
    "        self.val_loss_fn = self.__loss_fn(val_loss)\n",
    "\n",
    "        # MQESRNN\n",
    "        self.mq = isinstance(self.training_percentile, list)\n",
    "        self.output_size_m = len(self.training_percentile) if self.mq else 1\n",
    "\n",
    "        #Defining model\n",
    "        self.esrnn = _ESRNN(n_series=self.n_series, \n",
    "                            input_size=self.input_size,\n",
    "                            output_size=self.output_size, \n",
    "                            output_size_m=self.output_size_m,\n",
    "                            n_t=self.n_t, n_s=self.n_s,\n",
    "                            es_component=self.es_component, \n",
    "                            seasonality=self.seasonality,\n",
    "                            noise_std=self.noise_std, \n",
    "                            cell_type=self.cell_type,\n",
    "                            dilations=self.dilations, \n",
    "                            state_hsize=self.state_hsize,\n",
    "                            add_nl_layer=self.add_nl_layer, \n",
    "                            device=self.device).to(self.device)\n",
    "            \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def __loss_fn(self, loss_name: str):\n",
    "        #TODO: replace with kwargs\n",
    "        def loss(forecast, target, mask=None, x=None, levels=None):\n",
    "            if loss_name == 'SMYL':\n",
    "                return SmylLoss(y=target, y_hat=forecast, levels=levels, mask=mask,\n",
    "                                tau=(self.training_percentile / 100),\n",
    "                                level_variability_penalty=self.level_variability_penalty)\n",
    "            elif loss_name == 'MQ':\n",
    "                quantiles = [tau / 100 for tau in self.training_percentile]\n",
    "                quantiles = self.to_tensor(np.array(quantiles))\n",
    "                return MQLoss(y=target, y_hat=forecast, quantiles=quantiles, mask=mask)\n",
    "            elif loss_name == 'wMQ':\n",
    "                quantiles = [tau / 100 for tau in self.training_percentile]\n",
    "                quantiles = self.to_tensor(np.array(quantiles))\n",
    "                return wMQLoss(y=target, y_hat=forecast, quantiles=quantiles, mask=mask)\n",
    "            elif loss_name == 'MAPE':\n",
    "                return MAPELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MASE':\n",
    "                return MASELoss(y=target, y_hat=forecast, y_insample=x, seasonality=loss_hypar, mask=mask)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return SMAPELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MSE':\n",
    "                return MSELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'MAE':\n",
    "                return MAELoss(y=target, y_hat=forecast, mask=mask)\n",
    "            elif loss_name == 'PINBALL':\n",
    "                return PinballLoss(y=target, y_hat=forecast, mask=mask, \n",
    "                                   tau=(self.training_percentile/100))\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #Parsing batch\n",
    "        S = t.tensor(batch['S'])\n",
    "        Y = t.tensor(batch['Y'])\n",
    "        X = t.tensor(batch['X'])\n",
    "        idxs = t.tensor(batch['idxs'], dtype=t.long)\n",
    "        step_size = 24 # to be defined\n",
    "        outsample_y, forecast, levels = self.esrnn(S=S, Y=Y, X=X, idxs=idxs,\n",
    "                                                   step_size=step_size)\n",
    "        \n",
    "        outsample_mask = t.ones_like(outsample_y)\n",
    "        loss = self.loss_fn(x=Y,\n",
    "                            forecast=forecast,\n",
    "                            target=outsample_y,\n",
    "                            mask=outsample_mask, \n",
    "                            levels=levels) \n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, idx):\n",
    "        S = t.tensor(batch['S'])\n",
    "        Y = t.tensor(batch['Y'])\n",
    "        X = t.tensor(batch['X'])\n",
    "        idxs = t.tensor(batch['idxs'], dtype=t.long)\n",
    "        step_size = 24\n",
    "        \n",
    "        target, forecast = self.esrnn.predict(S=S, Y=Y, X=X, idxs=idxs,\n",
    "                                              step_size=step_size)\n",
    "        outsample_mask = t.ones_like(target)\n",
    "        loss = self.val_loss_fn(forecast=forecast,\n",
    "                                target=target,\n",
    "                                mask=outsample_mask)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = t.optim.Adam(self.esrnn.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch size will be ignored (shuffle=False). All constructed windows will be used to train.\n",
      "WARNING:root:Batch size will be ignored (shuffle=False). All constructed windows will be used to train.\n"
     ]
    }
   ],
   "source": [
    "from nixtla.data.datasets.epf import EPF, EPFInfo\n",
    "from nixtla.data.tsdataset import TimeSeriesDataset\n",
    "from nixtla.data.tsloader import TimeSeriesLoader\n",
    "\n",
    "Y_df, X_df, _ = EPF.load(directory='./data', group=EPFInfo.groups[0])\n",
    "\n",
    "X_df = X_df[['unique_id', 'ds', 'week_day']]\n",
    "\n",
    "# Leveling Y_df (multiplicative model)\n",
    "Y_min = Y_df.y.min()\n",
    "Y_df.y = Y_df.y - Y_min + 20\n",
    "\n",
    "train_ts_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=None, X_df=X_df,\n",
    "                                     ds_in_test=728*24, verbose=False)\n",
    "\n",
    "outsample_ts_dataset = TimeSeriesDataset(Y_df=Y_df, S_df=None, X_df=X_df,\n",
    "                                         ds_in_test=728*24, is_test=True, verbose=False)\n",
    "\n",
    "train_ts_loader = TimeSeriesLoader(ts_dataset=train_ts_dataset,\n",
    "                                   model='esrnn',\n",
    "                                   window_sampling_limit= 500_000, # To limit backprop time\n",
    "                                   input_size=7*24,\n",
    "                                   output_size=24,\n",
    "                                   idx_to_sample_freq=24,\n",
    "                                   len_sample_chunks=3*7*24,\n",
    "                                   complete_inputs=True,\n",
    "                                   batch_size=32,\n",
    "                                   shuffle=False)\n",
    "\n",
    "val_ts_loader = TimeSeriesLoader(ts_dataset=outsample_ts_dataset,\n",
    "                                 model='esrnn',\n",
    "                                 window_sampling_limit= 500_000, \n",
    "                                 input_size=7*24,\n",
    "                                 output_size=24,\n",
    "                                 idx_to_sample_freq=24,\n",
    "                                 len_sample_chunks=4*7*24,\n",
    "                                 complete_inputs=False,\n",
    "                                 batch_size=1,\n",
    "                                 shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esrnn = ESRNN(n_series=1, n_t=1, n_s=0,\n",
    "              input_size=5*24,\n",
    "              output_size=24,\n",
    "              seasonality=[24],\n",
    "              noise_std=0.01,\n",
    "              es_component='multiplicative',\n",
    "              cell_type='LSTM',\n",
    "              state_hsize=50,\n",
    "              dilations=[[1, 2], [7, 14]],\n",
    "              add_nl_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | esrnn | _ESRNN | 125 K \n",
      "---------------------------------\n",
      "125 K     Trainable params\n",
      "0         Non-trainable params\n",
      "125 K     Total params\n",
      "0.503     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1906ca2394c74a149f76d556f037ca2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/federicogarza2/anaconda3/envs/nixtla/lib/python3.7/site-packages/ipykernel_launcher.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/federicogarza2/anaconda3/envs/nixtla/lib/python3.7/site-packages/ipykernel_launcher.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d861c71be5a7446d8ea92bfb2b25021a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/federicogarza2/anaconda3/envs/nixtla/lib/python3.7/site-packages/ipykernel_launcher.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/federicogarza2/anaconda3/envs/nixtla/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3b0275c3344d0880bbe74081b94d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eefcc9749334f4aa1ef51d75ca388c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d5a03a1c2b497fbef21d3a6065f6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db31713fa534ed3bf0a76532c3345b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede25da517a9498caee96515dedb2fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=5)\n",
    "trainer.fit(esrnn, train_ts_loader, val_ts_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': tensor(3.5918), 'epoch': tensor(4.), 'train_loss': tensor(0.0392)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logged_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nixtla",
   "language": "python",
   "name": "nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
