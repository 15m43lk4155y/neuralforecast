{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.esrnn.mqesrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MQESRNN model\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from typing import List\n",
    "\n",
    "from neuralforecast.models.esrnn.esrnn import ESRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MQESRNN(ESRNN):\n",
    "    def __init__(self,\n",
    "                 n_series: int,\n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 n_x: int = 0,\n",
    "                 n_s: int = 0,\n",
    "                 sample_freq: int = 1,\n",
    "                 es_component: str = 'median_residual',\n",
    "                 cell_type: str = 'LSTM',\n",
    "                 state_hsize: int = 50,\n",
    "                 dilations: List[List[int]] = [[1, 2], [4, 8]],\n",
    "                 add_nl_layer: bool = False,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 lr_scheduler_step_size: int = 9,\n",
    "                 lr_decay: float = 0.9,\n",
    "                 gradient_eps: float = 1e-8,\n",
    "                 gradient_clipping_threshold: float = 20.,\n",
    "                 rnn_weight_decay: float = 0.,\n",
    "                 noise_std: float = 1e-3,\n",
    "                 testing_percentiles: List[float] = [2.5, 5., 50., 95., 97.5],\n",
    "                 training_percentiles: List[float] = [2.5, 5., 50., 95., 97.5],\n",
    "                 loss: str = 'MQ',\n",
    "                 val_loss: str = 'MQ',\n",
    "                 frequency: str = 'D'):\n",
    "        \"\"\" Multi-Quantile Exponential Smoothing Recurrent Neural Network\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_series: int\n",
    "            Number of time series.\n",
    "        n_x: int\n",
    "            Number of temporal exogenous variables.\n",
    "        n_s: int\n",
    "            Number of static variables.\n",
    "        input_size: int\n",
    "            input size of the recurrent neural network, usually a\n",
    "            multiple of seasonality\n",
    "        output_size: int\n",
    "            output_size or forecast horizon of the recurrent neural\n",
    "            network, usually multiple of seasonality\n",
    "        sample_freq: int\n",
    "            Step size between windows.\n",
    "        es_component: str\n",
    "            Exponential Smoothing component.\n",
    "            Default multiplicative.\n",
    "        cell_type: str\n",
    "            Type of RNN cell, available GRU, LSTM, RNN, ResidualLSTM.\n",
    "        state_hsize: int\n",
    "            dimension of hidden state of the recurrent neural network\n",
    "        dilations: int list\n",
    "            each list represents one chunk of Dilated LSTMS, connected in\n",
    "            standard ResNet fashion\n",
    "        add_nl_layer: bool\n",
    "            whether to insert a tanh() layer between the RNN stack and the\n",
    "            linear adaptor (output) layers\n",
    "        learning_rate: float\n",
    "            size of the stochastic gradient descent steps\n",
    "        lr_scheduler_step_size: int\n",
    "            this step_size is the period for each learning rate decay\n",
    "        lr_decay: float\n",
    "            Learning rate decay.\n",
    "        per_series_lr_multip: float\n",
    "            multiplier for per-series parameters smoothing and initial\n",
    "            seasonalities learning rate (default 1.0)\n",
    "        gradient_eps: float\n",
    "            term added to the Adam optimizer denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        gradient_clipping_threshold: float\n",
    "            max norm of gradient vector, with all parameters treated\n",
    "            as a single vector\n",
    "        rnn_weight_decay: float\n",
    "            parameter to control classic L2/Tikhonov regularization\n",
    "            of the rnn parameters\n",
    "        noise_std: float\n",
    "            standard deviation of white noise added to input during\n",
    "            fit to avoid the model from memorizing the train data\n",
    "        level_variability_penalty: float\n",
    "            this parameter controls the strength of the penalization\n",
    "            to the wigglines of the level vector, induces smoothness\n",
    "            in the output\n",
    "        testing_percentiles: List[float]\n",
    "            This values are only for diagnostic evaluation.\n",
    "        training_percentiles: List[float]\n",
    "            Percentiles to train and forecast.\n",
    "        loss: str\n",
    "            Loss used to train.\n",
    "            Available: 'MQ', 'wMQ'.\n",
    "        val_loss: str\n",
    "            Loss used to validate.\n",
    "            Available: 'MQ', 'wMQ'.\n",
    "        frequency: str\n",
    "            Time series frequency.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        **References:**\n",
    "        `Application\n",
    "        <https://arxiv.org/abs/2112.05673`__\n",
    "        \"\"\"\n",
    "        \n",
    "        allowed_losses = ['MQ', 'wMQ']\n",
    "        if loss not in allowed_losses:\n",
    "            raise Exception(f'Loss {loss} not allowed')\n",
    "            \n",
    "        if val_loss is not None and val_loss not in allowed_losses:\n",
    "            raise Exception(f'Val loss {val_loss} not allowed')\n",
    "        \n",
    "        allowed_es_component = ['median_residual', 'identity']\n",
    "        \n",
    "        if es_component not in allowed_es_component:\n",
    "            raise Exception(f'es component {es_component} not allowed')\n",
    "        \n",
    "        level_variability_penalty = 0\n",
    "        seasonality = []\n",
    "        per_series_lr_multip = 1\n",
    "        super(MQESRNN, self).__init__(n_series=n_series,\n",
    "                                      n_s=n_s,\n",
    "                                      n_x=n_x,\n",
    "                                      sample_freq=sample_freq,\n",
    "                                      input_size=input_size,\n",
    "                                      output_size=output_size,\n",
    "                                      es_component=es_component,\n",
    "                                      cell_type=cell_type,\n",
    "                                      state_hsize=state_hsize,\n",
    "                                      dilations=dilations,\n",
    "                                      add_nl_layer=add_nl_layer,\n",
    "                                      seasonality=seasonality,\n",
    "                                      learning_rate=learning_rate,\n",
    "                                      lr_scheduler_step_size=lr_scheduler_step_size,\n",
    "                                      lr_decay=lr_decay,\n",
    "                                      per_series_lr_multip=per_series_lr_multip,\n",
    "                                      gradient_eps=gradient_eps,\n",
    "                                      gradient_clipping_threshold=gradient_clipping_threshold,\n",
    "                                      rnn_weight_decay=rnn_weight_decay,\n",
    "                                      noise_std=noise_std,\n",
    "                                      level_variability_penalty=level_variability_penalty,\n",
    "                                      testing_percentile=testing_percentiles,\n",
    "                                      training_percentile=training_percentiles,\n",
    "                                      loss=loss,\n",
    "                                      val_loss=val_loss,\n",
    "                                      frequency=frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests MQESRNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as t\n",
    "\n",
    "from neuralforecast.data.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ds = 100 \n",
    "n_ts = 1_000\n",
    "\n",
    "output_size = 10\n",
    "\n",
    "uids = [f'uid_{i + 1}' for i in range(n_ts)]\n",
    "dss = pd.date_range(end='2020-01-01', periods=n_ds)\n",
    "\n",
    "Y_df = pd.DataFrame({'unique_id': np.repeat(uids, n_ds), 'ds': np.tile(dss, n_ts)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "Y_df['y'] = Y_df.groupby('unique_id').transform(lambda x: np.random.uniform(1, 100, size=len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df = Y_df.sort_values(['unique_id', 'ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(Y_df=Y_df,\n",
    "                                  ds_in_test=3*output_size,\n",
    "                                  is_test=False,\n",
    "                                  input_size=7*output_size,\n",
    "                                  output_size=output_size,\n",
    "                                  verbose=True)\n",
    "\n",
    "valid_dataset = TimeSeriesDataset(Y_df=Y_df,\n",
    "                                  input_size=7*output_size,\n",
    "                                  output_size=output_size,\n",
    "                                  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                batch_size=32,\n",
    "                                eq_batch_size=True,\n",
    "                                shuffle=True)\n",
    "\n",
    "valid_loader = TimeSeriesLoader(dataset=valid_dataset,\n",
    "                                batch_size=32,\n",
    "                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MQESRNN(n_series=train_dataset.n_series,\n",
    "                n_s=train_dataset.n_s,\n",
    "                n_x=train_dataset.n_x,\n",
    "                #sample_freq=dataset.sample_freq,\n",
    "                sample_freq=1,\n",
    "                input_size=7*2,\n",
    "                output_size=output_size,\n",
    "                learning_rate=1e-2,\n",
    "                lr_scheduler_step_size=30,\n",
    "                lr_decay=0.1,\n",
    "                gradient_eps=1e-8,\n",
    "                gradient_clipping_threshold=10,\n",
    "                rnn_weight_decay=0,\n",
    "                noise_std=0.001,\n",
    "                testing_percentiles=[30, 50, 70, 90],\n",
    "                training_percentiles=[30, 50, 70, 90],\n",
    "                es_component='median_residual',\n",
    "                cell_type='LSTM',\n",
    "                state_hsize=100,\n",
    "                dilations=[[1, 2], [4, 8]],\n",
    "                add_nl_layer=False,\n",
    "                loss='MQ',\n",
    "                val_loss='MQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=1, progress_bar_refresh_rate=5, deterministic=True)\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.predict(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_hat, mask = zip(*outputs)\n",
    "y_true = t.cat(y_true).numpy()\n",
    "y_hat = t.cat(y_hat).numpy()\n",
    "mask = t.cat(mask).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_true[-1000:, -1]\n",
    "y_hat = y_hat[-1000:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_true.flatten(), alpha=0.5, label='y')\n",
    "for idx, p in enumerate([30, 50, 70, 90]):\n",
    "    y_p = y_hat[:, :, idx]\n",
    "    plt.plot(y_p.flatten(), alpha=0.5, label=f'p{p}')\n",
    "    print(f'calibration p{p}: ', (y_true.flatten() <= y_p.flatten()).mean())\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model.forecast(Y_df, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.query('unique_id == \"uid_1\"').set_index('ds').plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
