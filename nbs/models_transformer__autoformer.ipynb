{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.transformer.autoformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoformer\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch import optim\n",
    "\n",
    "from nixtlats.models.components.embed import DataEmbedding, DataEmbedding_wo_pos\n",
    "from nixtlats.models.components.autocorrelation import (\n",
    "    AutoCorrelation, AutoCorrelationLayer\n",
    ")\n",
    "from nixtlats.models.components.autoformer import (\n",
    "    Encoder, Decoder, EncoderLayer, DecoderLayer,\n",
    "    my_Layernorm, series_decomp\n",
    ")\n",
    "from nixtlats.losses.utils import LossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _Autoformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer is the first method to achieve the series-wise connection,\n",
    "    with inherent O(LlogL) complexity\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, \n",
    "                 label_len, pred_len, output_attention,\n",
    "                 enc_in, dec_in, d_model, c_out, embed, freq, dropout,\n",
    "                 factor, n_heads, d_ff, moving_avg, activation, e_layers,\n",
    "                 d_layers):\n",
    "        super(_Autoformer, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.pred_len = pred_len\n",
    "        self.output_attention = output_attention\n",
    "\n",
    "        # Decomp\n",
    "        kernel_size = moving_avg\n",
    "        self.decomp = series_decomp(kernel_size)\n",
    "\n",
    "        # Embedding\n",
    "        # The series-wise connection inherently contains the sequential information.\n",
    "        # Thus, we can discard the position embedding of transformers.\n",
    "        self.enc_embedding = DataEmbedding_wo_pos(enc_in, d_model, embed, freq,\n",
    "                                                  dropout)\n",
    "        self.dec_embedding = DataEmbedding_wo_pos(dec_in, d_model, embed, freq,\n",
    "                                                  dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(False, factor, attention_dropout=dropout,\n",
    "                                        output_attention=output_attention),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    moving_avg=moving_avg,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            norm_layer=my_Layernorm(d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(True, factor, attention_dropout=dropout,\n",
    "                                        output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(False, factor, attention_dropout=dropout,\n",
    "                                        output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    c_out,\n",
    "                    d_ff,\n",
    "                    moving_avg=moving_avg,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation,\n",
    "                )\n",
    "                for l in range(d_layers)\n",
    "            ],\n",
    "            norm_layer=my_Layernorm(d_model),\n",
    "            projection=nn.Linear(d_model, c_out, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "        # decomp init\n",
    "        mean = torch.mean(x_enc, dim=1).unsqueeze(1).repeat(1, self.pred_len, 1)\n",
    "        zeros = torch.zeros([x_dec.shape[0], self.pred_len, x_dec.shape[2]], device=x_enc.device)\n",
    "        seasonal_init, trend_init = self.decomp(x_enc)\n",
    "        # decoder input\n",
    "        trend_init = torch.cat([trend_init[:, -self.label_len:, :], mean], dim=1)\n",
    "        seasonal_init = torch.cat([seasonal_init[:, -self.label_len:, :], zeros], dim=1)\n",
    "        # enc\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "        # dec\n",
    "        dec_out = self.dec_embedding(seasonal_init, x_mark_dec)\n",
    "        seasonal_part, trend_part = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask,\n",
    "                                                 trend=trend_init)\n",
    "        # final\n",
    "        dec_out = trend_part + seasonal_part\n",
    "\n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoformer model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Autoformer(pl.LightningModule):\n",
    "    def __init__(self, seq_len, \n",
    "                 label_len, pred_len, output_attention,\n",
    "                 enc_in, dec_in, d_model, c_out, embed, freq, dropout,\n",
    "                 factor, n_heads, d_ff, moving_avg, activation, e_layers, d_layers,\n",
    "                 loss_train, loss_valid, loss_hypar, learning_rate,\n",
    "                 lr_decay, weight_decay, lr_decay_step_size,\n",
    "                 random_seed):\n",
    "        super(Autoformer, self).__init__()\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.seq_len = seq_len \n",
    "        self.label_len = label_len \n",
    "        self.pred_len = pred_len \n",
    "        self.output_attention = output_attention\n",
    "        self.enc_in = enc_in \n",
    "        self.dec_in = dec_in \n",
    "        self.d_model = d_model \n",
    "        self.c_out = c_out \n",
    "        self.embed = embed \n",
    "        self.freq = freq \n",
    "        self.dropout = dropout\n",
    "        self.factor = factor \n",
    "        self.n_heads = n_heads \n",
    "        self.d_ff = d_ff \n",
    "        self.moving_avg = moving_avg \n",
    "        self.activation = activation \n",
    "        self.e_layers = e_layers\n",
    "        self.d_layers = d_layers\n",
    "        \n",
    "        # Loss functions\n",
    "        self.loss_train = loss_train\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.loss_valid = loss_valid\n",
    "        self.loss_fn_train = LossFunction(loss_train, \n",
    "                                          seasonality=self.loss_hypar)\n",
    "        self.loss_fn_valid = LossFunction(loss_valid,\n",
    "                                          seasonality=self.loss_hypar)\n",
    "        \n",
    "        # Regularization and optimization parameters      \n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr_decay_step_size = lr_decay_step_size\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.model = _Autoformer(seq_len, \n",
    "                                 label_len, pred_len, output_attention,\n",
    "                                 enc_in, dec_in, d_model, c_out, \n",
    "                                 embed, freq, dropout,\n",
    "                                 factor, n_heads, d_ff, \n",
    "                                 moving_avg, activation, e_layers,\n",
    "                                 d_layers)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Autoformer needs batch of shape (batch_size, time, series) for y\n",
    "        and (batch_size, time, exogenous) for x\n",
    "        and doesnt need X for each time series.\n",
    "        USE DataLoader from pytorch instead of TimeSeriesLoader.\n",
    "        \"\"\"\n",
    "        Y = batch['Y'].permute(0, 2, 1)\n",
    "        X = batch['X'][:, 0, :, :].permute(0, 2, 1)\n",
    "        sample_mask = batch['sample_mask'].permute(0, 2, 1)\n",
    "        available_mask = batch['available_mask']\n",
    "        \n",
    "        s_begin = 0\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "        \n",
    "        batch_x = Y[:, s_begin:s_end, :]\n",
    "        batch_y = Y[:, r_begin:r_end, :]\n",
    "        batch_x_mark = X[:, s_begin:s_end, :]\n",
    "        batch_y_mark = X[:, r_begin:r_end, :]\n",
    "        outsample_mask = sample_mask[:, r_begin:r_end, :]\n",
    "        \n",
    "        dec_inp = torch.zeros_like(batch_y[:, -self.pred_len:, :])\n",
    "        dec_inp = torch.cat([batch_y[:, :self.label_len, :], dec_inp], dim=1)\n",
    "        \n",
    "        if self.output_attention:\n",
    "            forecast = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "        else:\n",
    "            forecast = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            \n",
    "        batch_y = batch_y[:, -self.pred_len:, :]\n",
    "        outsample_mask = outsample_mask[:, -self.pred_len:, :]\n",
    "\n",
    "        return batch_y, forecast, outsample_mask\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        outsample_y, forecast, outsample_mask = self(batch)\n",
    "\n",
    "        loss = self.loss_fn_train(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample= batch['Y'].permute(0, 2, 1))\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, idx):\n",
    "        \n",
    "        outsample_y, forecast, outsample_mask = self(batch)\n",
    "\n",
    "        loss = self.loss_fn_valid(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample= batch['Y'].permute(0, 2, 1))\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.learning_rate, \n",
    "                               weight_decay=self.weight_decay)\n",
    "        \n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, \n",
    "                                                 step_size=self.lr_decay_step_size, \n",
    "                                                 gamma=self.lr_decay)\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoformer Usage Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtlats.data.datasets.ett import ETT\n",
    "\n",
    "Y_df, X_df, S_df = ETT.load(directory='./data', group='ETTm2')\n",
    "Y_df = Y_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = X_df.drop(columns=['unique_id', 'ds']).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Model and Data Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- s_begin = index\n",
    "- s_end = index + self.seq_len\n",
    "- r_begin = index + self.seq_len - self.label_len\n",
    "- r_end = index + self.seq_len + self.pred_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "mc_model = {}\n",
    "\n",
    "mc_model['seq_len'] = 96\n",
    "mc_model['label_len'] = 48\n",
    "mc_model['pred_len'] = 24\n",
    "mc_model['output_attention'] = False\n",
    "mc_model['enc_in'] = 7\n",
    "mc_model['dec_in'] = 7\n",
    "mc_model['d_model'] = 512\n",
    "mc_model['c_out'] = 7\n",
    "mc_model['embed'] = 'timeF'\n",
    "mc_model['freq'] = 'h'\n",
    "mc_model['dropout'] = 0.05\n",
    "mc_model['factor'] = 1\n",
    "mc_model['n_heads'] = 8\n",
    "mc_model['d_ff'] = 2_048\n",
    "mc_model['moving_avg'] = 25 \n",
    "mc_model['activation'] = 'gelu'\n",
    "mc_model['e_layers'] = 2 \n",
    "mc_model['d_layers'] = 1\n",
    "mc_model['loss_train'] = 'MAE'\n",
    "mc_model['loss_hypar'] = 0.5\n",
    "mc_model['loss_valid'] = 'MAE'\n",
    "mc_model['learning_rate'] = 0.001\n",
    "mc_model['lr_decay'] = 0.5\n",
    "mc_model['weight_decay'] = 0.\n",
    "mc_model['lr_decay_step_size'] = 2\n",
    "mc_model['random_seed'] = 1\n",
    "\n",
    "# Dataset parameters\n",
    "mc_data = {}\n",
    "mc_data['mode'] = 'iterate_windows'\n",
    "mc_data['n_time_in'] = mc_model['seq_len']\n",
    "mc_data['n_time_out'] = mc_model['pred_len']\n",
    "mc_data['batch_size'] = 32\n",
    "mc_data['normalizer_y'] = None\n",
    "mc_data['normalizer_x'] = None\n",
    "mc_data['max_epochs'] = 1\n",
    "mc_data['max_steps'] = 1\n",
    "mc_data['early_stop_patience'] = 20\n",
    "\n",
    "len_val = 11_520\n",
    "len_test = 11_520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Loaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtlats.data.tsdataset import IterateWindowsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from nixtlats.experiments.utils import create_datasets\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc_data,\n",
    "                                                                     S_df=None, \n",
    "                                                                     Y_df=Y_df, X_df=X_df,\n",
    "                                                                     f_cols=f_cols,\n",
    "                                                                     ds_in_val=len_val,\n",
    "                                                                     ds_in_test=len_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=int(mc_data['batch_size']),\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=1, # int(mc_data['batch_size'])\n",
    "                        shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=1, # int(mc_data['batch_size'])\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoformer(**mc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               min_delta=1e-4, \n",
    "                               patience=mc_data['early_stop_patience'],\n",
    "                               verbose=False,\n",
    "                               mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=mc_data['max_epochs'], \n",
    "                     max_steps=mc_data['max_steps'],\n",
    "                     gradient_clip_val=1.0,\n",
    "                     progress_bar_refresh_rate=10, \n",
    "                     check_val_every_n_epoch=1,\n",
    "                     callbacks=[early_stopping]\n",
    "                     )\n",
    "\n",
    "#trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = trainer.predict(model, val_loader)\n",
    "\n",
    "#print(\"outputs[0][0].shape\", outputs[0][0].shape)\n",
    "#print(\"outputs[0][1].shape\", outputs[0][1].shape)\n",
    "#print(\"outputs[0][2].shape\", outputs[0][2].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
