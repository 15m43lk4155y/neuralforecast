{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.transformer.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch import optim\n",
    "\n",
    "from neuralforecast.models.components.transformer import Decoder, DecoderLayer, Encoder, EncoderLayer, ConvLayer\n",
    "from neuralforecast.models.components.selfattention import FullAttention, AttentionLayer\n",
    "from neuralforecast.models.components.embed import DataEmbedding\n",
    "from neuralforecast.losses.utils import LossFunction\n",
    "from neuralforecast.data.tsdataset import IterateWindowsDataset\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla Transformer with O(L^2) complexity\n",
    "    \"\"\"\n",
    "    def __init__(self, pred_len, output_attention,\n",
    "                 enc_in, dec_in, d_model, c_out, embed, freq, dropout,\n",
    "                 factor, n_heads, d_ff, activation, e_layers,\n",
    "                 d_layers):\n",
    "        super(_Transformer, self).__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.output_attention = output_attention\n",
    "\n",
    "        # Embedding\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq,\n",
    "                                           dropout)\n",
    "        self.dec_embedding = DataEmbedding(dec_in, d_model, embed, freq,\n",
    "                                           dropout)\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, factor, attention_dropout=dropout,\n",
    "                                      output_attention=output_attention), d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(True, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation,\n",
    "                )\n",
    "                for l in range(d_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model),\n",
    "            projection=nn.Linear(d_model, c_out, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfomer model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self, seq_len: int, \n",
    "                 label_len: int, pred_len: int, output_attention: bool,\n",
    "                 enc_in: int, dec_in: int, d_model: int, c_out: int, \n",
    "                 embed: str, freq: str, dropout: float, factor: float, \n",
    "                 n_heads: int, d_ff: int, activation: str, \n",
    "                 e_layers: int, d_layers: int,\n",
    "                 loss_train: str, loss_valid: str, loss_hypar: float, \n",
    "                 learning_rate: float, lr_decay: float, weight_decay: float, \n",
    "                 lr_decay_step_size: int, random_seed: int):\n",
    "        super(Transformer, self).__init__()\n",
    "        \"\"\"\n",
    "        Vanilla Transformer model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_len: int\n",
    "            Input sequence size.\n",
    "        label_len: int\n",
    "            Label sequence size.\n",
    "        pred_len: int\n",
    "            Prediction sequence size.\n",
    "        output_attention: bool\n",
    "            If true use output attention for Transformer model.\n",
    "        enc_in: int\n",
    "            Number of encoders in data embedding layers.\n",
    "        dec_in: int\n",
    "            Number of decoders in data embedding layers.\n",
    "        d_model: int\n",
    "            Number of nodes for embedding layers.\n",
    "        c_out: int\n",
    "            Number of output nodes in projection layer.\n",
    "        embed: str\n",
    "            Type of embedding layers.\n",
    "        freq: str\n",
    "            Frequency for embedding layers.\n",
    "        dropout: float\n",
    "            Float between (0, 1). Dropout for Transformer.\n",
    "        factor: float\n",
    "            Factor for attention layer.\n",
    "        n_heads: int\n",
    "            Number of heads in attention layer.\n",
    "        d_ff: int\n",
    "            Number of inputs in encoder layers.\n",
    "        activation: str\n",
    "            Activation function for encoder layer.\n",
    "        e_layers: int\n",
    "            Number of encoder layers.\n",
    "        d_layers: int\n",
    "            Number of decoder layers.\n",
    "        loss_train: str\n",
    "            Loss to optimize.\n",
    "            An item from ['MAPE', 'MASE', 'SMAPE', 'MSE', 'MAE', 'QUANTILE', 'QUANTILE2'].\n",
    "        loss_valid: str\n",
    "            Validation loss.\n",
    "            An item from ['MAPE', 'MASE', 'SMAPE', 'RMSE', 'MAE', 'QUANTILE'].\n",
    "        loss_hypar: float\n",
    "            Hyperparameter for chosen loss.\n",
    "        learning_rate: float\n",
    "            Learning rate between (0, 1).\n",
    "        lr_decay: float\n",
    "            Decreasing multiplier for the learning rate.\n",
    "        weight_decay: float\n",
    "            L2 penalty for optimizer.\n",
    "        lr_decay_step_size: int \n",
    "            Steps between each learning rate decay.\n",
    "        random_seed: int\n",
    "            random_seed for pseudo random pytorch initializer and\n",
    "            numpy random generator.\n",
    "        \"\"\"\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.seq_len = seq_len \n",
    "        self.label_len = label_len \n",
    "        self.pred_len = pred_len \n",
    "        self.output_attention = output_attention\n",
    "        self.enc_in = enc_in \n",
    "        self.dec_in = dec_in \n",
    "        self.d_model = d_model \n",
    "        self.c_out = c_out \n",
    "        self.embed = embed\n",
    "        self.freq = freq \n",
    "        self.dropout = dropout\n",
    "        self.factor = factor \n",
    "        self.n_heads = n_heads \n",
    "        self.d_ff = d_ff \n",
    "        self.activation = activation \n",
    "        self.e_layers = e_layers\n",
    "        self.d_layers = d_layers\n",
    "        \n",
    "        # Loss functions\n",
    "        self.loss_train = loss_train\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.loss_valid = loss_valid\n",
    "        self.loss_fn_train = LossFunction(loss_train, \n",
    "                                          seasonality=self.loss_hypar)\n",
    "        self.loss_fn_valid = LossFunction(loss_valid,\n",
    "                                          seasonality=self.loss_hypar)\n",
    "        \n",
    "        # Regularization and optimization parameters      \n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr_decay_step_size = lr_decay_step_size\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.model = _Transformer(pred_len, output_attention,\n",
    "                                  enc_in, dec_in, d_model, c_out, \n",
    "                                  embed, freq, dropout,\n",
    "                                  factor, n_heads, d_ff, \n",
    "                                  activation, e_layers,\n",
    "                                  d_layers)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Autoformer needs batch of shape (batch_size, time, series) for y\n",
    "        and (batch_size, time, exogenous) for x\n",
    "        and doesnt need X for each time series.\n",
    "        USE DataLoader from pytorch instead of TimeSeriesLoader.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Protection for missing batch_size dimension\n",
    "        if batch['Y'].dim()<3:\n",
    "            batch['Y'] = batch['Y'][None,:,:]\n",
    "\n",
    "        if batch['X'] is not None:\n",
    "            if batch['X'].dim()<4:\n",
    "                batch['X'] = batch['X'][None,:,:,:]\n",
    "        \n",
    "        if batch['sample_mask'].dim()<3:\n",
    "            batch['sample_mask'] = batch['sample_mask'][None,:,:]\n",
    "\n",
    "        Y = batch['Y'].permute(0, 2, 1)\n",
    "        X = batch['X'][:, 0, :, :].permute(0, 2, 1)\n",
    "        sample_mask = batch['sample_mask'].permute(0, 2, 1)\n",
    "        available_mask = batch['available_mask']\n",
    "        \n",
    "        s_begin = 0\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "        \n",
    "        batch_x = Y[:, s_begin:s_end, :]\n",
    "        batch_y = Y[:, r_begin:r_end, :]\n",
    "        batch_x_mark = X[:, s_begin:s_end, :]\n",
    "        batch_y_mark = X[:, r_begin:r_end, :]\n",
    "        outsample_mask = sample_mask[:, r_begin:r_end, :]\n",
    "        \n",
    "        dec_inp = torch.zeros_like(batch_y[:, -self.pred_len:, :])\n",
    "        dec_inp = torch.cat([batch_y[:, :self.label_len, :], dec_inp], dim=1)\n",
    "        \n",
    "        if self.output_attention:\n",
    "            forecast = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "        else:\n",
    "            forecast = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            \n",
    "        batch_y = batch_y[:, -self.pred_len:, :]\n",
    "        outsample_mask = outsample_mask[:, -self.pred_len:, :]\n",
    "\n",
    "        return batch_y, forecast, outsample_mask, Y\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # Protection for missing batch_size dimension\n",
    "        if batch['Y'].dim()<3:\n",
    "            batch['Y'] = batch['Y'][None,:,:]\n",
    "\n",
    "        outsample_y, forecast, outsample_mask, Y = self(batch)\n",
    "\n",
    "        loss = self.loss_fn_train(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=Y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, idx):\n",
    "        \n",
    "        # Protection for missing batch_size dimension\n",
    "        if batch['Y'].dim()<3:\n",
    "            batch['Y'] = batch['Y'][None,:,:]\n",
    "\n",
    "        outsample_y, forecast, outsample_mask, Y = self(batch)\n",
    "\n",
    "        loss = self.loss_fn_valid(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=Y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.learning_rate, \n",
    "                               weight_decay=self.weight_decay)\n",
    "        \n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, \n",
    "                                                 step_size=self.lr_decay_step_size, \n",
    "                                                 gamma=self.lr_decay)\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def forecast(self: Transformer, Y_df: pd.DataFrame, X_df: pd.DataFrame = None, \n",
    "                S_df: pd.DataFrame = None, trainer: pl.Trainer =None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Method for forecasting self.n_time_out periods after last timestamp of Y_df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Dataframe with target time-series data, needs 'unique_id','ds' and 'y' columns.\n",
    "    X_df: pd.DataFrame\n",
    "        Dataframe with exogenous time-series data, needs 'unique_id' and 'ds' columns.\n",
    "        Note that 'unique_id' and 'ds' must match Y_df plus the forecasting horizon.\n",
    "    S_df: pd.DataFrame\n",
    "        Dataframe with static data, needs 'unique_id' column.\n",
    "    bath_size: int\n",
    "        Batch size for forecasting.\n",
    "    trainer: pl.Trainer\n",
    "        Trainer object for model training and evaluation.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    forecast_df: pd.DataFrame\n",
    "        Dataframe with forecasts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add forecast dates to Y_df\n",
    "    Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "    if X_df is not None:\n",
    "        X_df['ds'] = pd.to_datetime(X_df['ds'])\n",
    "    self.frequency = pd.infer_freq(Y_df[Y_df['unique_id']==Y_df['unique_id'][0]]['ds']) # Infer with first unique_id series\n",
    "\n",
    "    forecast_dates = pd.date_range(Y_df['ds'].max(), periods=self.pred_len+1, freq=self.frequency)[1:]\n",
    "    index = pd.MultiIndex.from_product([Y_df['unique_id'].unique(), forecast_dates], names=['unique_id', 'ds'])\n",
    "    forecast_df = pd.DataFrame({'y':[0]}, index=index).reset_index()\n",
    "\n",
    "    Y_df = Y_df.append(forecast_df).sort_values(['unique_id','ds']).reset_index(drop=True)\n",
    "    \n",
    "    # Dataset, loader and trainer\n",
    "    dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                    mask_df=None, f_cols=[],\n",
    "                                    input_size=self.seq_len,\n",
    "                                    output_size=self.pred_len,\n",
    "                                    ds_in_test=self.pred_len,\n",
    "                                    is_test=True,\n",
    "                                    verbose=True)\n",
    "\n",
    "    loader = TimeSeriesLoader(dataset=dataset,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False)\n",
    "\n",
    "    if trainer is None:\n",
    "        gpus = -1 if torch.cuda.is_available() else 0\n",
    "        trainer = pl.Trainer(progress_bar_refresh_rate=1,\n",
    "                             gpus=gpus,\n",
    "                             logger=False)\n",
    "\n",
    "    # Forecast\n",
    "    outputs = trainer.predict(self, loader)\n",
    "\n",
    "    # Process forecast and include in forecast_df\n",
    "    _, forecast, _, _ = [torch.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "    forecast = np.transpose(forecast, (0, 2, 1))\n",
    "    forecast_df['y'] = forecast.flatten()\n",
    "\n",
    "    return forecast_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Usage Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.data.datasets.long_horizon import LongHorizon\n",
    "\n",
    "Y_df, X_df, S_df = LongHorizon.load(directory='./data', group='ETTm2')\n",
    "Y_df = Y_df.reset_index(drop=True)\n",
    "Y_df.loc[Y_df['unique_id']=='OT','y'] = Y_df[Y_df['unique_id']=='OT']['y'] + 100 #To obseve differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>-0.041413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>-0.185467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:30:00</td>\n",
       "      <td>-0.257495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:45:00</td>\n",
       "      <td>-0.577510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>-0.385501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id                   ds         y\n",
       "0      HUFL  2016-07-01 00:00:00 -0.041413\n",
       "1      HUFL  2016-07-01 00:15:00 -0.185467\n",
       "2      HUFL  2016-07-01 00:30:00 -0.257495\n",
       "3      HUFL  2016-07-01 00:45:00 -0.577510\n",
       "4      HUFL  2016-07-01 01:00:00 -0.385501"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>ex_1</th>\n",
       "      <th>ex_2</th>\n",
       "      <th>ex_3</th>\n",
       "      <th>ex_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.00137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.00137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:30:00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.00137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:45:00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.00137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>-0.456522</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.00137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id                   ds      ex_1      ex_2  ex_3     ex_4\n",
       "0      HUFL  2016-07-01 00:00:00 -0.500000  0.166667  -0.5 -0.00137\n",
       "1      HUFL  2016-07-01 00:15:00 -0.500000  0.166667  -0.5 -0.00137\n",
       "2      HUFL  2016-07-01 00:30:00 -0.500000  0.166667  -0.5 -0.00137\n",
       "3      HUFL  2016-07-01 00:45:00 -0.500000  0.166667  -0.5 -0.00137\n",
       "4      HUFL  2016-07-01 01:00:00 -0.456522  0.166667  -0.5 -0.00137"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = X_df.drop(columns=['unique_id', 'ds']).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Model and Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "mc_model = {}\n",
    "\n",
    "mc_model['seq_len'] = 96\n",
    "mc_model['label_len'] = 48\n",
    "mc_model['pred_len'] = 96\n",
    "mc_model['output_attention'] = False\n",
    "mc_model['enc_in'] = 7\n",
    "mc_model['dec_in'] = 7\n",
    "mc_model['d_model'] = 512\n",
    "mc_model['c_out'] = 7\n",
    "mc_model['embed'] = 'timeF'\n",
    "mc_model['freq'] = 'h'\n",
    "mc_model['dropout'] = 0.05\n",
    "mc_model['factor'] = 1\n",
    "mc_model['n_heads'] = 8\n",
    "mc_model['d_ff'] = 2_048\n",
    "mc_model['activation'] = 'gelu'\n",
    "mc_model['e_layers'] = 2 \n",
    "mc_model['d_layers'] = 1\n",
    "mc_model['loss_train'] = 'MAE'\n",
    "mc_model['loss_hypar'] = 0.5\n",
    "mc_model['loss_valid'] = 'MAE'\n",
    "mc_model['learning_rate'] = 0.001\n",
    "mc_model['lr_decay'] = 0.5\n",
    "mc_model['weight_decay'] = 0.\n",
    "mc_model['lr_decay_step_size'] = 2\n",
    "mc_model['random_seed'] = 1\n",
    "\n",
    "# Dataset parameters\n",
    "mc_data = {}\n",
    "mc_data['mode'] = 'iterate_windows'\n",
    "mc_data['n_time_in'] = mc_model['seq_len']\n",
    "mc_data['n_time_out'] = mc_model['pred_len']\n",
    "mc_data['batch_size'] = 1\n",
    "mc_data['normalizer_y'] = None\n",
    "mc_data['normalizer_x'] = None\n",
    "mc_data['max_epochs'] = None\n",
    "mc_data['max_steps'] = 10\n",
    "mc_data['early_stop_patience'] = 20\n",
    "\n",
    "len_val = 11_520\n",
    "len_test = 11_520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Loaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.data.tsdataset import IterateWindowsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                                        ds                     \n",
      "                                       min                  max\n",
      "unique_id sample_mask                                          \n",
      "HUFL      0            2017-06-26 00:00:00  2018-02-20 23:45:00\n",
      "          1            2016-07-01 00:00:00  2017-06-25 23:45:00\n",
      "HULL      0            2017-06-26 00:00:00  2018-02-20 23:45:00\n",
      "          1            2016-07-01 00:00:00  2017-06-25 23:45:00\n",
      "LUFL      0            2017-06-26 00:00:00  2018-02-20 23:45:00\n",
      "          1            2016-07-01 00:00:00  2017-06-25 23:45:00\n",
      "LULL      0            2017-06-26 00:00:00  2018-02-20 23:45:00\n",
      "          1            2016-07-01 00:00:00  2017-06-25 23:45:00\n",
      "MUFL      0            2017-06-26 00:00:00  2018-02-20 23:45:00\n",
      "          1            2016-07-01 00:00:00  2017-06-25 23:45:00\n",
      "MULL      0            2017-06-26 00:00:00  2018-02-20 23:45:00\n",
      "          1            2016-07-01 00:00:00  2017-06-25 23:45:00\n",
      "OT        0            2017-06-26 00:00:00  2018-02-20 23:45:00\n",
      "          1            2016-07-01 00:00:00  2017-06-25 23:45:00\n",
      "INFO:root:\n",
      "Total data \t\t\t403200 time stamps \n",
      "Available percentage=100.0, \t403200 time stamps \n",
      "Insample  percentage=60.0, \t241920 time stamps \n",
      "Outsample percentage=40.0, \t161280 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                                        ds                     \n",
      "                                       min                  max\n",
      "unique_id sample_mask                                          \n",
      "HUFL      0            2016-07-01 00:00:00  2018-02-20 23:45:00\n",
      "          1            2017-06-26 00:00:00  2017-10-23 23:45:00\n",
      "HULL      0            2016-07-01 00:00:00  2018-02-20 23:45:00\n",
      "          1            2017-06-26 00:00:00  2017-10-23 23:45:00\n",
      "LUFL      0            2016-07-01 00:00:00  2018-02-20 23:45:00\n",
      "          1            2017-06-26 00:00:00  2017-10-23 23:45:00\n",
      "LULL      0            2016-07-01 00:00:00  2018-02-20 23:45:00\n",
      "          1            2017-06-26 00:00:00  2017-10-23 23:45:00\n",
      "MUFL      0            2016-07-01 00:00:00  2018-02-20 23:45:00\n",
      "          1            2017-06-26 00:00:00  2017-10-23 23:45:00\n",
      "MULL      0            2016-07-01 00:00:00  2018-02-20 23:45:00\n",
      "          1            2017-06-26 00:00:00  2017-10-23 23:45:00\n",
      "OT        0            2016-07-01 00:00:00  2018-02-20 23:45:00\n",
      "          1            2017-06-26 00:00:00  2017-10-23 23:45:00\n",
      "INFO:root:\n",
      "Total data \t\t\t403200 time stamps \n",
      "Available percentage=100.0, \t403200 time stamps \n",
      "Insample  percentage=20.0, \t80640 time stamps \n",
      "Outsample percentage=80.0, \t322560 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                                        ds                     \n",
      "                                       min                  max\n",
      "unique_id sample_mask                                          \n",
      "HUFL      0            2016-07-01 00:00:00  2017-10-23 23:45:00\n",
      "          1            2017-10-24 00:00:00  2018-02-20 23:45:00\n",
      "HULL      0            2016-07-01 00:00:00  2017-10-23 23:45:00\n",
      "          1            2017-10-24 00:00:00  2018-02-20 23:45:00\n",
      "LUFL      0            2016-07-01 00:00:00  2017-10-23 23:45:00\n",
      "          1            2017-10-24 00:00:00  2018-02-20 23:45:00\n",
      "LULL      0            2016-07-01 00:00:00  2017-10-23 23:45:00\n",
      "          1            2017-10-24 00:00:00  2018-02-20 23:45:00\n",
      "MUFL      0            2016-07-01 00:00:00  2017-10-23 23:45:00\n",
      "          1            2017-10-24 00:00:00  2018-02-20 23:45:00\n",
      "MULL      0            2016-07-01 00:00:00  2017-10-23 23:45:00\n",
      "          1            2017-10-24 00:00:00  2018-02-20 23:45:00\n",
      "OT        0            2016-07-01 00:00:00  2017-10-23 23:45:00\n",
      "          1            2017-10-24 00:00:00  2018-02-20 23:45:00\n",
      "INFO:root:\n",
      "Total data \t\t\t403200 time stamps \n",
      "Available percentage=100.0, \t403200 time stamps \n",
      "Insample  percentage=20.0, \t80640 time stamps \n",
      "Outsample percentage=80.0, \t322560 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from neuralforecast.experiments.utils import create_datasets\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc_data,\n",
    "                                                                     S_df=None, \n",
    "                                                                     Y_df=Y_df, X_df=X_df,\n",
    "                                                                     f_cols=f_cols,\n",
    "                                                                     ds_in_val=len_val,\n",
    "                                                                     ds_in_test=len_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=int(mc_data['batch_size']),\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=int(mc_data['batch_size']),\n",
    "                        shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=int(mc_data['batch_size']),\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(**mc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=10)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type         | Params\n",
      "---------------------------------------\n",
      "0 | model | _Transformer | 10.5 M\n",
      "---------------------------------------\n",
      "10.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.5 M    Total params\n",
      "42.160    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 120/68738 [00:03<38:05, 30.03it/s, loss=14.1, v_num=120, train_loss_step=13.20, val_loss=12.70, train_loss_epoch=14.10]\n"
     ]
    }
   ],
   "source": [
    "early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                            min_delta=1e-4, \n",
    "                                            patience=mc_data['early_stop_patience'],\n",
    "                                            verbose=False,\n",
    "                                            mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=mc_data['max_epochs'], \n",
    "                     max_steps=mc_data['max_steps'],\n",
    "                     gradient_clip_val=1.0,\n",
    "                     progress_bar_refresh_rate=10, \n",
    "                     check_val_every_n_epoch=1,\n",
    "                     num_sanity_val_steps=1,\n",
    "                     val_check_interval=1,\n",
    "                     limit_val_batches=1,\n",
    "                     callbacks=[early_stopping])\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = trainer.predict(model, val_loader)\n",
    "\n",
    "#print(\"outputs[0][0].shape\", outputs[0][0].shape)\n",
    "#print(\"outputs[0][1].shape\", outputs[0][1].shape)\n",
    "#print(\"outputs[0][2].shape\", outputs[0][2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>322555</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-23 22:45:00</td>\n",
       "      <td>99.424266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322556</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-23 23:00:00</td>\n",
       "      <td>99.424266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322557</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-23 23:15:00</td>\n",
       "      <td>99.405312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322558</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-23 23:30:00</td>\n",
       "      <td>99.386359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322559</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-23 23:45:00</td>\n",
       "      <td>99.367361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id                   ds          y\n",
       "322555        OT  2017-10-23 22:45:00  99.424266\n",
       "322556        OT  2017-10-23 23:00:00  99.424266\n",
       "322557        OT  2017-10-23 23:15:00  99.405312\n",
       "322558        OT  2017-10-23 23:30:00  99.386359\n",
       "322559        OT  2017-10-23 23:45:00  99.367361"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_forecast_df = Y_df[Y_df['ds']<'2017-10-24']\n",
    "Y_forecast_df = Y_forecast_df.reset_index(drop=True)\n",
    "Y_forecast_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>ex_1</th>\n",
       "      <th>ex_2</th>\n",
       "      <th>ex_3</th>\n",
       "      <th>ex_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>323227</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-24 22:45:00</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.310959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323228</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-24 23:00:00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.310959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323229</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-24 23:15:00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.310959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323230</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-24 23:30:00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.310959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323231</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-24 23:45:00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.310959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id                  ds      ex_1      ex_2      ex_3      ex_4\n",
       "323227        OT 2017-10-24 22:45:00  0.456522 -0.333333  0.266667  0.310959\n",
       "323228        OT 2017-10-24 23:00:00  0.500000 -0.333333  0.266667  0.310959\n",
       "323229        OT 2017-10-24 23:15:00  0.500000 -0.333333  0.266667  0.310959\n",
       "323230        OT 2017-10-24 23:30:00  0.500000 -0.333333  0.266667  0.310959\n",
       "323231        OT 2017-10-24 23:45:00  0.500000 -0.333333  0.266667  0.310959"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_forecast_df = X_df[X_df['ds']<'2017-10-25']\n",
    "X_forecast_df = X_forecast_df.reset_index(drop=True)\n",
    "X_forecast_df['ds'] = pd.to_datetime(X_forecast_df['ds'])\n",
    "X_forecast_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "HUFL      0           2016-07-01 2017-10-23 23:45:00\n",
      "          1           2017-10-24 2017-10-24 23:45:00\n",
      "HULL      0           2016-07-01 2017-10-23 23:45:00\n",
      "          1           2017-10-24 2017-10-24 23:45:00\n",
      "LUFL      0           2016-07-01 2017-10-23 23:45:00\n",
      "          1           2017-10-24 2017-10-24 23:45:00\n",
      "LULL      0           2016-07-01 2017-10-23 23:45:00\n",
      "          1           2017-10-24 2017-10-24 23:45:00\n",
      "MUFL      0           2016-07-01 2017-10-23 23:45:00\n",
      "          1           2017-10-24 2017-10-24 23:45:00\n",
      "MULL      0           2016-07-01 2017-10-23 23:45:00\n",
      "          1           2017-10-24 2017-10-24 23:45:00\n",
      "OT        0           2016-07-01 2017-10-23 23:45:00\n",
      "          1           2017-10-24 2017-10-24 23:45:00\n",
      "INFO:root:\n",
      "Total data \t\t\t323232 time stamps \n",
      "Available percentage=100.0, \t323232 time stamps \n",
      "Insample  percentage=0.21, \t672 time stamps \n",
      "Outsample percentage=99.79, \t322560 time stamps \n",
      "\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/cchallu/NIXTLA/neuralforecast/neuralforecast/data/tsloader.py:47: UserWarning: This class wraps the pytorch `DataLoader` with a special collate function. If you want to use yours simply use `DataLoader`. Removing collate_fn\n",
      "  'This class wraps the pytorch `DataLoader` with a '\n",
      "/Users/cchallu/opt/anaconda3/envs/neuralforecast/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1/1 [00:00<00:00, 17.19it/s]\n"
     ]
    }
   ],
   "source": [
    "forecast_df = model.forecast(Y_df=Y_forecast_df, X_df=X_forecast_df, S_df=S_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2017-10-24 00:00:00</td>\n",
       "      <td>-0.623713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2017-10-24 00:15:00</td>\n",
       "      <td>-0.630241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2017-10-24 00:30:00</td>\n",
       "      <td>-0.630227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2017-10-24 00:45:00</td>\n",
       "      <td>-0.630175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2017-10-24 01:00:00</td>\n",
       "      <td>-0.630084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-24 22:45:00</td>\n",
       "      <td>15.710655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-24 23:00:00</td>\n",
       "      <td>15.710675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-24 23:15:00</td>\n",
       "      <td>15.710443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-24 23:30:00</td>\n",
       "      <td>15.710240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>OT</td>\n",
       "      <td>2017-10-24 23:45:00</td>\n",
       "      <td>15.712422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>672 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id                  ds          y\n",
       "0        HUFL 2017-10-24 00:00:00  -0.623713\n",
       "1        HUFL 2017-10-24 00:15:00  -0.630241\n",
       "2        HUFL 2017-10-24 00:30:00  -0.630227\n",
       "3        HUFL 2017-10-24 00:45:00  -0.630175\n",
       "4        HUFL 2017-10-24 01:00:00  -0.630084\n",
       "..        ...                 ...        ...\n",
       "667        OT 2017-10-24 22:45:00  15.710655\n",
       "668        OT 2017-10-24 23:00:00  15.710675\n",
       "669        OT 2017-10-24 23:15:00  15.710443\n",
       "670        OT 2017-10-24 23:30:00  15.710240\n",
       "671        OT 2017-10-24 23:45:00  15.712422\n",
       "\n",
       "[672 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
